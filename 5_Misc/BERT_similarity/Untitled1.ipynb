{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de64636b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoConfig\n",
    "from transformers import AutoModelForCausalLM, PreTrainedModel\n",
    "import torch\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a0ec3d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"meta-llama/Llama-2-7b-chat-hf\"\n",
    "\n",
    "\n",
    "class LLaMA2Model(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        model_name = \"meta-llama/Llama-2-7b-chat-hf\"\n",
    "\n",
    "        config = AutoConfig.from_pretrained(model_name)\n",
    "        super(LLaMA2Model, self).__init__()\n",
    "        self.llama = AutoModelForCausalLM.from_pretrained(model_name, device_map='auto', torch_dtype=torch.float16)\n",
    "\n",
    "    def forward(self, input_id, mask=None):\n",
    "\n",
    "        output = self.llama(input_ids= input_id, attention_mask=mask,return_dict=False)\n",
    "        \n",
    "        return output\n",
    "\n",
    "\n",
    "# Load tokenizer and model\n",
    "\n",
    "# model_name = \"meta-llama/Llama-2-7b-chat-hf\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "# model = AutoModelForCausalLM.from_pretrained(model_name, device_map='auto', torch_dtype=torch.float16)\n",
    "\n",
    "model = LLaMA2Model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "96d0da67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===================================BUG REPORT===================================\n",
      "Welcome to bitsandbytes. For bug reports, please submit your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
      "================================================================================\n",
      "binary_path: C:\\Users\\hp\\miniconda3\\envs\\tf_gpu\\lib\\site-packages\\bitsandbytes\\cuda_setup\\libbitsandbytes_cuda116.dll\n",
      "CUDA SETUP: Loading binary C:\\Users\\hp\\miniconda3\\envs\\tf_gpu\\lib\\site-packages\\bitsandbytes\\cuda_setup\\libbitsandbytes_cuda116.dll...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hp\\miniconda3\\envs\\tf_gpu\\lib\\site-packages\\scipy\\__init__.py:132: UserWarning: A NumPy version >=1.21.6 and <1.28.0 is required for this version of SciPy (detected version 1.20.0)\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e7e152b8b5a344c181d3fc47f13474de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Some parameters are on the meta device device because they were offloaded to the cpu.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(32000, 4096)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
       "          (act_fn): SiLUActivation()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM\n",
    "import torch\n",
    "class LLaMA2Model(AutoModelForCausalLM):\n",
    "    @classmethod\n",
    "    def from_pretrained(cls, model_name, *model_args, **kwargs):\n",
    "        # This method will handle the model initialization\n",
    "        return super(LLaMA2Model, cls).from_pretrained(model_name, *model_args, **kwargs,device_map='auto',torch_dtype=torch.float16)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask=None):\n",
    "        return super(LLaMA2Model, self).forward(input_ids=input_ids, attention_mask=attention_mask)\n",
    "\n",
    "# Usage\n",
    "model_name = \"meta-llama/Llama-2-7b-chat-hf\"\n",
    "model = LLaMA2Model.from_pretrained(model_name)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bc2c694",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode input text\n",
    "input_text = \"What is capital of USA. Can u tell me more about it\"\n",
    "input_ids = tokenizer.encode(input_text, return_tensors='pt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bca509ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61f0c451",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Move model to evaluation mode\n",
    "\n",
    "model.eval()\n",
    "# Generate output\n",
    "output_ids = model.generate(input_ids,max_length=50)\n",
    "output_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cc13505",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Decode output to text\n",
    "output_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "\n",
    "print(output_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3820b0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "# Add the parent directory to the Python path\n",
    "from reghub_pack.models import LLaMA_RegHub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9163eee",
   "metadata": {},
   "outputs": [],
   "source": [
    "model=LLaMA_RegHub()\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "417cd079",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected exception formatting exception. Falling back to standard exception\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\hp\\miniconda3\\envs\\tf_gpu\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3526, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"C:\\Users\\hp\\AppData\\Local\\Temp\\ipykernel_412\\1820151757.py\", line 1, in <module>\n",
      "    get_ipython().system('nvidia-smi')\n",
      "  File \"C:\\Users\\hp\\miniconda3\\envs\\tf_gpu\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 653, in system_piped\n",
      "    self.user_ns[\"_exit_code\"] = system(cmd)\n",
      "  File \"C:\\Users\\hp\\miniconda3\\envs\\tf_gpu\\lib\\site-packages\\IPython\\utils\\_process_win32.py\", line 124, in system\n",
      "    return process_handler(cmd, _system_body)\n",
      "  File \"C:\\Users\\hp\\miniconda3\\envs\\tf_gpu\\lib\\site-packages\\IPython\\utils\\_process_common.py\", line 77, in process_handler\n",
      "    p = subprocess.Popen(cmd, shell=shell,\n",
      "  File \"C:\\Users\\hp\\miniconda3\\envs\\tf_gpu\\lib\\subprocess.py\", line 951, in __init__\n",
      "    self._execute_child(args, executable, preexec_fn, close_fds,\n",
      "  File \"C:\\Users\\hp\\miniconda3\\envs\\tf_gpu\\lib\\subprocess.py\", line 1436, in _execute_child\n",
      "    hp, ht, pid, tid = _winapi.CreateProcess(executable, args,\n",
      "OSError: [WinError 1455] The paging file is too small for this operation to complete\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\hp\\miniconda3\\envs\\tf_gpu\\lib\\site-packages\\executing\\executing.py\", line 317, in executing\n",
      "    args = executing_cache[key]\n",
      "KeyError: (<code object run_code at 0x000001C799483240, file \"C:\\Users\\hp\\miniconda3\\envs\\tf_gpu\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3490>, 1956781765184, 74)\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\hp\\miniconda3\\envs\\tf_gpu\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2120, in showtraceback\n",
      "    stb = self.InteractiveTB.structured_traceback(\n",
      "  File \"C:\\Users\\hp\\miniconda3\\envs\\tf_gpu\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1435, in structured_traceback\n",
      "    return FormattedTB.structured_traceback(\n",
      "  File \"C:\\Users\\hp\\miniconda3\\envs\\tf_gpu\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1326, in structured_traceback\n",
      "    return VerboseTB.structured_traceback(\n",
      "  File \"C:\\Users\\hp\\miniconda3\\envs\\tf_gpu\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1173, in structured_traceback\n",
      "    formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n",
      "  File \"C:\\Users\\hp\\miniconda3\\envs\\tf_gpu\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1063, in format_exception_as_a_whole\n",
      "    self.get_records(etb, number_of_lines_of_context, tb_offset) if etb else []\n",
      "  File \"C:\\Users\\hp\\miniconda3\\envs\\tf_gpu\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1160, in get_records\n",
      "    res = list(stack_data.FrameInfo.stack_data(etb, options=options))[tb_offset:]\n",
      "  File \"C:\\Users\\hp\\miniconda3\\envs\\tf_gpu\\lib\\site-packages\\stack_data\\core.py\", line 565, in stack_data\n",
      "    yield from collapse_repeated(\n",
      "  File \"C:\\Users\\hp\\miniconda3\\envs\\tf_gpu\\lib\\site-packages\\stack_data\\utils.py\", line 84, in collapse_repeated\n",
      "    yield from map(mapper, original_group)\n",
      "  File \"C:\\Users\\hp\\miniconda3\\envs\\tf_gpu\\lib\\site-packages\\stack_data\\core.py\", line 555, in mapper\n",
      "    return cls(f, options)\n",
      "  File \"C:\\Users\\hp\\miniconda3\\envs\\tf_gpu\\lib\\site-packages\\stack_data\\core.py\", line 520, in __init__\n",
      "    self.executing = Source.executing(frame_or_tb)\n",
      "  File \"C:\\Users\\hp\\miniconda3\\envs\\tf_gpu\\lib\\site-packages\\executing\\executing.py\", line 369, in executing\n",
      "    args = find(source=cls.for_frame(frame), retry_cache=True)\n",
      "  File \"C:\\Users\\hp\\miniconda3\\envs\\tf_gpu\\lib\\site-packages\\executing\\executing.py\", line 252, in for_frame\n",
      "    return cls.for_filename(frame.f_code.co_filename, frame.f_globals or {}, use_cache)\n",
      "  File \"C:\\Users\\hp\\miniconda3\\envs\\tf_gpu\\lib\\site-packages\\executing\\executing.py\", line 270, in for_filename\n",
      "    result = source_cache[filename] = cls._for_filename_and_lines(filename, lines)\n",
      "  File \"C:\\Users\\hp\\miniconda3\\envs\\tf_gpu\\lib\\site-packages\\executing\\executing.py\", line 281, in _for_filename_and_lines\n",
      "    result = source_cache[(filename, lines)] = cls(filename, lines)\n",
      "  File \"C:\\Users\\hp\\miniconda3\\envs\\tf_gpu\\lib\\site-packages\\stack_data\\core.py\", line 79, in __init__\n",
      "    super(Source, self).__init__(*args, **kwargs)\n",
      "  File \"C:\\Users\\hp\\miniconda3\\envs\\tf_gpu\\lib\\site-packages\\executing\\executing.py\", line 228, in __init__\n",
      "    self.tree = ast.parse(ast_text, filename=filename)\n",
      "  File \"C:\\Users\\hp\\miniconda3\\envs\\tf_gpu\\lib\\ast.py\", line 50, in parse\n",
      "    return compile(source, filename, mode, flags,\n",
      "MemoryError\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e5f7ae1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpu1",
   "language": "python",
   "name": "gpu1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
