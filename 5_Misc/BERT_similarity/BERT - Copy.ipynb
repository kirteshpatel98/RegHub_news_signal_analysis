{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4c5e2885",
   "metadata": {
    "id": "4c5e2885"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hp\\miniconda3\\envs\\tf_gpu\\lib\\site-packages\\scipy\\__init__.py:132: UserWarning: A NumPy version >=1.21.6 and <1.28.0 is required for this version of SciPy (detected version 1.20.0)\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "# ! pip install transformers\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import BertTokenizer\n",
    "from transformers import BertForSequenceClassification, AdamW\n",
    "\n",
    "import torchsummary as summary\n",
    "from tqdm import tqdm\n",
    "\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from transformers import BertModel\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import base64\n",
    "import gc\n",
    "import glob, os\n",
    "\n",
    "from torch.optim import Adam\n",
    "from tqdm import tqdm\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import json\n",
    "from general_functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4351b623",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndef create_onedrive_directdownload(onedrive_link):\\n    data_bytes64 = base64.b64encode(bytes(onedrive_link, \\'utf-8\\'))\\n    data_bytes64_String = data_bytes64.decode(\\'utf-8\\').replace(\\'/\\',\\'_\\').replace(\\'+\\',\\'-\\').rstrip(\"=\")\\n    resultUrl = f\"https://api.onedrive.com/v1.0/shares/u!{data_bytes64_String}/root/content\"\\n    return resultUrl\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "def create_onedrive_directdownload(onedrive_link):\n",
    "    data_bytes64 = base64.b64encode(bytes(onedrive_link, 'utf-8'))\n",
    "    data_bytes64_String = data_bytes64.decode('utf-8').replace('/','_').replace('+','-').rstrip(\"=\")\n",
    "    resultUrl = f\"https://api.onedrive.com/v1.0/shares/u!{data_bytes64_String}/root/content\"\n",
    "    return resultUrl\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1b1f6bd4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nonedrive_link='https://1drv.ms/u/s!AoiE7xOoBAsngsgx2HKIw1RxUnATTg?e=XsxZGD'\\nonedrive_direct_link=create_onedrive_directdownload(onedrive_link)\\nonedrive_direct_link\\ndf=pd.read_csv(onedrive_direct_link)\\n\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "onedrive_link='https://1drv.ms/u/s!AoiE7xOoBAsngsgx2HKIw1RxUnATTg?e=XsxZGD'\n",
    "onedrive_direct_link=create_onedrive_directdownload(onedrive_link)\n",
    "onedrive_direct_link\n",
    "df=pd.read_csv(onedrive_direct_link)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3ebd1036-13db-4bc8-a542-0fe8f08633d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import data\n",
    "# Access aws credentials from json file\n",
    "with open(\"../aws_credentials.json\", 'r') as file:\n",
    "    aws_creds_json = json.load(file)\n",
    "# Specify s3 bucket\n",
    "bucket = \"fs-reghub-news-analysis\"\n",
    "\n",
    "# Connect to aws and dowload the files\n",
    "aws = awsOps(aws_creds_json)\n",
    "df = aws.get_df(bucket=bucket, file=\"data_rule_labels_v1.csv\")\n",
    "df_categories = aws.get_df(bucket=bucket, file=\"rule_labels_v1.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "59c1a0cf-980c-41d8-8e93-b72caa861778",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>news_content</th>\n",
       "      <th>rule_labels_comb</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Berenberg Bank analysts have provided a buy ra...</td>\n",
       "      <td>['market']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The article states that Berenberg, a German in...</td>\n",
       "      <td>['market']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>In their analysis on October 30, 2023, experts...</td>\n",
       "      <td>['market']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The private bank Berenberg has upgraded its ra...</td>\n",
       "      <td>['papers', 'market']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>In a research note published by Sebastian Bray...</td>\n",
       "      <td>['sanctions', 'papers', 'market']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5517</th>\n",
       "      <td>INTERVIEW Interview with Les Échos Interview w...</td>\n",
       "      <td>['legal', 'statements', 'guidelines']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5518</th>\n",
       "      <td>UBS's latest Investor Watch report reveals tha...</td>\n",
       "      <td>['reports', 'personnel']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5519</th>\n",
       "      <td>SNB erwartet für 2021 Jahresgewinn von rund 26...</td>\n",
       "      <td>['legal', 'reports', 'market']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5520</th>\n",
       "      <td>0:00 News A cryptocurrency exchange in Hong Ko...</td>\n",
       "      <td>['legal', 'reports', 'guidelines', 'press']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5521</th>\n",
       "      <td>Bloomberg has reported that a number of indivi...</td>\n",
       "      <td>['legal', 'reports', 'statements', 'press']</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5522 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           news_content  \\\n",
       "0     Berenberg Bank analysts have provided a buy ra...   \n",
       "1     The article states that Berenberg, a German in...   \n",
       "2     In their analysis on October 30, 2023, experts...   \n",
       "3     The private bank Berenberg has upgraded its ra...   \n",
       "4     In a research note published by Sebastian Bray...   \n",
       "...                                                 ...   \n",
       "5517  INTERVIEW Interview with Les Échos Interview w...   \n",
       "5518  UBS's latest Investor Watch report reveals tha...   \n",
       "5519  SNB erwartet für 2021 Jahresgewinn von rund 26...   \n",
       "5520  0:00 News A cryptocurrency exchange in Hong Ko...   \n",
       "5521  Bloomberg has reported that a number of indivi...   \n",
       "\n",
       "                                 rule_labels_comb  \n",
       "0                                      ['market']  \n",
       "1                                      ['market']  \n",
       "2                                      ['market']  \n",
       "3                            ['papers', 'market']  \n",
       "4               ['sanctions', 'papers', 'market']  \n",
       "...                                           ...  \n",
       "5517        ['legal', 'statements', 'guidelines']  \n",
       "5518                     ['reports', 'personnel']  \n",
       "5519               ['legal', 'reports', 'market']  \n",
       "5520  ['legal', 'reports', 'guidelines', 'press']  \n",
       "5521  ['legal', 'reports', 'statements', 'press']  \n",
       "\n",
       "[5522 rows x 2 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.drop('Unnamed: 0',axis=1,inplace=True)\n",
    "df=df[['news_content','rule_labels_comb']]\n",
    "df=df[~df['rule_labels_comb'].isna()]\n",
    "df=df[(df['rule_labels_comb'].apply(len)!=2)]\n",
    "df = df.reset_index()\n",
    "del df['index']\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b5dd1747",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ast import literal_eval\n",
    "df['rule_labels_comb'] = df['rule_labels_comb'].apply(literal_eval)\n",
    "df['target']='[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0]'\n",
    "df['target'] = df['target'].apply(literal_eval)\n",
    "\n",
    "df['target'].iloc[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7dd33e6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                [8]\n",
       "1                [8]\n",
       "2                [8]\n",
       "3             [2, 8]\n",
       "4          [1, 2, 8]\n",
       "            ...     \n",
       "5517       [0, 4, 5]\n",
       "5518          [3, 7]\n",
       "5519       [0, 3, 8]\n",
       "5520    [0, 3, 5, 6]\n",
       "5521    [0, 3, 4, 6]\n",
       "Name: rule_labels_comb, Length: 5522, dtype: object"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "for index, value in df['rule_labels_comb'].iteritems():\n",
    "    class_dic={'legal':0,'sanctions':1,'papers':2,'reports':3,'statements':4,'guidelines':5,'press':6,'personnel':7,'market':8}\n",
    "    val=[class_dic.get(i, i) for i in value]\n",
    "    df['rule_labels_comb'].iloc[index]=val\n",
    "    y=0\n",
    "    for x in df['rule_labels_comb'].iloc[index]:\n",
    "        df['target'].iloc[index][x]=1.0\n",
    "        y=y+1\n",
    "df['rule_labels_comb']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b02483e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0]\n",
       "1       [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0]\n",
       "2       [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0]\n",
       "3       [0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0]\n",
       "4       [0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0]\n",
       "                            ...                      \n",
       "5517    [1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0]\n",
       "5518    [0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0]\n",
       "5519    [1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0]\n",
       "5520    [1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0]\n",
       "5521    [1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0]\n",
       "Name: target, Length: 5522, dtype: object"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fec05ad",
   "metadata": {
    "id": "5fec05ad"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fff40ee7",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fff40ee7",
    "outputId": "11c0fdd4-a78f-4d43-ed76-1abbd19e0f37"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nfrom google.colab import drive\\ndrive.mount('/content/drive')\\n\\nimport os\\nos.chdir('/content/drive/My Drive/')\\n\\n\\ndf=pd.read_csv('BERT_data.csv')\\ndf=df[~(df['content']=='nan')]\\ndf['content']=df['content'].astype(str)\\ndf['subject']=df['subject'].astype(str)\\n\""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import data from gdrive\n",
    "'''\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "import os\n",
    "os.chdir('/content/drive/My Drive/')\n",
    "\n",
    "\n",
    "df=pd.read_csv('BERT_data.csv')\n",
    "df=df[~(df['content']=='nan')]\n",
    "df['content']=df['content'].astype(str)\n",
    "df['subject']=df['subject'].astype(str)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "862a17fe",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "id": "862a17fe",
    "outputId": "6aa6d00d-cdc8-49ac-b34e-38be032d4249"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>news_content</th>\n",
       "      <th>rule_labels_comb</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Berenberg Bank analysts have provided a buy ra...</td>\n",
       "      <td>[8]</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The article states that Berenberg, a German in...</td>\n",
       "      <td>[8]</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>In their analysis on October 30, 2023, experts...</td>\n",
       "      <td>[8]</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The private bank Berenberg has upgraded its ra...</td>\n",
       "      <td>[2, 8]</td>\n",
       "      <td>[0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>In a research note published by Sebastian Bray...</td>\n",
       "      <td>[1, 2, 8]</td>\n",
       "      <td>[0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5517</th>\n",
       "      <td>INTERVIEW Interview with Les Échos Interview w...</td>\n",
       "      <td>[0, 4, 5]</td>\n",
       "      <td>[1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5518</th>\n",
       "      <td>UBS's latest Investor Watch report reveals tha...</td>\n",
       "      <td>[3, 7]</td>\n",
       "      <td>[0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5519</th>\n",
       "      <td>SNB erwartet für 2021 Jahresgewinn von rund 26...</td>\n",
       "      <td>[0, 3, 8]</td>\n",
       "      <td>[1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5520</th>\n",
       "      <td>0:00 News A cryptocurrency exchange in Hong Ko...</td>\n",
       "      <td>[0, 3, 5, 6]</td>\n",
       "      <td>[1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5521</th>\n",
       "      <td>Bloomberg has reported that a number of indivi...</td>\n",
       "      <td>[0, 3, 4, 6]</td>\n",
       "      <td>[1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5522 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           news_content rule_labels_comb  \\\n",
       "0     Berenberg Bank analysts have provided a buy ra...              [8]   \n",
       "1     The article states that Berenberg, a German in...              [8]   \n",
       "2     In their analysis on October 30, 2023, experts...              [8]   \n",
       "3     The private bank Berenberg has upgraded its ra...           [2, 8]   \n",
       "4     In a research note published by Sebastian Bray...        [1, 2, 8]   \n",
       "...                                                 ...              ...   \n",
       "5517  INTERVIEW Interview with Les Échos Interview w...        [0, 4, 5]   \n",
       "5518  UBS's latest Investor Watch report reveals tha...           [3, 7]   \n",
       "5519  SNB erwartet für 2021 Jahresgewinn von rund 26...        [0, 3, 8]   \n",
       "5520  0:00 News A cryptocurrency exchange in Hong Ko...     [0, 3, 5, 6]   \n",
       "5521  Bloomberg has reported that a number of indivi...     [0, 3, 4, 6]   \n",
       "\n",
       "                                             target  \n",
       "0     [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0]  \n",
       "1     [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0]  \n",
       "2     [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0]  \n",
       "3     [0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0]  \n",
       "4     [0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0]  \n",
       "...                                             ...  \n",
       "5517  [1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0]  \n",
       "5518  [0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0]  \n",
       "5519  [1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0]  \n",
       "5520  [1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0]  \n",
       "5521  [1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0]  \n",
       "\n",
       "[5522 rows x 3 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# change range of labels, minimum should be zero\n",
    "df['rule_labels_comb']=df['rule_labels_comb'].astype(str)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b2181d08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\ndf_3=df[(df['result']==3) & (df['content'].str.len()<350)]\\ndf=df[~(df['result']==3)]\\ndf=pd.concat([df,df_3])\\ndf\\n\""
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# additional filtering to balance classes\n",
    "'''\n",
    "df_3=df[(df['result']==3) & (df['content'].str.len()<350)]\n",
    "df=df[~(df['result']==3)]\n",
    "df=pd.concat([df,df_3])\n",
    "df\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "97659d46",
   "metadata": {
    "id": "97659d46"
   },
   "outputs": [],
   "source": [
    "# import BERT tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "class Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self,df):\n",
    "        self.labels=df['target']\n",
    "        self.text=[tokenizer(text,padding='max_length',truncation=True,return_tensors=\"pt\") for text in df['news_content']]\n",
    "\n",
    "    def classes(self):\n",
    "        return self.labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def get_batch_labels(self, idx):\n",
    "        # Fetch a batch of labels\n",
    "        return np.array(self.labels[idx])\n",
    "\n",
    "    def get_batch_texts(self, idx):\n",
    "        # Fetch a batch of inputs\n",
    "        return self.text[idx]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        batch_texts = self.get_batch_texts(idx)\n",
    "        batch_y = self.get_batch_labels(idx)\n",
    "\n",
    "        return batch_texts, batch_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8a1a835a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train test split\n",
    "X_train, X_test= train_test_split(df[['news_content','target']], test_size=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2f08669a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train=X_train\n",
    "df_val=X_test\n",
    "df_test=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "yDzCUBywaqK_",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yDzCUBywaqK_",
    "outputId": "f0c830cc-e9c2-4e35-8c9f-e7b0e6f9f5aa"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>news_content</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1269</th>\n",
       "      <td>The article discusses the planned downsizing o...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>In its quarterly report for the third quarter ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1843</th>\n",
       "      <td>The article discusses the recent rally in the ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1897</th>\n",
       "      <td>The DAX is expected to open higher at the star...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>366</th>\n",
       "      <td>The article discusses the connection between D...</td>\n",
       "      <td>[1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2447</th>\n",
       "      <td>The German stock market index (DAX) closed sli...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3968</th>\n",
       "      <td>Separately Berenberg Bank cut their price targ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1741</th>\n",
       "      <td>The article discusses the review of the crimin...</td>\n",
       "      <td>[0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3043</th>\n",
       "      <td>Asset Management  Investment Bank  Wealth Mana...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4741</th>\n",
       "      <td>Schott bereitet den Börsengang der PharmaSpart...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4141 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           news_content  \\\n",
       "1269  The article discusses the planned downsizing o...   \n",
       "22    In its quarterly report for the third quarter ...   \n",
       "1843  The article discusses the recent rally in the ...   \n",
       "1897  The DAX is expected to open higher at the star...   \n",
       "366   The article discusses the connection between D...   \n",
       "...                                                 ...   \n",
       "2447  The German stock market index (DAX) closed sli...   \n",
       "3968  Separately Berenberg Bank cut their price targ...   \n",
       "1741  The article discusses the review of the crimin...   \n",
       "3043  Asset Management  Investment Bank  Wealth Mana...   \n",
       "4741  Schott bereitet den Börsengang der PharmaSpart...   \n",
       "\n",
       "                                             target  \n",
       "1269  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0]  \n",
       "22    [0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0]  \n",
       "1843  [0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0]  \n",
       "1897  [0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0]  \n",
       "366   [1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0]  \n",
       "...                                             ...  \n",
       "2447  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0]  \n",
       "3968  [0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0]  \n",
       "1741  [0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]  \n",
       "3043  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0]  \n",
       "4741  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0]  \n",
       "\n",
       "[4141 rows x 2 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0cedf1b1",
   "metadata": {
    "id": "0cedf1b1"
   },
   "outputs": [],
   "source": [
    "# BERT classifier architecture, with 7 output classes\n",
    "class BertClassifier(nn.Module):\n",
    "\n",
    "    def __init__(self, dropout=0.5):\n",
    "\n",
    "        super(BertClassifier, self).__init__()\n",
    "\n",
    "        self.bert = BertModel.from_pretrained('bert-base-uncased')\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.linear = nn.Linear(768, 9)\n",
    "        torch.nn.init.kaiming_uniform_(self.linear.weight, nonlinearity='relu')\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def classes(self):\n",
    "        return self.labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def get_batch_labels(self, idx):\n",
    "        # Fetch a batch of labels\n",
    "        return np.array(self.labels[idx])\n",
    "\n",
    "    def get_batch_texts(self, idx):\n",
    "        # Fetch a batch of inputs\n",
    "        return self.text[idx]\n",
    "\n",
    "    def forward(self, input_id, mask):\n",
    "\n",
    "        _, pooled_output = self.bert(input_ids= input_id, attention_mask=mask,return_dict=False)\n",
    "        dropout_output = self.dropout(pooled_output)\n",
    "        linear_output = self.linear(dropout_output)\n",
    "        final_layer = self.relu(linear_output)\n",
    "\n",
    "        return final_layer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "50e427bd",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "50e427bd",
    "outputId": "00e951a8-0497-4f1f-d04e-7d941be41a07"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# change runtype to GPU\n",
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "383e6dad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "EPOCHS = 100\n",
    "model = BertClassifier()\n",
    "LR = 0.00001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "39d24417",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>news_content</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Anita Dushyanth is a Healthcare equity researc...</td>\n",
       "      <td>[0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Hinter unseren beiden heutigen Protagonisten D...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Die Experten der RBC Capital Markets bewerten ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Flossbach von Storch is one of Europe's larges...</td>\n",
       "      <td>[0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The use of Artificial Intelligence (AI) in hea...</td>\n",
       "      <td>[1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1376</th>\n",
       "      <td>Oddo BHF, a financial services group, has main...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1377</th>\n",
       "      <td>HSBC Trinkaus &amp; Burkhardt, a financial analyst...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1378</th>\n",
       "      <td>Kaum verändert im Vergleich zu der letzten Not...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1379</th>\n",
       "      <td>Am deutschen Aktienmarkt ist die Aktie der Com...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1380</th>\n",
       "      <td>Nachdem die SVB in den USA mit dem Rücken zur ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1381 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           news_content  \\\n",
       "0     Anita Dushyanth is a Healthcare equity researc...   \n",
       "1     Hinter unseren beiden heutigen Protagonisten D...   \n",
       "2     Die Experten der RBC Capital Markets bewerten ...   \n",
       "3     Flossbach von Storch is one of Europe's larges...   \n",
       "4     The use of Artificial Intelligence (AI) in hea...   \n",
       "...                                                 ...   \n",
       "1376  Oddo BHF, a financial services group, has main...   \n",
       "1377  HSBC Trinkaus & Burkhardt, a financial analyst...   \n",
       "1378  Kaum verändert im Vergleich zu der letzten Not...   \n",
       "1379  Am deutschen Aktienmarkt ist die Aktie der Com...   \n",
       "1380  Nachdem die SVB in den USA mit dem Rücken zur ...   \n",
       "\n",
       "                                             target  \n",
       "0     [0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0]  \n",
       "1     [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0]  \n",
       "2     [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0]  \n",
       "3     [0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]  \n",
       "4     [1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]  \n",
       "...                                             ...  \n",
       "1376  [0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0]  \n",
       "1377  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0]  \n",
       "1378  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0]  \n",
       "1379  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0]  \n",
       "1380  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0]  \n",
       "\n",
       "[1381 rows x 2 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train=df_train.reset_index()\n",
    "df_train.drop('index',axis=1,inplace=True)\n",
    "\n",
    "df_val=df_val.reset_index()\n",
    "df_val.drop('index',axis=1,inplace=True)\n",
    "df_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bd93f8c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert.embeddings.word_embeddings.weight\n",
      "bert.embeddings.position_embeddings.weight\n",
      "bert.embeddings.token_type_embeddings.weight\n",
      "bert.embeddings.LayerNorm.weight\n",
      "bert.embeddings.LayerNorm.bias\n",
      "bert.encoder.layer.0.attention.self.query.weight\n",
      "bert.encoder.layer.0.attention.self.query.bias\n",
      "bert.encoder.layer.0.attention.self.key.weight\n",
      "bert.encoder.layer.0.attention.self.key.bias\n",
      "bert.encoder.layer.0.attention.self.value.weight\n",
      "bert.encoder.layer.0.attention.self.value.bias\n",
      "bert.encoder.layer.0.attention.output.dense.weight\n",
      "bert.encoder.layer.0.attention.output.dense.bias\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.weight\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.bias\n",
      "bert.encoder.layer.0.intermediate.dense.weight\n",
      "bert.encoder.layer.0.intermediate.dense.bias\n",
      "bert.encoder.layer.0.output.dense.weight\n",
      "bert.encoder.layer.0.output.dense.bias\n",
      "bert.encoder.layer.0.output.LayerNorm.weight\n",
      "bert.encoder.layer.0.output.LayerNorm.bias\n",
      "bert.encoder.layer.1.attention.self.query.weight\n",
      "bert.encoder.layer.1.attention.self.query.bias\n",
      "bert.encoder.layer.1.attention.self.key.weight\n",
      "bert.encoder.layer.1.attention.self.key.bias\n",
      "bert.encoder.layer.1.attention.self.value.weight\n",
      "bert.encoder.layer.1.attention.self.value.bias\n",
      "bert.encoder.layer.1.attention.output.dense.weight\n",
      "bert.encoder.layer.1.attention.output.dense.bias\n",
      "bert.encoder.layer.1.attention.output.LayerNorm.weight\n",
      "bert.encoder.layer.1.attention.output.LayerNorm.bias\n",
      "bert.encoder.layer.1.intermediate.dense.weight\n",
      "bert.encoder.layer.1.intermediate.dense.bias\n",
      "bert.encoder.layer.1.output.dense.weight\n",
      "bert.encoder.layer.1.output.dense.bias\n",
      "bert.encoder.layer.1.output.LayerNorm.weight\n",
      "bert.encoder.layer.1.output.LayerNorm.bias\n",
      "bert.encoder.layer.2.attention.self.query.weight\n",
      "bert.encoder.layer.2.attention.self.query.bias\n",
      "bert.encoder.layer.2.attention.self.key.weight\n",
      "bert.encoder.layer.2.attention.self.key.bias\n",
      "bert.encoder.layer.2.attention.self.value.weight\n",
      "bert.encoder.layer.2.attention.self.value.bias\n",
      "bert.encoder.layer.2.attention.output.dense.weight\n",
      "bert.encoder.layer.2.attention.output.dense.bias\n",
      "bert.encoder.layer.2.attention.output.LayerNorm.weight\n",
      "bert.encoder.layer.2.attention.output.LayerNorm.bias\n",
      "bert.encoder.layer.2.intermediate.dense.weight\n",
      "bert.encoder.layer.2.intermediate.dense.bias\n",
      "bert.encoder.layer.2.output.dense.weight\n",
      "bert.encoder.layer.2.output.dense.bias\n",
      "bert.encoder.layer.2.output.LayerNorm.weight\n",
      "bert.encoder.layer.2.output.LayerNorm.bias\n",
      "bert.encoder.layer.3.attention.self.query.weight\n",
      "bert.encoder.layer.3.attention.self.query.bias\n",
      "bert.encoder.layer.3.attention.self.key.weight\n",
      "bert.encoder.layer.3.attention.self.key.bias\n",
      "bert.encoder.layer.3.attention.self.value.weight\n",
      "bert.encoder.layer.3.attention.self.value.bias\n",
      "bert.encoder.layer.3.attention.output.dense.weight\n",
      "bert.encoder.layer.3.attention.output.dense.bias\n",
      "bert.encoder.layer.3.attention.output.LayerNorm.weight\n",
      "bert.encoder.layer.3.attention.output.LayerNorm.bias\n",
      "bert.encoder.layer.3.intermediate.dense.weight\n",
      "bert.encoder.layer.3.intermediate.dense.bias\n",
      "bert.encoder.layer.3.output.dense.weight\n",
      "bert.encoder.layer.3.output.dense.bias\n",
      "bert.encoder.layer.3.output.LayerNorm.weight\n",
      "bert.encoder.layer.3.output.LayerNorm.bias\n",
      "bert.encoder.layer.4.attention.self.query.weight\n",
      "bert.encoder.layer.4.attention.self.query.bias\n",
      "bert.encoder.layer.4.attention.self.key.weight\n",
      "bert.encoder.layer.4.attention.self.key.bias\n",
      "bert.encoder.layer.4.attention.self.value.weight\n",
      "bert.encoder.layer.4.attention.self.value.bias\n",
      "bert.encoder.layer.4.attention.output.dense.weight\n",
      "bert.encoder.layer.4.attention.output.dense.bias\n",
      "bert.encoder.layer.4.attention.output.LayerNorm.weight\n",
      "bert.encoder.layer.4.attention.output.LayerNorm.bias\n",
      "bert.encoder.layer.4.intermediate.dense.weight\n",
      "bert.encoder.layer.4.intermediate.dense.bias\n",
      "bert.encoder.layer.4.output.dense.weight\n",
      "bert.encoder.layer.4.output.dense.bias\n",
      "bert.encoder.layer.4.output.LayerNorm.weight\n",
      "bert.encoder.layer.4.output.LayerNorm.bias\n",
      "bert.encoder.layer.5.attention.self.query.weight\n",
      "bert.encoder.layer.5.attention.self.query.bias\n",
      "bert.encoder.layer.5.attention.self.key.weight\n",
      "bert.encoder.layer.5.attention.self.key.bias\n",
      "bert.encoder.layer.5.attention.self.value.weight\n",
      "bert.encoder.layer.5.attention.self.value.bias\n",
      "bert.encoder.layer.5.attention.output.dense.weight\n",
      "bert.encoder.layer.5.attention.output.dense.bias\n",
      "bert.encoder.layer.5.attention.output.LayerNorm.weight\n",
      "bert.encoder.layer.5.attention.output.LayerNorm.bias\n",
      "bert.encoder.layer.5.intermediate.dense.weight\n",
      "bert.encoder.layer.5.intermediate.dense.bias\n",
      "bert.encoder.layer.5.output.dense.weight\n",
      "bert.encoder.layer.5.output.dense.bias\n",
      "bert.encoder.layer.5.output.LayerNorm.weight\n",
      "bert.encoder.layer.5.output.LayerNorm.bias\n",
      "bert.encoder.layer.6.attention.self.query.weight\n",
      "bert.encoder.layer.6.attention.self.query.bias\n",
      "bert.encoder.layer.6.attention.self.key.weight\n",
      "bert.encoder.layer.6.attention.self.key.bias\n",
      "bert.encoder.layer.6.attention.self.value.weight\n",
      "bert.encoder.layer.6.attention.self.value.bias\n",
      "bert.encoder.layer.6.attention.output.dense.weight\n",
      "bert.encoder.layer.6.attention.output.dense.bias\n",
      "bert.encoder.layer.6.attention.output.LayerNorm.weight\n",
      "bert.encoder.layer.6.attention.output.LayerNorm.bias\n",
      "bert.encoder.layer.6.intermediate.dense.weight\n",
      "bert.encoder.layer.6.intermediate.dense.bias\n",
      "bert.encoder.layer.6.output.dense.weight\n",
      "bert.encoder.layer.6.output.dense.bias\n",
      "bert.encoder.layer.6.output.LayerNorm.weight\n",
      "bert.encoder.layer.6.output.LayerNorm.bias\n",
      "bert.encoder.layer.7.attention.self.query.weight\n",
      "bert.encoder.layer.7.attention.self.query.bias\n",
      "bert.encoder.layer.7.attention.self.key.weight\n",
      "bert.encoder.layer.7.attention.self.key.bias\n",
      "bert.encoder.layer.7.attention.self.value.weight\n",
      "bert.encoder.layer.7.attention.self.value.bias\n",
      "bert.encoder.layer.7.attention.output.dense.weight\n",
      "bert.encoder.layer.7.attention.output.dense.bias\n",
      "bert.encoder.layer.7.attention.output.LayerNorm.weight\n",
      "bert.encoder.layer.7.attention.output.LayerNorm.bias\n",
      "bert.encoder.layer.7.intermediate.dense.weight\n",
      "bert.encoder.layer.7.intermediate.dense.bias\n",
      "bert.encoder.layer.7.output.dense.weight\n",
      "bert.encoder.layer.7.output.dense.bias\n",
      "bert.encoder.layer.7.output.LayerNorm.weight\n",
      "bert.encoder.layer.7.output.LayerNorm.bias\n",
      "bert.encoder.layer.8.attention.self.query.weight\n",
      "bert.encoder.layer.8.attention.self.query.bias\n",
      "bert.encoder.layer.8.attention.self.key.weight\n",
      "bert.encoder.layer.8.attention.self.key.bias\n",
      "bert.encoder.layer.8.attention.self.value.weight\n",
      "bert.encoder.layer.8.attention.self.value.bias\n",
      "bert.encoder.layer.8.attention.output.dense.weight\n",
      "bert.encoder.layer.8.attention.output.dense.bias\n",
      "bert.encoder.layer.8.attention.output.LayerNorm.weight\n",
      "bert.encoder.layer.8.attention.output.LayerNorm.bias\n",
      "bert.encoder.layer.8.intermediate.dense.weight\n",
      "bert.encoder.layer.8.intermediate.dense.bias\n",
      "bert.encoder.layer.8.output.dense.weight\n",
      "bert.encoder.layer.8.output.dense.bias\n",
      "bert.encoder.layer.8.output.LayerNorm.weight\n",
      "bert.encoder.layer.8.output.LayerNorm.bias\n",
      "bert.encoder.layer.9.attention.self.query.weight\n",
      "bert.encoder.layer.9.attention.self.query.bias\n",
      "bert.encoder.layer.9.attention.self.key.weight\n",
      "bert.encoder.layer.9.attention.self.key.bias\n",
      "bert.encoder.layer.9.attention.self.value.weight\n",
      "bert.encoder.layer.9.attention.self.value.bias\n",
      "bert.encoder.layer.9.attention.output.dense.weight\n",
      "bert.encoder.layer.9.attention.output.dense.bias\n",
      "bert.encoder.layer.9.attention.output.LayerNorm.weight\n",
      "bert.encoder.layer.9.attention.output.LayerNorm.bias\n",
      "bert.encoder.layer.9.intermediate.dense.weight\n",
      "bert.encoder.layer.9.intermediate.dense.bias\n",
      "bert.encoder.layer.9.output.dense.weight\n",
      "bert.encoder.layer.9.output.dense.bias\n",
      "bert.encoder.layer.9.output.LayerNorm.weight\n",
      "bert.encoder.layer.9.output.LayerNorm.bias\n",
      "bert.encoder.layer.10.attention.self.query.weight\n",
      "bert.encoder.layer.10.attention.self.query.bias\n",
      "bert.encoder.layer.10.attention.self.key.weight\n",
      "bert.encoder.layer.10.attention.self.key.bias\n",
      "bert.encoder.layer.10.attention.self.value.weight\n",
      "bert.encoder.layer.10.attention.self.value.bias\n",
      "bert.encoder.layer.10.attention.output.dense.weight\n",
      "bert.encoder.layer.10.attention.output.dense.bias\n",
      "bert.encoder.layer.10.attention.output.LayerNorm.weight\n",
      "bert.encoder.layer.10.attention.output.LayerNorm.bias\n",
      "bert.encoder.layer.10.intermediate.dense.weight\n",
      "bert.encoder.layer.10.intermediate.dense.bias\n",
      "bert.encoder.layer.10.output.dense.weight\n",
      "bert.encoder.layer.10.output.dense.bias\n",
      "bert.encoder.layer.10.output.LayerNorm.weight\n",
      "bert.encoder.layer.10.output.LayerNorm.bias\n",
      "bert.encoder.layer.11.attention.self.query.weight\n",
      "bert.encoder.layer.11.attention.self.query.bias\n",
      "bert.encoder.layer.11.attention.self.key.weight\n",
      "bert.encoder.layer.11.attention.self.key.bias\n",
      "bert.encoder.layer.11.attention.self.value.weight\n",
      "bert.encoder.layer.11.attention.self.value.bias\n",
      "bert.encoder.layer.11.attention.output.dense.weight\n",
      "bert.encoder.layer.11.attention.output.dense.bias\n",
      "bert.encoder.layer.11.attention.output.LayerNorm.weight\n",
      "bert.encoder.layer.11.attention.output.LayerNorm.bias\n",
      "bert.encoder.layer.11.intermediate.dense.weight\n",
      "bert.encoder.layer.11.intermediate.dense.bias\n",
      "bert.encoder.layer.11.output.dense.weight\n",
      "bert.encoder.layer.11.output.dense.bias\n",
      "bert.encoder.layer.11.output.LayerNorm.weight\n",
      "bert.encoder.layer.11.output.LayerNorm.bias\n",
      "bert.pooler.dense.weight\n",
      "bert.pooler.dense.bias\n",
      "linear.weight\n",
      "linear.bias\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "201"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# display BERT layers\n",
    "n=0\n",
    "for x in model.state_dict():\n",
    "    n=n+1\n",
    "    print(x)\n",
    "n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c9734c9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# freeze first 8 layers \n",
    "n=0\n",
    "for param in model.parameters():\n",
    "    n=n+1\n",
    "    param.requires_grad = False\n",
    "    if n==(201-68):\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8a1fb41f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The article discusses the planned downsizing of Credit Suisse, with a particular impact on its investment banking division. The staff reduction will be carried out in multiple stages.'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# change datatypes of input data\n",
    "df_train['news_content']=df_train['news_content'].astype(str)\n",
    "\n",
    "df_val['news_content']=df_val['news_content'].astype(str)\n",
    "df_train['news_content'].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1dd80045",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "72ff0c5d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "72ff0c5d",
    "outputId": "0f9a549a-7e09-4eaf-f974-3aaced2a7338",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/100: 100%|████████████████████████████████████████████████████████| 130/130 [08:08<00:00,  3.76s/item, Epoch=1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 1 | Train Loss:  0.083             | Train Accuracy:  0.511             | Val Loss:  0.078             | Val Accuracy:  0.584             | True Train Accuracy:  0.855             | True Val Accuracy:  0.869\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/100:   9%|██▍                       | 12/130 [00:49<08:11,  4.16s/item, acc=0.719, loss=0.0775, true_acc=0.851]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[25], line 188\u001b[0m\n\u001b[0;32m    184\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m glob\u001b[38;5;241m.\u001b[39mglob(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBERT_checkpoint*.pth\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    185\u001b[0m         os\u001b[38;5;241m.\u001b[39mremove(f)  \n\u001b[1;32m--> 188\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdf_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdf_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mLR\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mEPOCHS\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[25], line 87\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(model, train_data, val_data, learning_rate, epochs)\u001b[0m\n\u001b[0;32m     85\u001b[0m     batch_loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m     86\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m---> 87\u001b[0m     p_bar\u001b[38;5;241m.\u001b[39mset_postfix(loss\u001b[38;5;241m=\u001b[39mbatch_loss\u001b[38;5;241m.\u001b[39mitem() \u001b[38;5;241m/\u001b[39m \u001b[38;5;28;43mlen\u001b[39;49m(train_input[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m'\u001b[39m]), acc\u001b[38;5;241m=\u001b[39macc \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(train_input[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m'\u001b[39m]), true_acc\u001b[38;5;241m=\u001b[39mtrue_acc_batch \u001b[38;5;241m/\u001b[39m (\u001b[38;5;28mlen\u001b[39m(train_input[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m'\u001b[39m])\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m9\u001b[39m))\n\u001b[0;32m     88\u001b[0m     p_bar\u001b[38;5;241m.\u001b[39mupdate()\n\u001b[0;32m     89\u001b[0m p_bar\u001b[38;5;241m.\u001b[39mset_postfix({\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEpoch\u001b[39m\u001b[38;5;124m'\u001b[39m: epoch_num \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m})\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def train(model, train_data, val_data, learning_rate, epochs):\n",
    "    \n",
    "    stop_criteria=0\n",
    "    \n",
    "    train, val = Dataset(train_data), Dataset(val_data)\n",
    "    \n",
    "    # mini batching\n",
    "    train_dataloader = torch.utils.data.DataLoader(train, batch_size=32)\n",
    "    val_dataloader = torch.utils.data.DataLoader(val, batch_size=32)\n",
    "\n",
    "    use_cuda = torch.cuda.is_available()\n",
    "    device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = Adam(model.parameters(), lr= learning_rate)\n",
    "    \n",
    "    if use_cuda:\n",
    "\n",
    "            model = model.cuda()\n",
    "            criterion = criterion.cuda()\n",
    "            \n",
    "    plot_val_acc=[]\n",
    "    plot_train_acc=[]\n",
    "    plot_epoch=[]\n",
    "    plot_train_loss=[]\n",
    "    plot_val_loss=[]\n",
    "\n",
    "    for epoch_num in range(epochs):\n",
    "\n",
    "            total_acc_train = 0\n",
    "            total_loss_train = 0\n",
    "\n",
    "            true_acc=0\n",
    "            n=0\n",
    "            with tqdm(total=len(train_dataloader), desc=f'Epoch {epoch_num + 1}/{epochs}', unit='item',position=0,leave=True) as p_bar:\n",
    "                for train_input, train_label in train_dataloader:\n",
    "                    train_label = train_label.to(device) # to cuda GPU\n",
    "                    mask = train_input['attention_mask'].to(device) # attention mask\n",
    "                    input_id = train_input['input_ids'].squeeze(1).to(device)\n",
    "\n",
    "                    l1_loss=0\n",
    "\n",
    "                    # for L1 regularization\n",
    "                    a=0\n",
    "                    reg_loss = 0\n",
    "                    for param in model.parameters():\n",
    "                        a=a+1\n",
    "                        if a >=201-68:\n",
    "                            reg_loss += torch.norm(param, 1) \n",
    "                    '''\n",
    "                    factor = 0.00001 #lambda for L1 regularization\n",
    "                    l1_loss=factor * reg_loss # L1 loss\n",
    "                    '''\n",
    "                    # model output\n",
    "                    output = model(input_id, mask)\n",
    "\n",
    "\n",
    "                    # loss value\n",
    "                    batch_loss = criterion(output, train_label) + l1_loss\n",
    "                    total_loss_train += batch_loss.item() \n",
    "\n",
    "                    # train accuracy \n",
    "                    output=output.squeeze()\n",
    "                    train_label=train_label.squeeze()\n",
    "                    # print(output)\n",
    "                    # print(train_label)\n",
    "                    # print(torch.gather(train_label, 1, torch.argmax(output,dim=1).view(-1, 1)))\n",
    "                    acc = torch.gather(train_label, 1, torch.argmax(output,dim=1).view(-1, 1)).sum().item()\n",
    "                    total_acc_train += acc\n",
    "                    \n",
    "                    # true accuracy\n",
    "                    true_acc_batch=0\n",
    "                    for (x,y) in zip(torch.round(F.sigmoid(output)*10)/10,train_label): # ceil or round\n",
    "                        x=torch.tensor(x)\n",
    "                        y=torch.tensor(y)\n",
    "                        # print(x)\n",
    "                        # print(y)\n",
    "                        x[x<1]=0\n",
    "                        true_acc=true_acc+torch.sum(x==y).item()\n",
    "                        true_acc_batch=true_acc_batch+torch.sum(x==y).item()\n",
    "\n",
    "\n",
    "                    # backpropogation\n",
    "                    model.zero_grad()\n",
    "                    batch_loss.backward()\n",
    "                    optimizer.step()\n",
    "                    p_bar.set_postfix(loss=batch_loss.item() / len(train_input['input_ids']), acc=acc / len(train_input['input_ids']), true_acc=true_acc_batch / (len(train_input['input_ids'])*9))\n",
    "                    p_bar.update()\n",
    "                p_bar.set_postfix({'Epoch': epoch_num + 1})\n",
    "            # print(output)\n",
    "            # print(train_label)\n",
    "\n",
    "            total_acc_val = 0\n",
    "            total_loss_val = 0\n",
    "            true_val_acc=0\n",
    "            \n",
    "            # for validation accuracy\n",
    "            with torch.no_grad():\n",
    "\n",
    "                for val_input, val_label in val_dataloader:\n",
    "\n",
    "                    val_label = val_label.to(device)\n",
    "                    mask = val_input['attention_mask'].to(device)\n",
    "                    input_id = val_input['input_ids'].squeeze(1).to(device)\n",
    "                    \n",
    "                    # validation output\n",
    "                    output = model(input_id, mask)\n",
    "                    \n",
    "                    # validation loss value\n",
    "                    batch_loss = criterion(output, val_label)\n",
    "                    total_loss_val += batch_loss.item()\n",
    "                \n",
    "                    # validation accuracy\n",
    "                    output=output.squeeze()\n",
    "                    val_label=val_label.squeeze()\n",
    "                    acc = torch.gather(val_label, 1, torch.argmax(output,dim=1).view(-1, 1)).sum().item()\n",
    "                    total_acc_val += acc\n",
    "                    \n",
    "                    # true accuracy\n",
    "                    for (x,y) in zip(torch.round(F.sigmoid(output)*10)/10,val_label): # ceil or round\n",
    "                        x=torch.tensor(x)\n",
    "                        y=torch.tensor(y)\n",
    "                        x[x<1]=0\n",
    "                        true_val_acc=true_val_acc+torch.sum(x==y).item()\n",
    "            \n",
    "            print(\n",
    "            f'Epochs: {epoch_num + 1} | Train Loss: {total_loss_train / len(train_data): .3f} \\\n",
    "            | Train Accuracy: {total_acc_train / len(train_data): .3f} \\\n",
    "            | Val Loss: {total_loss_val / len(val_data): .3f} \\\n",
    "            | Val Accuracy: {total_acc_val / len(val_data): .3f} \\\n",
    "            | True Train Accuracy: {true_acc / (len(train_data)*9): .3f} \\\n",
    "            | True Val Accuracy: {true_val_acc / (len(val_data)*9): .3f}',end='\\r')\n",
    "            \n",
    "            plot_train_acc.append(total_acc_train / len(train_data))\n",
    "            plot_val_acc.append(total_acc_val / len(val_data))\n",
    "            plot_epoch.append(epoch_num + 1)\n",
    "            plot_train_loss.append(total_loss_train / (len(train_data)*9))\n",
    "            plot_val_loss.append(total_loss_val / (len(val_data)*9))\n",
    "            \n",
    "            # model checkpoint\n",
    "            checkpoint = {\n",
    "            'epoch': epoch_num+1,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            }\n",
    "            \n",
    "            torch.save(checkpoint, f'BERT_checkpoint_epoch{epoch_num+1}.pth')\n",
    "            \n",
    "            try:\n",
    "                if plot_val_loss[-1]>plot_val_loss[-2] and plot_val_loss[-2]>plot_val_loss[-3]:\n",
    "                    break\n",
    "            except:\n",
    "                pass\n",
    "    \n",
    "        \n",
    "    fig, ax = fig,ax= plt.subplots(1, 2, figsize=(10, 7))\n",
    "    fig.suptitle('Accuracy and Loss')\n",
    "    \n",
    "    \n",
    "    \n",
    "    ax[0].plot(plot_epoch,plot_val_acc, label='Validation Accuracy',color='orange')\n",
    "    ax[0].plot(plot_epoch,plot_train_acc, label='Training Accuracy',color='red')\n",
    "    ax[0].legend()\n",
    "    ax[0].grid(True)\n",
    "    \n",
    "    ax[1].plot(plot_epoch,plot_train_loss, label=' Training Loss',color='blue')\n",
    "    ax[1].plot(plot_epoch,plot_val_loss, label=' Validation Loss',color='yellow')\n",
    "    ax[1].legend()\n",
    "    ax[1].grid(True)\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "    # retrieve best checkpoint model\n",
    "    checkpoint = torch.load(f'BERT_checkpoint_epoch{epoch_num+1-2}.pth')\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    \n",
    "    \n",
    "    # save best model\n",
    "    torch.save(model, 'BERT_classifier.pth')\n",
    "\n",
    "    # save model to aws S3 cloud\n",
    "    with open(\"../aws_credentials.json\", 'r') as file:\n",
    "        aws_creds_json = json.load(file)\n",
    "    bucket = \"fs-reghub-news-analysis\"\n",
    "    aws = awsOps(aws_creds_json)\n",
    "    aws.upload_file(bucket=bucket, name):\n",
    "    \n",
    "    # delete remaining model checkpoints\n",
    "    for f in glob.glob(\"BERT_checkpoint*.pth\"):\n",
    "        os.remove(f)  \n",
    "        \n",
    "        \n",
    "train(model, df_train, df_val, LR, EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcf9c50b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1201baaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../aws_credentials.json\", 'r') as file:\n",
    "        aws_creds_json = json.load(file)\n",
    "bucket = \"fs-reghub-news-analysis\"\n",
    "aws = awsOps(aws_creds_json)\n",
    "\n",
    "\n",
    "\n",
    "aws.upload_file(bucket=bucket,path='BERT_classifier.pth', name='BERT_classifier.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "070da5db",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "EOL while scanning string literal (3406149190.py, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[43], line 2\u001b[1;36m\u001b[0m\n\u001b[1;33m    path = path.replace(\"\\\\\", f\"\\\")\u001b[0m\n\u001b[1;37m                                   ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m EOL while scanning string literal\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d58c24bd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "gpu1",
   "language": "python",
   "name": "gpu1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
