{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4c5e2885",
   "metadata": {
    "id": "4c5e2885"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hp\\miniconda3\\envs\\tf_gpu\\lib\\site-packages\\scipy\\__init__.py:132: UserWarning: A NumPy version >=1.21.6 and <1.28.0 is required for this version of SciPy (detected version 1.20.0)\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "# ! pip install transformers\n",
    "import torch\n",
    "import numpy as np\n",
    "from transformers import BertTokenizer\n",
    "from transformers import BertForSequenceClassification, AdamW\n",
    "\n",
    "import torchsummary as summary\n",
    "from tqdm import tqdm\n",
    "\n",
    "from torch import nn\n",
    "from transformers import BertModel\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import base64\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4351b623",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_onedrive_directdownload(onedrive_link):\n",
    "    data_bytes64 = base64.b64encode(bytes(onedrive_link, 'utf-8'))\n",
    "    data_bytes64_String = data_bytes64.decode('utf-8').replace('/','_').replace('+','-').rstrip(\"=\")\n",
    "    resultUrl = f\"https://api.onedrive.com/v1.0/shares/u!{data_bytes64_String}/root/content\"\n",
    "    return resultUrl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1b1f6bd4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>news_content</th>\n",
       "      <th>rule_labels_comb</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The article discusses how M.M. Warburg may hav...</td>\n",
       "      <td>['legal']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The Cologne Prosecutor's Office accuses German...</td>\n",
       "      <td>['collaboration', 'legal']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Deutsche Bank AG has acquired an additional 49...</td>\n",
       "      <td>['collaboration']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Mozambique has reached a negotiated agreement ...</td>\n",
       "      <td>['collaboration', 'legal']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The UBS APAC Sustainable Finance Conference 20...</td>\n",
       "      <td>['personnel', 'legal']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2589</th>\n",
       "      <td>Arbeiten bei HSBC Trinkaus &amp; Burkhardt GmbH in...</td>\n",
       "      <td>['product']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2590</th>\n",
       "      <td>HSBC muss MillionenStrafe wegen WhatsAppNutzun...</td>\n",
       "      <td>['personnel']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2591</th>\n",
       "      <td>Alle offenen Sozialkompetent Jobs bei Bethmann...</td>\n",
       "      <td>['personnel']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2592</th>\n",
       "      <td>Citigroup Inc. Deutsche Bank AG HSBC Holdings ...</td>\n",
       "      <td>['legal']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2593</th>\n",
       "      <td>The Competition and Markets Authority said Cit...</td>\n",
       "      <td>['legal']</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2594 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           news_content  \\\n",
       "0     The article discusses how M.M. Warburg may hav...   \n",
       "1     The Cologne Prosecutor's Office accuses German...   \n",
       "2     Deutsche Bank AG has acquired an additional 49...   \n",
       "3     Mozambique has reached a negotiated agreement ...   \n",
       "4     The UBS APAC Sustainable Finance Conference 20...   \n",
       "...                                                 ...   \n",
       "2589  Arbeiten bei HSBC Trinkaus & Burkhardt GmbH in...   \n",
       "2590  HSBC muss MillionenStrafe wegen WhatsAppNutzun...   \n",
       "2591  Alle offenen Sozialkompetent Jobs bei Bethmann...   \n",
       "2592  Citigroup Inc. Deutsche Bank AG HSBC Holdings ...   \n",
       "2593  The Competition and Markets Authority said Cit...   \n",
       "\n",
       "                rule_labels_comb  \n",
       "0                      ['legal']  \n",
       "1     ['collaboration', 'legal']  \n",
       "2              ['collaboration']  \n",
       "3     ['collaboration', 'legal']  \n",
       "4         ['personnel', 'legal']  \n",
       "...                          ...  \n",
       "2589                 ['product']  \n",
       "2590               ['personnel']  \n",
       "2591               ['personnel']  \n",
       "2592                   ['legal']  \n",
       "2593                   ['legal']  \n",
       "\n",
       "[2594 rows x 2 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "onedrive_link='https://1drv.ms/u/s!AoiE7xOoBAsngsgmC-PI9ray-gfELQ?e=UtffII'\n",
    "onedrive_direct_link=create_onedrive_directdownload(onedrive_link)\n",
    "onedrive_direct_link\n",
    "df=pd.read_csv(onedrive_direct_link)\n",
    "df.drop('Unnamed: 0',axis=1,inplace=True)\n",
    "df=df[['news_content','rule_labels_comb']]\n",
    "df=df[~df['rule_labels_comb'].isna()]\n",
    "df=df[(df['rule_labels_comb'].apply(len)!=2)]\n",
    "df = df.reset_index()\n",
    "del df['index']\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b5dd1747",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ast import literal_eval\n",
    "df['rule_labels_comb'] = df['rule_labels_comb'].apply(literal_eval)\n",
    "df['target']='[0.0,0.0,0.0,0.0]'\n",
    "df['target'] = df['target'].apply(literal_eval)\n",
    "\n",
    "df['target'].iloc[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7dd33e6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0          [3]\n",
       "1       [2, 3]\n",
       "2          [2]\n",
       "3       [2, 3]\n",
       "4       [0, 3]\n",
       "         ...  \n",
       "2589       [1]\n",
       "2590       [0]\n",
       "2591       [0]\n",
       "2592       [3]\n",
       "2593       [3]\n",
       "Name: rule_labels_comb, Length: 2594, dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for index, value in df['rule_labels_comb'].iteritems():\n",
    "    class_dic={'personnel':0,'product':1,'collaboration':2,'legal':3}\n",
    "    val=[class_dic.get(i, i) for i in value]\n",
    "    df['rule_labels_comb'].iloc[index]=val\n",
    "    y=0\n",
    "    for x in df['rule_labels_comb'].iloc[index]:\n",
    "        df['target'].iloc[index][x]=1.0\n",
    "        y=y+1\n",
    "df['rule_labels_comb']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b02483e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       [0.0, 0.0, 0.0, 1.0]\n",
       "1       [0.0, 0.0, 1.0, 1.0]\n",
       "2       [0.0, 0.0, 1.0, 0.0]\n",
       "3       [0.0, 0.0, 1.0, 1.0]\n",
       "4       [1.0, 0.0, 0.0, 1.0]\n",
       "                ...         \n",
       "2589    [0.0, 1.0, 0.0, 0.0]\n",
       "2590    [1.0, 0.0, 0.0, 0.0]\n",
       "2591    [1.0, 0.0, 0.0, 0.0]\n",
       "2592    [0.0, 0.0, 0.0, 1.0]\n",
       "2593    [0.0, 0.0, 0.0, 1.0]\n",
       "Name: target, Length: 2594, dtype: object"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6cccaed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5fec05ad",
   "metadata": {
    "id": "5fec05ad"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "66"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fff40ee7",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fff40ee7",
    "outputId": "11c0fdd4-a78f-4d43-ed76-1abbd19e0f37"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nfrom google.colab import drive\\ndrive.mount('/content/drive')\\n\\nimport os\\nos.chdir('/content/drive/My Drive/')\\n\\n\\ndf=pd.read_csv('BERT_data.csv')\\ndf=df[~(df['content']=='nan')]\\ndf['content']=df['content'].astype(str)\\ndf['subject']=df['subject'].astype(str)\\n\""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import data from gdrive\n",
    "'''\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "import os\n",
    "os.chdir('/content/drive/My Drive/')\n",
    "\n",
    "\n",
    "df=pd.read_csv('BERT_data.csv')\n",
    "df=df[~(df['content']=='nan')]\n",
    "df['content']=df['content'].astype(str)\n",
    "df['subject']=df['subject'].astype(str)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "862a17fe",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "id": "862a17fe",
    "outputId": "6aa6d00d-cdc8-49ac-b34e-38be032d4249"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>news_content</th>\n",
       "      <th>rule_labels_comb</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The article discusses how M.M. Warburg may hav...</td>\n",
       "      <td>[3]</td>\n",
       "      <td>[0.0, 0.0, 0.0, 1.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The Cologne Prosecutor's Office accuses German...</td>\n",
       "      <td>[2, 3]</td>\n",
       "      <td>[0.0, 0.0, 1.0, 1.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Deutsche Bank AG has acquired an additional 49...</td>\n",
       "      <td>[2]</td>\n",
       "      <td>[0.0, 0.0, 1.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Mozambique has reached a negotiated agreement ...</td>\n",
       "      <td>[2, 3]</td>\n",
       "      <td>[0.0, 0.0, 1.0, 1.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The UBS APAC Sustainable Finance Conference 20...</td>\n",
       "      <td>[0, 3]</td>\n",
       "      <td>[1.0, 0.0, 0.0, 1.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2589</th>\n",
       "      <td>Arbeiten bei HSBC Trinkaus &amp; Burkhardt GmbH in...</td>\n",
       "      <td>[1]</td>\n",
       "      <td>[0.0, 1.0, 0.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2590</th>\n",
       "      <td>HSBC muss MillionenStrafe wegen WhatsAppNutzun...</td>\n",
       "      <td>[0]</td>\n",
       "      <td>[1.0, 0.0, 0.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2591</th>\n",
       "      <td>Alle offenen Sozialkompetent Jobs bei Bethmann...</td>\n",
       "      <td>[0]</td>\n",
       "      <td>[1.0, 0.0, 0.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2592</th>\n",
       "      <td>Citigroup Inc. Deutsche Bank AG HSBC Holdings ...</td>\n",
       "      <td>[3]</td>\n",
       "      <td>[0.0, 0.0, 0.0, 1.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2593</th>\n",
       "      <td>The Competition and Markets Authority said Cit...</td>\n",
       "      <td>[3]</td>\n",
       "      <td>[0.0, 0.0, 0.0, 1.0]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2594 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           news_content rule_labels_comb  \\\n",
       "0     The article discusses how M.M. Warburg may hav...              [3]   \n",
       "1     The Cologne Prosecutor's Office accuses German...           [2, 3]   \n",
       "2     Deutsche Bank AG has acquired an additional 49...              [2]   \n",
       "3     Mozambique has reached a negotiated agreement ...           [2, 3]   \n",
       "4     The UBS APAC Sustainable Finance Conference 20...           [0, 3]   \n",
       "...                                                 ...              ...   \n",
       "2589  Arbeiten bei HSBC Trinkaus & Burkhardt GmbH in...              [1]   \n",
       "2590  HSBC muss MillionenStrafe wegen WhatsAppNutzun...              [0]   \n",
       "2591  Alle offenen Sozialkompetent Jobs bei Bethmann...              [0]   \n",
       "2592  Citigroup Inc. Deutsche Bank AG HSBC Holdings ...              [3]   \n",
       "2593  The Competition and Markets Authority said Cit...              [3]   \n",
       "\n",
       "                    target  \n",
       "0     [0.0, 0.0, 0.0, 1.0]  \n",
       "1     [0.0, 0.0, 1.0, 1.0]  \n",
       "2     [0.0, 0.0, 1.0, 0.0]  \n",
       "3     [0.0, 0.0, 1.0, 1.0]  \n",
       "4     [1.0, 0.0, 0.0, 1.0]  \n",
       "...                    ...  \n",
       "2589  [0.0, 1.0, 0.0, 0.0]  \n",
       "2590  [1.0, 0.0, 0.0, 0.0]  \n",
       "2591  [1.0, 0.0, 0.0, 0.0]  \n",
       "2592  [0.0, 0.0, 0.0, 1.0]  \n",
       "2593  [0.0, 0.0, 0.0, 1.0]  \n",
       "\n",
       "[2594 rows x 3 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# change range of labels, minimum should be zero\n",
    "df['rule_labels_comb']=df['rule_labels_comb'].astype(str)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3d96f8e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b2181d08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\ndf_3=df[(df['result']==3) & (df['content'].str.len()<350)]\\ndf=df[~(df['result']==3)]\\ndf=pd.concat([df,df_3])\\ndf\\n\""
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# additional filtering to balance classes\n",
    "'''\n",
    "df_3=df[(df['result']==3) & (df['content'].str.len()<350)]\n",
    "df=df[~(df['result']==3)]\n",
    "df=pd.concat([df,df_3])\n",
    "df\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "97659d46",
   "metadata": {
    "id": "97659d46"
   },
   "outputs": [],
   "source": [
    "# import BERT tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "class Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self,df):\n",
    "        self.labels=df['target']\n",
    "        self.text=[tokenizer(text,padding='max_length',truncation=True,return_tensors=\"pt\") for text in df['news_content']]\n",
    "\n",
    "    def classes(self):\n",
    "        return self.labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def get_batch_labels(self, idx):\n",
    "        # Fetch a batch of labels\n",
    "        return np.array(self.labels[idx])\n",
    "\n",
    "    def get_batch_texts(self, idx):\n",
    "        # Fetch a batch of inputs\n",
    "        return self.text[idx]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        batch_texts = self.get_batch_texts(idx)\n",
    "        batch_y = self.get_batch_labels(idx)\n",
    "\n",
    "        return batch_texts, batch_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8a1a835a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train test split\n",
    "X_train, X_test= train_test_split(df[['news_content','target']], test_size=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2f08669a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train=X_train\n",
    "df_val=X_test\n",
    "df_test=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "yDzCUBywaqK_",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yDzCUBywaqK_",
    "outputId": "f0c830cc-e9c2-4e35-8c9f-e7b0e6f9f5aa"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>news_content</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>552</th>\n",
       "      <td>The article discusses a research analysis and ...</td>\n",
       "      <td>[0.0, 1.0, 1.0, 1.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1746</th>\n",
       "      <td>A Berlin-based family office has advertised fo...</td>\n",
       "      <td>[1.0, 0.0, 0.0, 1.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The UBS APAC Sustainable Finance Conference 20...</td>\n",
       "      <td>[1.0, 0.0, 0.0, 1.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2248</th>\n",
       "      <td>The German bank M.M. Warburg is reportedly exp...</td>\n",
       "      <td>[1.0, 0.0, 1.0, 1.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1004</th>\n",
       "      <td>The Versorgungsanstalt des Bundes und der LÃ¤n...</td>\n",
       "      <td>[1.0, 0.0, 1.0, 1.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2474</th>\n",
       "      <td>The Swiss government will not launch an invest...</td>\n",
       "      <td>[1.0, 0.0, 0.0, 1.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2436</th>\n",
       "      <td>Group of experts on banking stability now unde...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 1.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>The article discusses the connection between D...</td>\n",
       "      <td>[1.0, 0.0, 1.0, 1.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1175</th>\n",
       "      <td>2023 Archegos: FINMA schliesst Verfahren gegen...</td>\n",
       "      <td>[0.0, 1.0, 1.0, 1.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2331</th>\n",
       "      <td>The article discusses the potential for stock ...</td>\n",
       "      <td>[0.0, 1.0, 0.0, 0.0]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1945 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           news_content                target\n",
       "552   The article discusses a research analysis and ...  [0.0, 1.0, 1.0, 1.0]\n",
       "1746  A Berlin-based family office has advertised fo...  [1.0, 0.0, 0.0, 1.0]\n",
       "4     The UBS APAC Sustainable Finance Conference 20...  [1.0, 0.0, 0.0, 1.0]\n",
       "2248  The German bank M.M. Warburg is reportedly exp...  [1.0, 0.0, 1.0, 1.0]\n",
       "1004  The Versorgungsanstalt des Bundes und der LÃ¤n...  [1.0, 0.0, 1.0, 1.0]\n",
       "...                                                 ...                   ...\n",
       "2474  The Swiss government will not launch an invest...  [1.0, 0.0, 0.0, 1.0]\n",
       "2436  Group of experts on banking stability now unde...  [0.0, 0.0, 0.0, 1.0]\n",
       "36    The article discusses the connection between D...  [1.0, 0.0, 1.0, 1.0]\n",
       "1175  2023 Archegos: FINMA schliesst Verfahren gegen...  [0.0, 1.0, 1.0, 1.0]\n",
       "2331  The article discusses the potential for stock ...  [0.0, 1.0, 0.0, 0.0]\n",
       "\n",
       "[1945 rows x 2 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0cedf1b1",
   "metadata": {
    "id": "0cedf1b1"
   },
   "outputs": [],
   "source": [
    "# BERT classifier architecture, with 7 output classes\n",
    "class BertClassifier(nn.Module):\n",
    "\n",
    "    def __init__(self, dropout=0.5):\n",
    "\n",
    "        super(BertClassifier, self).__init__()\n",
    "\n",
    "        self.bert = BertModel.from_pretrained('bert-base-uncased')\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.linear = nn.Linear(768, 4)\n",
    "        torch.nn.init.kaiming_uniform_(self.linear.weight, nonlinearity='relu')\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def classes(self):\n",
    "        return self.labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def get_batch_labels(self, idx):\n",
    "        # Fetch a batch of labels\n",
    "        return np.array(self.labels[idx])\n",
    "\n",
    "    def get_batch_texts(self, idx):\n",
    "        # Fetch a batch of inputs\n",
    "        return self.text[idx]\n",
    "\n",
    "    def forward(self, input_id, mask):\n",
    "\n",
    "        _, pooled_output = self.bert(input_ids= input_id, attention_mask=mask,return_dict=False)\n",
    "        dropout_output = self.dropout(pooled_output)\n",
    "        linear_output = self.linear(dropout_output)\n",
    "        final_layer = self.relu(linear_output)\n",
    "\n",
    "        return final_layer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "50e427bd",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "50e427bd",
    "outputId": "00e951a8-0497-4f1f-d04e-7d941be41a07"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# change runtype to GPU\n",
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "383e6dad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "EPOCHS = 50\n",
    "model = BertClassifier()\n",
    "LR = 0.000001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "39d24417",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>news_content</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The article discusses a research analysis and ...</td>\n",
       "      <td>[0.0, 1.0, 1.0, 1.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A Berlin-based family office has advertised fo...</td>\n",
       "      <td>[1.0, 0.0, 0.0, 1.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The UBS APAC Sustainable Finance Conference 20...</td>\n",
       "      <td>[1.0, 0.0, 0.0, 1.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The German bank M.M. Warburg is reportedly exp...</td>\n",
       "      <td>[1.0, 0.0, 1.0, 1.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The Versorgungsanstalt des Bundes und der LÃ¤n...</td>\n",
       "      <td>[1.0, 0.0, 1.0, 1.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1940</th>\n",
       "      <td>The Swiss government will not launch an invest...</td>\n",
       "      <td>[1.0, 0.0, 0.0, 1.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1941</th>\n",
       "      <td>Group of experts on banking stability now unde...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 1.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1942</th>\n",
       "      <td>The article discusses the connection between D...</td>\n",
       "      <td>[1.0, 0.0, 1.0, 1.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1943</th>\n",
       "      <td>2023 Archegos: FINMA schliesst Verfahren gegen...</td>\n",
       "      <td>[0.0, 1.0, 1.0, 1.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1944</th>\n",
       "      <td>The article discusses the potential for stock ...</td>\n",
       "      <td>[0.0, 1.0, 0.0, 0.0]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1945 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           news_content                target\n",
       "0     The article discusses a research analysis and ...  [0.0, 1.0, 1.0, 1.0]\n",
       "1     A Berlin-based family office has advertised fo...  [1.0, 0.0, 0.0, 1.0]\n",
       "2     The UBS APAC Sustainable Finance Conference 20...  [1.0, 0.0, 0.0, 1.0]\n",
       "3     The German bank M.M. Warburg is reportedly exp...  [1.0, 0.0, 1.0, 1.0]\n",
       "4     The Versorgungsanstalt des Bundes und der LÃ¤n...  [1.0, 0.0, 1.0, 1.0]\n",
       "...                                                 ...                   ...\n",
       "1940  The Swiss government will not launch an invest...  [1.0, 0.0, 0.0, 1.0]\n",
       "1941  Group of experts on banking stability now unde...  [0.0, 0.0, 0.0, 1.0]\n",
       "1942  The article discusses the connection between D...  [1.0, 0.0, 1.0, 1.0]\n",
       "1943  2023 Archegos: FINMA schliesst Verfahren gegen...  [0.0, 1.0, 1.0, 1.0]\n",
       "1944  The article discusses the potential for stock ...  [0.0, 1.0, 0.0, 0.0]\n",
       "\n",
       "[1945 rows x 2 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train=df_train.reset_index()\n",
    "df_train.drop('index',axis=1,inplace=True)\n",
    "df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bd93f8c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert.embeddings.word_embeddings.weight\n",
      "bert.embeddings.position_embeddings.weight\n",
      "bert.embeddings.token_type_embeddings.weight\n",
      "bert.embeddings.LayerNorm.weight\n",
      "bert.embeddings.LayerNorm.bias\n",
      "bert.encoder.layer.0.attention.self.query.weight\n",
      "bert.encoder.layer.0.attention.self.query.bias\n",
      "bert.encoder.layer.0.attention.self.key.weight\n",
      "bert.encoder.layer.0.attention.self.key.bias\n",
      "bert.encoder.layer.0.attention.self.value.weight\n",
      "bert.encoder.layer.0.attention.self.value.bias\n",
      "bert.encoder.layer.0.attention.output.dense.weight\n",
      "bert.encoder.layer.0.attention.output.dense.bias\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.weight\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.bias\n",
      "bert.encoder.layer.0.intermediate.dense.weight\n",
      "bert.encoder.layer.0.intermediate.dense.bias\n",
      "bert.encoder.layer.0.output.dense.weight\n",
      "bert.encoder.layer.0.output.dense.bias\n",
      "bert.encoder.layer.0.output.LayerNorm.weight\n",
      "bert.encoder.layer.0.output.LayerNorm.bias\n",
      "bert.encoder.layer.1.attention.self.query.weight\n",
      "bert.encoder.layer.1.attention.self.query.bias\n",
      "bert.encoder.layer.1.attention.self.key.weight\n",
      "bert.encoder.layer.1.attention.self.key.bias\n",
      "bert.encoder.layer.1.attention.self.value.weight\n",
      "bert.encoder.layer.1.attention.self.value.bias\n",
      "bert.encoder.layer.1.attention.output.dense.weight\n",
      "bert.encoder.layer.1.attention.output.dense.bias\n",
      "bert.encoder.layer.1.attention.output.LayerNorm.weight\n",
      "bert.encoder.layer.1.attention.output.LayerNorm.bias\n",
      "bert.encoder.layer.1.intermediate.dense.weight\n",
      "bert.encoder.layer.1.intermediate.dense.bias\n",
      "bert.encoder.layer.1.output.dense.weight\n",
      "bert.encoder.layer.1.output.dense.bias\n",
      "bert.encoder.layer.1.output.LayerNorm.weight\n",
      "bert.encoder.layer.1.output.LayerNorm.bias\n",
      "bert.encoder.layer.2.attention.self.query.weight\n",
      "bert.encoder.layer.2.attention.self.query.bias\n",
      "bert.encoder.layer.2.attention.self.key.weight\n",
      "bert.encoder.layer.2.attention.self.key.bias\n",
      "bert.encoder.layer.2.attention.self.value.weight\n",
      "bert.encoder.layer.2.attention.self.value.bias\n",
      "bert.encoder.layer.2.attention.output.dense.weight\n",
      "bert.encoder.layer.2.attention.output.dense.bias\n",
      "bert.encoder.layer.2.attention.output.LayerNorm.weight\n",
      "bert.encoder.layer.2.attention.output.LayerNorm.bias\n",
      "bert.encoder.layer.2.intermediate.dense.weight\n",
      "bert.encoder.layer.2.intermediate.dense.bias\n",
      "bert.encoder.layer.2.output.dense.weight\n",
      "bert.encoder.layer.2.output.dense.bias\n",
      "bert.encoder.layer.2.output.LayerNorm.weight\n",
      "bert.encoder.layer.2.output.LayerNorm.bias\n",
      "bert.encoder.layer.3.attention.self.query.weight\n",
      "bert.encoder.layer.3.attention.self.query.bias\n",
      "bert.encoder.layer.3.attention.self.key.weight\n",
      "bert.encoder.layer.3.attention.self.key.bias\n",
      "bert.encoder.layer.3.attention.self.value.weight\n",
      "bert.encoder.layer.3.attention.self.value.bias\n",
      "bert.encoder.layer.3.attention.output.dense.weight\n",
      "bert.encoder.layer.3.attention.output.dense.bias\n",
      "bert.encoder.layer.3.attention.output.LayerNorm.weight\n",
      "bert.encoder.layer.3.attention.output.LayerNorm.bias\n",
      "bert.encoder.layer.3.intermediate.dense.weight\n",
      "bert.encoder.layer.3.intermediate.dense.bias\n",
      "bert.encoder.layer.3.output.dense.weight\n",
      "bert.encoder.layer.3.output.dense.bias\n",
      "bert.encoder.layer.3.output.LayerNorm.weight\n",
      "bert.encoder.layer.3.output.LayerNorm.bias\n",
      "bert.encoder.layer.4.attention.self.query.weight\n",
      "bert.encoder.layer.4.attention.self.query.bias\n",
      "bert.encoder.layer.4.attention.self.key.weight\n",
      "bert.encoder.layer.4.attention.self.key.bias\n",
      "bert.encoder.layer.4.attention.self.value.weight\n",
      "bert.encoder.layer.4.attention.self.value.bias\n",
      "bert.encoder.layer.4.attention.output.dense.weight\n",
      "bert.encoder.layer.4.attention.output.dense.bias\n",
      "bert.encoder.layer.4.attention.output.LayerNorm.weight\n",
      "bert.encoder.layer.4.attention.output.LayerNorm.bias\n",
      "bert.encoder.layer.4.intermediate.dense.weight\n",
      "bert.encoder.layer.4.intermediate.dense.bias\n",
      "bert.encoder.layer.4.output.dense.weight\n",
      "bert.encoder.layer.4.output.dense.bias\n",
      "bert.encoder.layer.4.output.LayerNorm.weight\n",
      "bert.encoder.layer.4.output.LayerNorm.bias\n",
      "bert.encoder.layer.5.attention.self.query.weight\n",
      "bert.encoder.layer.5.attention.self.query.bias\n",
      "bert.encoder.layer.5.attention.self.key.weight\n",
      "bert.encoder.layer.5.attention.self.key.bias\n",
      "bert.encoder.layer.5.attention.self.value.weight\n",
      "bert.encoder.layer.5.attention.self.value.bias\n",
      "bert.encoder.layer.5.attention.output.dense.weight\n",
      "bert.encoder.layer.5.attention.output.dense.bias\n",
      "bert.encoder.layer.5.attention.output.LayerNorm.weight\n",
      "bert.encoder.layer.5.attention.output.LayerNorm.bias\n",
      "bert.encoder.layer.5.intermediate.dense.weight\n",
      "bert.encoder.layer.5.intermediate.dense.bias\n",
      "bert.encoder.layer.5.output.dense.weight\n",
      "bert.encoder.layer.5.output.dense.bias\n",
      "bert.encoder.layer.5.output.LayerNorm.weight\n",
      "bert.encoder.layer.5.output.LayerNorm.bias\n",
      "bert.encoder.layer.6.attention.self.query.weight\n",
      "bert.encoder.layer.6.attention.self.query.bias\n",
      "bert.encoder.layer.6.attention.self.key.weight\n",
      "bert.encoder.layer.6.attention.self.key.bias\n",
      "bert.encoder.layer.6.attention.self.value.weight\n",
      "bert.encoder.layer.6.attention.self.value.bias\n",
      "bert.encoder.layer.6.attention.output.dense.weight\n",
      "bert.encoder.layer.6.attention.output.dense.bias\n",
      "bert.encoder.layer.6.attention.output.LayerNorm.weight\n",
      "bert.encoder.layer.6.attention.output.LayerNorm.bias\n",
      "bert.encoder.layer.6.intermediate.dense.weight\n",
      "bert.encoder.layer.6.intermediate.dense.bias\n",
      "bert.encoder.layer.6.output.dense.weight\n",
      "bert.encoder.layer.6.output.dense.bias\n",
      "bert.encoder.layer.6.output.LayerNorm.weight\n",
      "bert.encoder.layer.6.output.LayerNorm.bias\n",
      "bert.encoder.layer.7.attention.self.query.weight\n",
      "bert.encoder.layer.7.attention.self.query.bias\n",
      "bert.encoder.layer.7.attention.self.key.weight\n",
      "bert.encoder.layer.7.attention.self.key.bias\n",
      "bert.encoder.layer.7.attention.self.value.weight\n",
      "bert.encoder.layer.7.attention.self.value.bias\n",
      "bert.encoder.layer.7.attention.output.dense.weight\n",
      "bert.encoder.layer.7.attention.output.dense.bias\n",
      "bert.encoder.layer.7.attention.output.LayerNorm.weight\n",
      "bert.encoder.layer.7.attention.output.LayerNorm.bias\n",
      "bert.encoder.layer.7.intermediate.dense.weight\n",
      "bert.encoder.layer.7.intermediate.dense.bias\n",
      "bert.encoder.layer.7.output.dense.weight\n",
      "bert.encoder.layer.7.output.dense.bias\n",
      "bert.encoder.layer.7.output.LayerNorm.weight\n",
      "bert.encoder.layer.7.output.LayerNorm.bias\n",
      "bert.encoder.layer.8.attention.self.query.weight\n",
      "bert.encoder.layer.8.attention.self.query.bias\n",
      "bert.encoder.layer.8.attention.self.key.weight\n",
      "bert.encoder.layer.8.attention.self.key.bias\n",
      "bert.encoder.layer.8.attention.self.value.weight\n",
      "bert.encoder.layer.8.attention.self.value.bias\n",
      "bert.encoder.layer.8.attention.output.dense.weight\n",
      "bert.encoder.layer.8.attention.output.dense.bias\n",
      "bert.encoder.layer.8.attention.output.LayerNorm.weight\n",
      "bert.encoder.layer.8.attention.output.LayerNorm.bias\n",
      "bert.encoder.layer.8.intermediate.dense.weight\n",
      "bert.encoder.layer.8.intermediate.dense.bias\n",
      "bert.encoder.layer.8.output.dense.weight\n",
      "bert.encoder.layer.8.output.dense.bias\n",
      "bert.encoder.layer.8.output.LayerNorm.weight\n",
      "bert.encoder.layer.8.output.LayerNorm.bias\n",
      "bert.encoder.layer.9.attention.self.query.weight\n",
      "bert.encoder.layer.9.attention.self.query.bias\n",
      "bert.encoder.layer.9.attention.self.key.weight\n",
      "bert.encoder.layer.9.attention.self.key.bias\n",
      "bert.encoder.layer.9.attention.self.value.weight\n",
      "bert.encoder.layer.9.attention.self.value.bias\n",
      "bert.encoder.layer.9.attention.output.dense.weight\n",
      "bert.encoder.layer.9.attention.output.dense.bias\n",
      "bert.encoder.layer.9.attention.output.LayerNorm.weight\n",
      "bert.encoder.layer.9.attention.output.LayerNorm.bias\n",
      "bert.encoder.layer.9.intermediate.dense.weight\n",
      "bert.encoder.layer.9.intermediate.dense.bias\n",
      "bert.encoder.layer.9.output.dense.weight\n",
      "bert.encoder.layer.9.output.dense.bias\n",
      "bert.encoder.layer.9.output.LayerNorm.weight\n",
      "bert.encoder.layer.9.output.LayerNorm.bias\n",
      "bert.encoder.layer.10.attention.self.query.weight\n",
      "bert.encoder.layer.10.attention.self.query.bias\n",
      "bert.encoder.layer.10.attention.self.key.weight\n",
      "bert.encoder.layer.10.attention.self.key.bias\n",
      "bert.encoder.layer.10.attention.self.value.weight\n",
      "bert.encoder.layer.10.attention.self.value.bias\n",
      "bert.encoder.layer.10.attention.output.dense.weight\n",
      "bert.encoder.layer.10.attention.output.dense.bias\n",
      "bert.encoder.layer.10.attention.output.LayerNorm.weight\n",
      "bert.encoder.layer.10.attention.output.LayerNorm.bias\n",
      "bert.encoder.layer.10.intermediate.dense.weight\n",
      "bert.encoder.layer.10.intermediate.dense.bias\n",
      "bert.encoder.layer.10.output.dense.weight\n",
      "bert.encoder.layer.10.output.dense.bias\n",
      "bert.encoder.layer.10.output.LayerNorm.weight\n",
      "bert.encoder.layer.10.output.LayerNorm.bias\n",
      "bert.encoder.layer.11.attention.self.query.weight\n",
      "bert.encoder.layer.11.attention.self.query.bias\n",
      "bert.encoder.layer.11.attention.self.key.weight\n",
      "bert.encoder.layer.11.attention.self.key.bias\n",
      "bert.encoder.layer.11.attention.self.value.weight\n",
      "bert.encoder.layer.11.attention.self.value.bias\n",
      "bert.encoder.layer.11.attention.output.dense.weight\n",
      "bert.encoder.layer.11.attention.output.dense.bias\n",
      "bert.encoder.layer.11.attention.output.LayerNorm.weight\n",
      "bert.encoder.layer.11.attention.output.LayerNorm.bias\n",
      "bert.encoder.layer.11.intermediate.dense.weight\n",
      "bert.encoder.layer.11.intermediate.dense.bias\n",
      "bert.encoder.layer.11.output.dense.weight\n",
      "bert.encoder.layer.11.output.dense.bias\n",
      "bert.encoder.layer.11.output.LayerNorm.weight\n",
      "bert.encoder.layer.11.output.LayerNorm.bias\n",
      "bert.pooler.dense.weight\n",
      "bert.pooler.dense.bias\n",
      "linear.weight\n",
      "linear.bias\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "201"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# display BERT layers\n",
    "n=0\n",
    "for x in model.state_dict():\n",
    "    n=n+1\n",
    "    print(x)\n",
    "n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c9734c9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# freeze first 8 layers \n",
    "n=0\n",
    "for param in model.parameters():\n",
    "    n=n+1\n",
    "    param.requires_grad = False\n",
    "    if n==(201-68):\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8a1fb41f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The article discusses a research analysis and presentation conducted by ODDO BHF in Frankfurt, Germany, aimed at acquiring potential clients. The analysis and presentation took place over seven months, from February 2023 to the present. The purpose of this initiative is to attract new clients and showcase the expertise and services offered by ODDO BHF. Frankfurt, located in the Hesse region of Germany, serves as the headquarters for this research and analysis project.'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# change datatypes of input data\n",
    "df_train['news_content']=df_train['news_content'].astype(str)\n",
    "\n",
    "df_val['news_content']=df_val['news_content'].astype(str)\n",
    "df_train['news_content'].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "72ff0c5d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "72ff0c5d",
    "outputId": "0f9a549a-7e09-4eaf-f974-3aaced2a7338"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|██████▉                                                                          | 83/973 [00:08<01:30,  9.85it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[24], line 93\u001b[0m\n\u001b[0;32m     80\u001b[0m \u001b[38;5;250m                    \u001b[39m\u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[0;32m     81\u001b[0m \u001b[38;5;124;03m                    # validation accuracy\u001b[39;00m\n\u001b[0;32m     82\u001b[0m \u001b[38;5;124;03m                    acc = (output.argmax(dim=1) == val_label).sum().item()\u001b[39;00m\n\u001b[0;32m     83\u001b[0m \u001b[38;5;124;03m                    total_acc_val += acc\u001b[39;00m\n\u001b[0;32m     84\u001b[0m \u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[0;32m     85\u001b[0m             \u001b[38;5;28mprint\u001b[39m(\n\u001b[0;32m     86\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEpochs: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch_num\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m | Train Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtotal_loss_train\u001b[38;5;250m \u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mlen\u001b[39m(train_data)\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m .3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;130;01m\\\u001b[39;00m\n\u001b[0;32m     87\u001b[0m \u001b[38;5;124m                | Train Accuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtotal_acc_train\u001b[38;5;250m \u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mlen\u001b[39m(train_data)\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m .3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;130;01m\\\u001b[39;00m\n\u001b[0;32m     88\u001b[0m \u001b[38;5;124m                | Val Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtotal_loss_val\u001b[38;5;250m \u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mlen\u001b[39m(val_data)\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m .3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;130;01m\\\u001b[39;00m\n\u001b[0;32m     89\u001b[0m \u001b[38;5;124m                | Val Accuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtotal_acc_val\u001b[38;5;250m \u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mlen\u001b[39m(val_data)\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m .3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 93\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdf_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdf_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mLR\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mEPOCHS\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[24], line 51\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(model, train_data, val_data, learning_rate, epochs)\u001b[0m\n\u001b[0;32m     49\u001b[0m \u001b[38;5;66;03m# loss value\u001b[39;00m\n\u001b[0;32m     50\u001b[0m batch_loss \u001b[38;5;241m=\u001b[39m criterion(output, train_label) \u001b[38;5;241m+\u001b[39m l1_loss\n\u001b[1;32m---> 51\u001b[0m total_loss_train \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m batch_loss\u001b[38;5;241m.\u001b[39mitem() \n\u001b[0;32m     52\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[0;32m     53\u001b[0m \u001b[38;5;124;03m# train accuracy \u001b[39;00m\n\u001b[0;32m     54\u001b[0m \u001b[38;5;124;03macc = (output.argmax(dim=1) == train_label).sum().item()\u001b[39;00m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;124;03mtotal_acc_train += acc\u001b[39;00m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;66;03m# backpropogation\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from torch.optim import Adam\n",
    "from tqdm import tqdm\n",
    "\n",
    "def train(model, train_data, val_data, learning_rate, epochs):\n",
    "\n",
    "    train, val = Dataset(train_data), Dataset(val_data)\n",
    "    \n",
    "    # mini batching\n",
    "    train_dataloader = torch.utils.data.DataLoader(train, batch_size=2)\n",
    "    val_dataloader = torch.utils.data.DataLoader(val, batch_size=30)\n",
    "\n",
    "    use_cuda = torch.cuda.is_available()\n",
    "    device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = Adam(model.parameters(), lr= learning_rate)\n",
    "    \n",
    "    if use_cuda:\n",
    "\n",
    "            model = model.cuda()\n",
    "            criterion = criterion.cuda()\n",
    "\n",
    "    for epoch_num in range(epochs):\n",
    "\n",
    "            total_acc_train = 0\n",
    "            total_loss_train = 0\n",
    "            n=0\n",
    "            for train_input, train_label in tqdm(train_dataloader):\n",
    "                train_label = train_label.to(device) # to cuda GPU\n",
    "                mask = train_input['attention_mask'].to(device) # attention mask\n",
    "                input_id = train_input['input_ids'].squeeze(1).to(device)\n",
    "                \n",
    "                l1_loss=0\n",
    "                \n",
    "                # for L1 regularization\n",
    "                a=0\n",
    "                reg_loss = 0\n",
    "                for param in model.parameters():\n",
    "                    a=a+1\n",
    "                    if a >=201-68:\n",
    "                        reg_loss += torch.norm(param, 1) \n",
    "                \n",
    "                factor = 0.00001 #lambda for L1 regularization\n",
    "                l1_loss=factor * reg_loss # L1 loss\n",
    "                \n",
    "                # model output\n",
    "                output = model(input_id, mask)\n",
    "                \n",
    "                # loss value\n",
    "                batch_loss = criterion(output, train_label) + l1_loss\n",
    "                total_loss_train += batch_loss.item() \n",
    "                '''\n",
    "                # train accuracy \n",
    "                acc = (output.argmax(dim=1) == train_label).sum().item()\n",
    "                total_acc_train += acc\n",
    "                '''\n",
    "                # backpropogation\n",
    "                model.zero_grad()\n",
    "                batch_loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "            total_acc_val = 0\n",
    "            total_loss_val = 0\n",
    "            \n",
    "            # for validation accuracy\n",
    "            with torch.no_grad():\n",
    "\n",
    "                for val_input, val_label in val_dataloader:\n",
    "\n",
    "                    val_label = val_label.to(device)\n",
    "                    mask = val_input['attention_mask'].to(device)\n",
    "                    input_id = val_input['input_ids'].squeeze(1).to(device)\n",
    "                    \n",
    "                    # validation output\n",
    "                    output = model(input_id, mask)\n",
    "                    \n",
    "                    # validation loss value\n",
    "                    batch_loss = criterion(output, val_label.long())\n",
    "                    total_loss_val += batch_loss.item()\n",
    "                    '''\n",
    "                    # validation accuracy\n",
    "                    acc = (output.argmax(dim=1) == val_label).sum().item()\n",
    "                    total_acc_val += acc\n",
    "'''\n",
    "            print(\n",
    "                f'Epochs: {epoch_num + 1} | Train Loss: {total_loss_train / len(train_data): .3f} \\\n",
    "                | Train Accuracy: {total_acc_train / len(train_data): .3f} \\\n",
    "                | Val Loss: {total_loss_val / len(val_data): .3f} \\\n",
    "                | Val Accuracy: {total_acc_val / len(val_data): .3f}')\n",
    "\n",
    "\n",
    "\n",
    "train(model, df_train, df_val, LR, EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "id": "dd7129e9",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dd7129e9",
    "outputId": "1f8c2c2f-cdf8-4752-d7dd-7044e816c573"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n%%timeit -n 1 -r 1\\ndef evaluate(model, test_data):\\n\\n    test = Dataset(test_data)\\n\\n    test_dataloader = torch.utils.data.DataLoader(test, batch_size=2)\\n\\n    use_cuda = torch.cuda.is_available()\\n    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\\n\\n    if use_cuda:\\n\\n        model = model.cuda()\\n\\n    total_acc_test = 0\\n    with torch.no_grad():\\n\\n        for test_input, test_label in test_dataloader:\\n\\n              test_label = test_label.to(device)\\n              mask = test_input[\\'attention_mask\\'].to(device)\\n              input_id = test_input[\\'input_ids\\'].squeeze(1).to(device)\\n\\n              output = model(input_id, mask)\\n\\n              acc = (output.argmax(dim=1) == test_label).sum().item()\\n              total_acc_test += acc\\n\\n    print(f\\'Test Accuracy: {total_acc_test / len(test_data): .3f}\\')\\n\\nevaluate(model, df_test)\\n'"
      ]
     },
     "execution_count": 272,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# incase test data is used\n",
    "'''\n",
    "%%timeit -n 1 -r 1\n",
    "def evaluate(model, test_data):\n",
    "\n",
    "    test = Dataset(test_data)\n",
    "\n",
    "    test_dataloader = torch.utils.data.DataLoader(test, batch_size=2)\n",
    "\n",
    "    use_cuda = torch.cuda.is_available()\n",
    "    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "\n",
    "    if use_cuda:\n",
    "\n",
    "        model = model.cuda()\n",
    "\n",
    "    total_acc_test = 0\n",
    "    with torch.no_grad():\n",
    "\n",
    "        for test_input, test_label in test_dataloader:\n",
    "\n",
    "              test_label = test_label.to(device)\n",
    "              mask = test_input['attention_mask'].to(device)\n",
    "              input_id = test_input['input_ids'].squeeze(1).to(device)\n",
    "\n",
    "              output = model(input_id, mask)\n",
    "\n",
    "              acc = (output.argmax(dim=1) == test_label).sum().item()\n",
    "              total_acc_test += acc\n",
    "\n",
    "    print(f'Test Accuracy: {total_acc_test / len(test_data): .3f}')\n",
    "\n",
    "evaluate(model, df_test)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b8f285b",
   "metadata": {
    "id": "6b8f285b"
   },
   "outputs": [],
   "source": [
    "# save model file\n",
    "torch.save(model.state_dict(), 'BERT_model_freeze.pth')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "gpu1",
   "language": "python",
   "name": "gpu1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
