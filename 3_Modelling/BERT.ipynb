{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4c5e2885",
   "metadata": {
    "id": "4c5e2885"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hp\\miniconda3\\envs\\tf_gpu\\lib\\site-packages\\scipy\\__init__.py:132: UserWarning: A NumPy version >=1.21.6 and <1.28.0 is required for this version of SciPy (detected version 1.20.0)\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "# ! pip install transformers\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from transformers import BertTokenizer\n",
    "from transformers import BertForSequenceClassification, AdamW\n",
    "from torch.optim import Adam\n",
    "\n",
    "import torchsummary as summary\n",
    "from tqdm import tqdm\n",
    "\n",
    "from torch import nn\n",
    "from transformers import BertModel\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import base64\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4351b623",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_onedrive_directdownload(onedrive_link):\n",
    "    data_bytes64 = base64.b64encode(bytes(onedrive_link, 'utf-8'))\n",
    "    data_bytes64_String = data_bytes64.decode('utf-8').replace('/','_').replace('+','-').rstrip(\"=\")\n",
    "    resultUrl = f\"https://api.onedrive.com/v1.0/shares/u!{data_bytes64_String}/root/content\"\n",
    "    return resultUrl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1b1f6bd4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>news_content</th>\n",
       "      <th>rule_labels_comb</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The article discusses how M.M. Warburg may hav...</td>\n",
       "      <td>['legal']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The Cologne Prosecutor's Office accuses German...</td>\n",
       "      <td>['collaboration', 'legal']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Deutsche Bank AG has acquired an additional 49...</td>\n",
       "      <td>['collaboration']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Mozambique has reached a negotiated agreement ...</td>\n",
       "      <td>['collaboration', 'legal']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The UBS APAC Sustainable Finance Conference 20...</td>\n",
       "      <td>['personnel', 'legal']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2589</th>\n",
       "      <td>Arbeiten bei HSBC Trinkaus &amp; Burkhardt GmbH in...</td>\n",
       "      <td>['product']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2590</th>\n",
       "      <td>HSBC muss MillionenStrafe wegen WhatsAppNutzun...</td>\n",
       "      <td>['personnel']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2591</th>\n",
       "      <td>Alle offenen Sozialkompetent Jobs bei Bethmann...</td>\n",
       "      <td>['personnel']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2592</th>\n",
       "      <td>Citigroup Inc. Deutsche Bank AG HSBC Holdings ...</td>\n",
       "      <td>['legal']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2593</th>\n",
       "      <td>The Competition and Markets Authority said Cit...</td>\n",
       "      <td>['legal']</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2594 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           news_content  \\\n",
       "0     The article discusses how M.M. Warburg may hav...   \n",
       "1     The Cologne Prosecutor's Office accuses German...   \n",
       "2     Deutsche Bank AG has acquired an additional 49...   \n",
       "3     Mozambique has reached a negotiated agreement ...   \n",
       "4     The UBS APAC Sustainable Finance Conference 20...   \n",
       "...                                                 ...   \n",
       "2589  Arbeiten bei HSBC Trinkaus & Burkhardt GmbH in...   \n",
       "2590  HSBC muss MillionenStrafe wegen WhatsAppNutzun...   \n",
       "2591  Alle offenen Sozialkompetent Jobs bei Bethmann...   \n",
       "2592  Citigroup Inc. Deutsche Bank AG HSBC Holdings ...   \n",
       "2593  The Competition and Markets Authority said Cit...   \n",
       "\n",
       "                rule_labels_comb  \n",
       "0                      ['legal']  \n",
       "1     ['collaboration', 'legal']  \n",
       "2              ['collaboration']  \n",
       "3     ['collaboration', 'legal']  \n",
       "4         ['personnel', 'legal']  \n",
       "...                          ...  \n",
       "2589                 ['product']  \n",
       "2590               ['personnel']  \n",
       "2591               ['personnel']  \n",
       "2592                   ['legal']  \n",
       "2593                   ['legal']  \n",
       "\n",
       "[2594 rows x 2 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "onedrive_link='https://1drv.ms/u/s!AoiE7xOoBAsngsgmC-PI9ray-gfELQ?e=UtffII'\n",
    "onedrive_direct_link=create_onedrive_directdownload(onedrive_link)\n",
    "onedrive_direct_link\n",
    "df=pd.read_csv(onedrive_direct_link)\n",
    "df.drop('Unnamed: 0',axis=1,inplace=True)\n",
    "df=df[['news_content','rule_labels_comb']]\n",
    "df=df[~df['rule_labels_comb'].isna()]\n",
    "df=df[(df['rule_labels_comb'].apply(len)!=2)]\n",
    "df = df.reset_index()\n",
    "del df['index']\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b5dd1747",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ast import literal_eval\n",
    "df['rule_labels_comb'] = df['rule_labels_comb'].apply(literal_eval)\n",
    "df['target']='[0.0,0.0,0.0,0.0]'\n",
    "df['target'] = df['target'].apply(literal_eval)\n",
    "\n",
    "df['target'].iloc[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7dd33e6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0          [3]\n",
       "1       [2, 3]\n",
       "2          [2]\n",
       "3       [2, 3]\n",
       "4       [0, 3]\n",
       "         ...  \n",
       "2589       [1]\n",
       "2590       [0]\n",
       "2591       [0]\n",
       "2592       [3]\n",
       "2593       [3]\n",
       "Name: rule_labels_comb, Length: 2594, dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for index, value in df['rule_labels_comb'].iteritems():\n",
    "    class_dic={'personnel':0,'product':1,'collaboration':2,'legal':3}\n",
    "    val=[class_dic.get(i, i) for i in value]\n",
    "    df['rule_labels_comb'].iloc[index]=val\n",
    "    y=0\n",
    "    for x in df['rule_labels_comb'].iloc[index]:\n",
    "        df['target'].iloc[index][x]=1.0\n",
    "        y=y+1\n",
    "df['rule_labels_comb']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b02483e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       [0.0, 0.0, 0.0, 1.0]\n",
       "1       [0.0, 0.0, 1.0, 1.0]\n",
       "2       [0.0, 0.0, 1.0, 0.0]\n",
       "3       [0.0, 0.0, 1.0, 1.0]\n",
       "4       [1.0, 0.0, 0.0, 1.0]\n",
       "                ...         \n",
       "2589    [0.0, 1.0, 0.0, 0.0]\n",
       "2590    [1.0, 0.0, 0.0, 0.0]\n",
       "2591    [1.0, 0.0, 0.0, 0.0]\n",
       "2592    [0.0, 0.0, 0.0, 1.0]\n",
       "2593    [0.0, 0.0, 0.0, 1.0]\n",
       "Name: target, Length: 2594, dtype: object"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fec05ad",
   "metadata": {
    "id": "5fec05ad"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fff40ee7",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fff40ee7",
    "outputId": "11c0fdd4-a78f-4d43-ed76-1abbd19e0f37"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nfrom google.colab import drive\\ndrive.mount('/content/drive')\\n\\nimport os\\nos.chdir('/content/drive/My Drive/')\\n\\n\\ndf=pd.read_csv('BERT_data.csv')\\ndf=df[~(df['content']=='nan')]\\ndf['content']=df['content'].astype(str)\\ndf['subject']=df['subject'].astype(str)\\n\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import data from gdrive\n",
    "'''\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "import os\n",
    "os.chdir('/content/drive/My Drive/')\n",
    "\n",
    "\n",
    "df=pd.read_csv('BERT_data.csv')\n",
    "df=df[~(df['content']=='nan')]\n",
    "df['content']=df['content'].astype(str)\n",
    "df['subject']=df['subject'].astype(str)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "862a17fe",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "id": "862a17fe",
    "outputId": "6aa6d00d-cdc8-49ac-b34e-38be032d4249"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>news_content</th>\n",
       "      <th>rule_labels_comb</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The article discusses how M.M. Warburg may hav...</td>\n",
       "      <td>[3]</td>\n",
       "      <td>[0.0, 0.0, 0.0, 1.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The Cologne Prosecutor's Office accuses German...</td>\n",
       "      <td>[2, 3]</td>\n",
       "      <td>[0.0, 0.0, 1.0, 1.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Deutsche Bank AG has acquired an additional 49...</td>\n",
       "      <td>[2]</td>\n",
       "      <td>[0.0, 0.0, 1.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Mozambique has reached a negotiated agreement ...</td>\n",
       "      <td>[2, 3]</td>\n",
       "      <td>[0.0, 0.0, 1.0, 1.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The UBS APAC Sustainable Finance Conference 20...</td>\n",
       "      <td>[0, 3]</td>\n",
       "      <td>[1.0, 0.0, 0.0, 1.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2589</th>\n",
       "      <td>Arbeiten bei HSBC Trinkaus &amp; Burkhardt GmbH in...</td>\n",
       "      <td>[1]</td>\n",
       "      <td>[0.0, 1.0, 0.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2590</th>\n",
       "      <td>HSBC muss MillionenStrafe wegen WhatsAppNutzun...</td>\n",
       "      <td>[0]</td>\n",
       "      <td>[1.0, 0.0, 0.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2591</th>\n",
       "      <td>Alle offenen Sozialkompetent Jobs bei Bethmann...</td>\n",
       "      <td>[0]</td>\n",
       "      <td>[1.0, 0.0, 0.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2592</th>\n",
       "      <td>Citigroup Inc. Deutsche Bank AG HSBC Holdings ...</td>\n",
       "      <td>[3]</td>\n",
       "      <td>[0.0, 0.0, 0.0, 1.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2593</th>\n",
       "      <td>The Competition and Markets Authority said Cit...</td>\n",
       "      <td>[3]</td>\n",
       "      <td>[0.0, 0.0, 0.0, 1.0]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2594 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           news_content rule_labels_comb  \\\n",
       "0     The article discusses how M.M. Warburg may hav...              [3]   \n",
       "1     The Cologne Prosecutor's Office accuses German...           [2, 3]   \n",
       "2     Deutsche Bank AG has acquired an additional 49...              [2]   \n",
       "3     Mozambique has reached a negotiated agreement ...           [2, 3]   \n",
       "4     The UBS APAC Sustainable Finance Conference 20...           [0, 3]   \n",
       "...                                                 ...              ...   \n",
       "2589  Arbeiten bei HSBC Trinkaus & Burkhardt GmbH in...              [1]   \n",
       "2590  HSBC muss MillionenStrafe wegen WhatsAppNutzun...              [0]   \n",
       "2591  Alle offenen Sozialkompetent Jobs bei Bethmann...              [0]   \n",
       "2592  Citigroup Inc. Deutsche Bank AG HSBC Holdings ...              [3]   \n",
       "2593  The Competition and Markets Authority said Cit...              [3]   \n",
       "\n",
       "                    target  \n",
       "0     [0.0, 0.0, 0.0, 1.0]  \n",
       "1     [0.0, 0.0, 1.0, 1.0]  \n",
       "2     [0.0, 0.0, 1.0, 0.0]  \n",
       "3     [0.0, 0.0, 1.0, 1.0]  \n",
       "4     [1.0, 0.0, 0.0, 1.0]  \n",
       "...                    ...  \n",
       "2589  [0.0, 1.0, 0.0, 0.0]  \n",
       "2590  [1.0, 0.0, 0.0, 0.0]  \n",
       "2591  [1.0, 0.0, 0.0, 0.0]  \n",
       "2592  [0.0, 0.0, 0.0, 1.0]  \n",
       "2593  [0.0, 0.0, 0.0, 1.0]  \n",
       "\n",
       "[2594 rows x 3 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# change range of labels, minimum should be zero\n",
    "df['rule_labels_comb']=df['rule_labels_comb'].astype(str)\n",
    "# df['target']=df['target'].apply(lambda x: list(np.array(x) / sum(x)))\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d761d486",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b2181d08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\ndf_3=df[(df['result']==3) & (df['content'].str.len()<350)]\\ndf=df[~(df['result']==3)]\\ndf=pd.concat([df,df_3])\\ndf\\n\""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# additional filtering to balance classes\n",
    "'''\n",
    "df_3=df[(df['result']==3) & (df['content'].str.len()<350)]\n",
    "df=df[~(df['result']==3)]\n",
    "df=pd.concat([df,df_3])\n",
    "df\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "97659d46",
   "metadata": {
    "id": "97659d46"
   },
   "outputs": [],
   "source": [
    "# import BERT tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "class Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self,df):\n",
    "        self.labels=df['target']\n",
    "        self.text=[tokenizer(text,padding='max_length',truncation=True,return_tensors=\"pt\") for text in df['news_content']]\n",
    "\n",
    "    def classes(self):\n",
    "        return self.labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def get_batch_labels(self, idx):\n",
    "        # Fetch a batch of labels\n",
    "        return np.array(self.labels[idx])\n",
    "\n",
    "    def get_batch_texts(self, idx):\n",
    "        # Fetch a batch of inputs\n",
    "        return self.text[idx]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        batch_texts = self.get_batch_texts(idx)\n",
    "        batch_y = self.get_batch_labels(idx)\n",
    "\n",
    "        return batch_texts, batch_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8a1a835a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train test split\n",
    "X_train, X_test= train_test_split(df[['news_content','target']], test_size=0.25,stratify=df['target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2f08669a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train=X_train\n",
    "df_val=X_test\n",
    "df_test=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "yDzCUBywaqK_",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yDzCUBywaqK_",
    "outputId": "f0c830cc-e9c2-4e35-8c9f-e7b0e6f9f5aa"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>news_content</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>415</th>\n",
       "      <td>According to Berenberg Bank's Chief Economist,...</td>\n",
       "      <td>[0.0, 1.0, 0.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>739</th>\n",
       "      <td>Oddo BHF, a financial services company, has ma...</td>\n",
       "      <td>[0.0, 1.0, 0.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>630</th>\n",
       "      <td>Oddo BHF, a German financial services company,...</td>\n",
       "      <td>[0.0, 1.0, 0.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2335</th>\n",
       "      <td>Major Wall Street banks must face a lawsuit ac...</td>\n",
       "      <td>[0.0, 0.0, 1.0, 1.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1712</th>\n",
       "      <td>German banking company Commerzbank is set to r...</td>\n",
       "      <td>[1.0, 1.0, 0.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1709</th>\n",
       "      <td>Analysts at Deutsche Bank have raised their pr...</td>\n",
       "      <td>[1.0, 0.0, 0.0, 1.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1536</th>\n",
       "      <td>According to a report from finews.ch, Wafik Be...</td>\n",
       "      <td>[0.0, 1.0, 1.0, 1.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2262</th>\n",
       "      <td>Two senior investment bankers at Credit Suisse...</td>\n",
       "      <td>[0.0, 0.0, 1.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2304</th>\n",
       "      <td>The DAX had a sluggish start on Fronleichnam h...</td>\n",
       "      <td>[0.0, 0.0, 1.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1344</th>\n",
       "      <td>The article discusses the potential risk of ex...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 1.0]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1945 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           news_content                target\n",
       "415   According to Berenberg Bank's Chief Economist,...  [0.0, 1.0, 0.0, 0.0]\n",
       "739   Oddo BHF, a financial services company, has ma...  [0.0, 1.0, 0.0, 0.0]\n",
       "630   Oddo BHF, a German financial services company,...  [0.0, 1.0, 0.0, 0.0]\n",
       "2335  Major Wall Street banks must face a lawsuit ac...  [0.0, 0.0, 1.0, 1.0]\n",
       "1712  German banking company Commerzbank is set to r...  [1.0, 1.0, 0.0, 0.0]\n",
       "...                                                 ...                   ...\n",
       "1709  Analysts at Deutsche Bank have raised their pr...  [1.0, 0.0, 0.0, 1.0]\n",
       "1536  According to a report from finews.ch, Wafik Be...  [0.0, 1.0, 1.0, 1.0]\n",
       "2262  Two senior investment bankers at Credit Suisse...  [0.0, 0.0, 1.0, 0.0]\n",
       "2304  The DAX had a sluggish start on Fronleichnam h...  [0.0, 0.0, 1.0, 0.0]\n",
       "1344  The article discusses the potential risk of ex...  [0.0, 0.0, 0.0, 1.0]\n",
       "\n",
       "[1945 rows x 2 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0cedf1b1",
   "metadata": {
    "id": "0cedf1b1"
   },
   "outputs": [],
   "source": [
    "# BERT classifier architecture, with 7 output classes\n",
    "class BertClassifier(nn.Module):\n",
    "\n",
    "    def __init__(self, dropout=0.5):\n",
    "\n",
    "        super(BertClassifier, self).__init__()\n",
    "\n",
    "        self.bert = BertModel.from_pretrained('bert-base-uncased')\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.linear = nn.Linear(768, 4)\n",
    "        torch.nn.init.kaiming_uniform_(self.linear.weight, nonlinearity='relu')\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def classes(self):\n",
    "        return self.labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def get_batch_labels(self, idx):\n",
    "        # Fetch a batch of labels\n",
    "        return np.array(self.labels[idx])\n",
    "\n",
    "    def get_batch_texts(self, idx):\n",
    "        # Fetch a batch of inputs\n",
    "        return self.text[idx]\n",
    "\n",
    "    def forward(self, input_id, mask):\n",
    "\n",
    "        _, pooled_output = self.bert(input_ids= input_id, attention_mask=mask,return_dict=False)\n",
    "        dropout_output = self.dropout(pooled_output)\n",
    "        linear_output = self.linear(dropout_output)\n",
    "        final_layer = self.relu(linear_output)\n",
    "\n",
    "        return final_layer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "50e427bd",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "50e427bd",
    "outputId": "00e951a8-0497-4f1f-d04e-7d941be41a07"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# change runtype to GPU\n",
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "383e6dad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "EPOCHS = 22\n",
    "model = BertClassifier()\n",
    "LR = 0.00006"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "39d24417",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>news_content</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>&lt;br&gt;1. German companies, including KGaA Helm A...</td>\n",
       "      <td>[1.0, 1.0, 1.0, 1.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>In dem Artikel wird ÃƒÂ¼ber den Prozess gegen Ch...</td>\n",
       "      <td>[0.0, 1.0, 1.0, 1.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The article is about regulatory filings made b...</td>\n",
       "      <td>[0.0, 0.0, 1.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The article discusses a court showdown between...</td>\n",
       "      <td>[0.0, 0.0, 1.0, 1.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>German-based Berenberg Bank has set a target p...</td>\n",
       "      <td>[0.0, 1.0, 0.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>644</th>\n",
       "      <td>Fitch Ratings has affirmed the BBB long-term i...</td>\n",
       "      <td>[0.0, 0.0, 1.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>645</th>\n",
       "      <td>UBS, a Swiss investment bank, is applying its ...</td>\n",
       "      <td>[0.0, 1.0, 0.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>646</th>\n",
       "      <td>This German article discusses the downward tre...</td>\n",
       "      <td>[1.0, 0.0, 0.0, 1.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>647</th>\n",
       "      <td>The article discusses how Swiss cantonal banks...</td>\n",
       "      <td>[0.0, 0.0, 1.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>648</th>\n",
       "      <td>The article describes a job opening for a (Sen...</td>\n",
       "      <td>[1.0, 0.0, 0.0, 0.0]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>649 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          news_content                target\n",
       "0    <br>1. German companies, including KGaA Helm A...  [1.0, 1.0, 1.0, 1.0]\n",
       "1    In dem Artikel wird ÃƒÂ¼ber den Prozess gegen Ch...  [0.0, 1.0, 1.0, 1.0]\n",
       "2    The article is about regulatory filings made b...  [0.0, 0.0, 1.0, 0.0]\n",
       "3    The article discusses a court showdown between...  [0.0, 0.0, 1.0, 1.0]\n",
       "4    German-based Berenberg Bank has set a target p...  [0.0, 1.0, 0.0, 0.0]\n",
       "..                                                 ...                   ...\n",
       "644  Fitch Ratings has affirmed the BBB long-term i...  [0.0, 0.0, 1.0, 0.0]\n",
       "645  UBS, a Swiss investment bank, is applying its ...  [0.0, 1.0, 0.0, 0.0]\n",
       "646  This German article discusses the downward tre...  [1.0, 0.0, 0.0, 1.0]\n",
       "647  The article discusses how Swiss cantonal banks...  [0.0, 0.0, 1.0, 0.0]\n",
       "648  The article describes a job opening for a (Sen...  [1.0, 0.0, 0.0, 0.0]\n",
       "\n",
       "[649 rows x 2 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train=df_train.reset_index()\n",
    "df_train.drop('index',axis=1,inplace=True)\n",
    "\n",
    "df_val=df_val.reset_index()\n",
    "df_val.drop('index',axis=1,inplace=True)\n",
    "df_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bd93f8c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert.embeddings.word_embeddings.weight\n",
      "bert.embeddings.position_embeddings.weight\n",
      "bert.embeddings.token_type_embeddings.weight\n",
      "bert.embeddings.LayerNorm.weight\n",
      "bert.embeddings.LayerNorm.bias\n",
      "bert.encoder.layer.0.attention.self.query.weight\n",
      "bert.encoder.layer.0.attention.self.query.bias\n",
      "bert.encoder.layer.0.attention.self.key.weight\n",
      "bert.encoder.layer.0.attention.self.key.bias\n",
      "bert.encoder.layer.0.attention.self.value.weight\n",
      "bert.encoder.layer.0.attention.self.value.bias\n",
      "bert.encoder.layer.0.attention.output.dense.weight\n",
      "bert.encoder.layer.0.attention.output.dense.bias\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.weight\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.bias\n",
      "bert.encoder.layer.0.intermediate.dense.weight\n",
      "bert.encoder.layer.0.intermediate.dense.bias\n",
      "bert.encoder.layer.0.output.dense.weight\n",
      "bert.encoder.layer.0.output.dense.bias\n",
      "bert.encoder.layer.0.output.LayerNorm.weight\n",
      "bert.encoder.layer.0.output.LayerNorm.bias\n",
      "bert.encoder.layer.1.attention.self.query.weight\n",
      "bert.encoder.layer.1.attention.self.query.bias\n",
      "bert.encoder.layer.1.attention.self.key.weight\n",
      "bert.encoder.layer.1.attention.self.key.bias\n",
      "bert.encoder.layer.1.attention.self.value.weight\n",
      "bert.encoder.layer.1.attention.self.value.bias\n",
      "bert.encoder.layer.1.attention.output.dense.weight\n",
      "bert.encoder.layer.1.attention.output.dense.bias\n",
      "bert.encoder.layer.1.attention.output.LayerNorm.weight\n",
      "bert.encoder.layer.1.attention.output.LayerNorm.bias\n",
      "bert.encoder.layer.1.intermediate.dense.weight\n",
      "bert.encoder.layer.1.intermediate.dense.bias\n",
      "bert.encoder.layer.1.output.dense.weight\n",
      "bert.encoder.layer.1.output.dense.bias\n",
      "bert.encoder.layer.1.output.LayerNorm.weight\n",
      "bert.encoder.layer.1.output.LayerNorm.bias\n",
      "bert.encoder.layer.2.attention.self.query.weight\n",
      "bert.encoder.layer.2.attention.self.query.bias\n",
      "bert.encoder.layer.2.attention.self.key.weight\n",
      "bert.encoder.layer.2.attention.self.key.bias\n",
      "bert.encoder.layer.2.attention.self.value.weight\n",
      "bert.encoder.layer.2.attention.self.value.bias\n",
      "bert.encoder.layer.2.attention.output.dense.weight\n",
      "bert.encoder.layer.2.attention.output.dense.bias\n",
      "bert.encoder.layer.2.attention.output.LayerNorm.weight\n",
      "bert.encoder.layer.2.attention.output.LayerNorm.bias\n",
      "bert.encoder.layer.2.intermediate.dense.weight\n",
      "bert.encoder.layer.2.intermediate.dense.bias\n",
      "bert.encoder.layer.2.output.dense.weight\n",
      "bert.encoder.layer.2.output.dense.bias\n",
      "bert.encoder.layer.2.output.LayerNorm.weight\n",
      "bert.encoder.layer.2.output.LayerNorm.bias\n",
      "bert.encoder.layer.3.attention.self.query.weight\n",
      "bert.encoder.layer.3.attention.self.query.bias\n",
      "bert.encoder.layer.3.attention.self.key.weight\n",
      "bert.encoder.layer.3.attention.self.key.bias\n",
      "bert.encoder.layer.3.attention.self.value.weight\n",
      "bert.encoder.layer.3.attention.self.value.bias\n",
      "bert.encoder.layer.3.attention.output.dense.weight\n",
      "bert.encoder.layer.3.attention.output.dense.bias\n",
      "bert.encoder.layer.3.attention.output.LayerNorm.weight\n",
      "bert.encoder.layer.3.attention.output.LayerNorm.bias\n",
      "bert.encoder.layer.3.intermediate.dense.weight\n",
      "bert.encoder.layer.3.intermediate.dense.bias\n",
      "bert.encoder.layer.3.output.dense.weight\n",
      "bert.encoder.layer.3.output.dense.bias\n",
      "bert.encoder.layer.3.output.LayerNorm.weight\n",
      "bert.encoder.layer.3.output.LayerNorm.bias\n",
      "bert.encoder.layer.4.attention.self.query.weight\n",
      "bert.encoder.layer.4.attention.self.query.bias\n",
      "bert.encoder.layer.4.attention.self.key.weight\n",
      "bert.encoder.layer.4.attention.self.key.bias\n",
      "bert.encoder.layer.4.attention.self.value.weight\n",
      "bert.encoder.layer.4.attention.self.value.bias\n",
      "bert.encoder.layer.4.attention.output.dense.weight\n",
      "bert.encoder.layer.4.attention.output.dense.bias\n",
      "bert.encoder.layer.4.attention.output.LayerNorm.weight\n",
      "bert.encoder.layer.4.attention.output.LayerNorm.bias\n",
      "bert.encoder.layer.4.intermediate.dense.weight\n",
      "bert.encoder.layer.4.intermediate.dense.bias\n",
      "bert.encoder.layer.4.output.dense.weight\n",
      "bert.encoder.layer.4.output.dense.bias\n",
      "bert.encoder.layer.4.output.LayerNorm.weight\n",
      "bert.encoder.layer.4.output.LayerNorm.bias\n",
      "bert.encoder.layer.5.attention.self.query.weight\n",
      "bert.encoder.layer.5.attention.self.query.bias\n",
      "bert.encoder.layer.5.attention.self.key.weight\n",
      "bert.encoder.layer.5.attention.self.key.bias\n",
      "bert.encoder.layer.5.attention.self.value.weight\n",
      "bert.encoder.layer.5.attention.self.value.bias\n",
      "bert.encoder.layer.5.attention.output.dense.weight\n",
      "bert.encoder.layer.5.attention.output.dense.bias\n",
      "bert.encoder.layer.5.attention.output.LayerNorm.weight\n",
      "bert.encoder.layer.5.attention.output.LayerNorm.bias\n",
      "bert.encoder.layer.5.intermediate.dense.weight\n",
      "bert.encoder.layer.5.intermediate.dense.bias\n",
      "bert.encoder.layer.5.output.dense.weight\n",
      "bert.encoder.layer.5.output.dense.bias\n",
      "bert.encoder.layer.5.output.LayerNorm.weight\n",
      "bert.encoder.layer.5.output.LayerNorm.bias\n",
      "bert.encoder.layer.6.attention.self.query.weight\n",
      "bert.encoder.layer.6.attention.self.query.bias\n",
      "bert.encoder.layer.6.attention.self.key.weight\n",
      "bert.encoder.layer.6.attention.self.key.bias\n",
      "bert.encoder.layer.6.attention.self.value.weight\n",
      "bert.encoder.layer.6.attention.self.value.bias\n",
      "bert.encoder.layer.6.attention.output.dense.weight\n",
      "bert.encoder.layer.6.attention.output.dense.bias\n",
      "bert.encoder.layer.6.attention.output.LayerNorm.weight\n",
      "bert.encoder.layer.6.attention.output.LayerNorm.bias\n",
      "bert.encoder.layer.6.intermediate.dense.weight\n",
      "bert.encoder.layer.6.intermediate.dense.bias\n",
      "bert.encoder.layer.6.output.dense.weight\n",
      "bert.encoder.layer.6.output.dense.bias\n",
      "bert.encoder.layer.6.output.LayerNorm.weight\n",
      "bert.encoder.layer.6.output.LayerNorm.bias\n",
      "bert.encoder.layer.7.attention.self.query.weight\n",
      "bert.encoder.layer.7.attention.self.query.bias\n",
      "bert.encoder.layer.7.attention.self.key.weight\n",
      "bert.encoder.layer.7.attention.self.key.bias\n",
      "bert.encoder.layer.7.attention.self.value.weight\n",
      "bert.encoder.layer.7.attention.self.value.bias\n",
      "bert.encoder.layer.7.attention.output.dense.weight\n",
      "bert.encoder.layer.7.attention.output.dense.bias\n",
      "bert.encoder.layer.7.attention.output.LayerNorm.weight\n",
      "bert.encoder.layer.7.attention.output.LayerNorm.bias\n",
      "bert.encoder.layer.7.intermediate.dense.weight\n",
      "bert.encoder.layer.7.intermediate.dense.bias\n",
      "bert.encoder.layer.7.output.dense.weight\n",
      "bert.encoder.layer.7.output.dense.bias\n",
      "bert.encoder.layer.7.output.LayerNorm.weight\n",
      "bert.encoder.layer.7.output.LayerNorm.bias\n",
      "bert.encoder.layer.8.attention.self.query.weight\n",
      "bert.encoder.layer.8.attention.self.query.bias\n",
      "bert.encoder.layer.8.attention.self.key.weight\n",
      "bert.encoder.layer.8.attention.self.key.bias\n",
      "bert.encoder.layer.8.attention.self.value.weight\n",
      "bert.encoder.layer.8.attention.self.value.bias\n",
      "bert.encoder.layer.8.attention.output.dense.weight\n",
      "bert.encoder.layer.8.attention.output.dense.bias\n",
      "bert.encoder.layer.8.attention.output.LayerNorm.weight\n",
      "bert.encoder.layer.8.attention.output.LayerNorm.bias\n",
      "bert.encoder.layer.8.intermediate.dense.weight\n",
      "bert.encoder.layer.8.intermediate.dense.bias\n",
      "bert.encoder.layer.8.output.dense.weight\n",
      "bert.encoder.layer.8.output.dense.bias\n",
      "bert.encoder.layer.8.output.LayerNorm.weight\n",
      "bert.encoder.layer.8.output.LayerNorm.bias\n",
      "bert.encoder.layer.9.attention.self.query.weight\n",
      "bert.encoder.layer.9.attention.self.query.bias\n",
      "bert.encoder.layer.9.attention.self.key.weight\n",
      "bert.encoder.layer.9.attention.self.key.bias\n",
      "bert.encoder.layer.9.attention.self.value.weight\n",
      "bert.encoder.layer.9.attention.self.value.bias\n",
      "bert.encoder.layer.9.attention.output.dense.weight\n",
      "bert.encoder.layer.9.attention.output.dense.bias\n",
      "bert.encoder.layer.9.attention.output.LayerNorm.weight\n",
      "bert.encoder.layer.9.attention.output.LayerNorm.bias\n",
      "bert.encoder.layer.9.intermediate.dense.weight\n",
      "bert.encoder.layer.9.intermediate.dense.bias\n",
      "bert.encoder.layer.9.output.dense.weight\n",
      "bert.encoder.layer.9.output.dense.bias\n",
      "bert.encoder.layer.9.output.LayerNorm.weight\n",
      "bert.encoder.layer.9.output.LayerNorm.bias\n",
      "bert.encoder.layer.10.attention.self.query.weight\n",
      "bert.encoder.layer.10.attention.self.query.bias\n",
      "bert.encoder.layer.10.attention.self.key.weight\n",
      "bert.encoder.layer.10.attention.self.key.bias\n",
      "bert.encoder.layer.10.attention.self.value.weight\n",
      "bert.encoder.layer.10.attention.self.value.bias\n",
      "bert.encoder.layer.10.attention.output.dense.weight\n",
      "bert.encoder.layer.10.attention.output.dense.bias\n",
      "bert.encoder.layer.10.attention.output.LayerNorm.weight\n",
      "bert.encoder.layer.10.attention.output.LayerNorm.bias\n",
      "bert.encoder.layer.10.intermediate.dense.weight\n",
      "bert.encoder.layer.10.intermediate.dense.bias\n",
      "bert.encoder.layer.10.output.dense.weight\n",
      "bert.encoder.layer.10.output.dense.bias\n",
      "bert.encoder.layer.10.output.LayerNorm.weight\n",
      "bert.encoder.layer.10.output.LayerNorm.bias\n",
      "bert.encoder.layer.11.attention.self.query.weight\n",
      "bert.encoder.layer.11.attention.self.query.bias\n",
      "bert.encoder.layer.11.attention.self.key.weight\n",
      "bert.encoder.layer.11.attention.self.key.bias\n",
      "bert.encoder.layer.11.attention.self.value.weight\n",
      "bert.encoder.layer.11.attention.self.value.bias\n",
      "bert.encoder.layer.11.attention.output.dense.weight\n",
      "bert.encoder.layer.11.attention.output.dense.bias\n",
      "bert.encoder.layer.11.attention.output.LayerNorm.weight\n",
      "bert.encoder.layer.11.attention.output.LayerNorm.bias\n",
      "bert.encoder.layer.11.intermediate.dense.weight\n",
      "bert.encoder.layer.11.intermediate.dense.bias\n",
      "bert.encoder.layer.11.output.dense.weight\n",
      "bert.encoder.layer.11.output.dense.bias\n",
      "bert.encoder.layer.11.output.LayerNorm.weight\n",
      "bert.encoder.layer.11.output.LayerNorm.bias\n",
      "bert.pooler.dense.weight\n",
      "bert.pooler.dense.bias\n",
      "linear.weight\n",
      "linear.bias\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "201"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# display BERT layers\n",
    "n=0\n",
    "for x in model.state_dict():\n",
    "    n=n+1\n",
    "    print(x)\n",
    "n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c9734c9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# freeze first 8 layers \n",
    "n=0\n",
    "for param in model.parameters():\n",
    "    n=n+1\n",
    "    param.requires_grad = False\n",
    "    if n==(201-68):\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8a1fb41f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"According to Berenberg Bank's Chief Economist, Holger Schmieding, global trade is expected to become less globalized in the future, shifting towards regionalization. This transformation will involve a shift from goods trade to services trade, which could benefit countries like the United States and India, who specialize in services.\""
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# change datatypes of input data\n",
    "df_train['news_content']=df_train['news_content'].astype(str)\n",
    "\n",
    "df_val['news_content']=df_val['news_content'].astype(str)\n",
    "df_train['news_content'].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1dd80045",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "27ed3838",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "def accuracy_calc(l):\n",
    "    p=l.tolist()\n",
    "    out=[]\n",
    "    for a in p:\n",
    "        \n",
    "        b={}\n",
    "        for i,x in enumerate(a):\n",
    "            b[i]=x\n",
    "        sorted_dict = dict(sorted(b.items(), key=lambda item: item[1], reverse=True))\n",
    "        copy_dic=sorted_dict\n",
    "        copy_dic = copy.deepcopy(sorted_dict)\n",
    "        su=0\n",
    "        for p,q in sorted_dict.items():\n",
    "            su=su+q\n",
    "            copy_dic[p]=1\n",
    "            if su>sum(list(sorted_dict.values()))*.8:\n",
    "                break\n",
    "        l=list(dict(sorted(copy_dic.items(), key=lambda item: item[0])).values())\n",
    "        out.append([0 if x != 1 else x for x in l])    \n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "72ff0c5d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "72ff0c5d",
    "outputId": "0f9a549a-7e09-4eaf-f974-3aaced2a7338"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 65/65 [01:58<00:00,  1.83s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 1 | Train Loss:  0.074                 | Train Accuracy:  0.430                 | Val Loss:  0.074                 | Val Accuracy:  0.159\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 65/65 [01:58<00:00,  1.82s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 2 | Train Loss:  0.071                 | Train Accuracy:  0.545                 | Val Loss:  0.070                 | Val Accuracy:  0.299\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 65/65 [01:58<00:00,  1.82s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 3 | Train Loss:  0.065                 | Train Accuracy:  0.708                 | Val Loss:  0.062                 | Val Accuracy:  0.391\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 65/65 [01:59<00:00,  1.84s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 4 | Train Loss:  0.055                 | Train Accuracy:  0.865                 | Val Loss:  0.055                 | Val Accuracy:  0.450\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 65/65 [02:02<00:00,  1.88s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 5 | Train Loss:  0.048                 | Train Accuracy:  0.925                 | Val Loss:  0.055                 | Val Accuracy:  0.368\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                                                                          | 7/65 [00:13<01:52,  1.95s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[24], line 113\u001b[0m\n\u001b[0;32m    102\u001b[0m                 \u001b[38;5;66;03m# print(output)\u001b[39;00m\n\u001b[0;32m    103\u001b[0m                 \u001b[38;5;66;03m# print(val_label)\u001b[39;00m\n\u001b[0;32m    105\u001b[0m             \u001b[38;5;28mprint\u001b[39m(\n\u001b[0;32m    106\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEpochs: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch_num\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m | Train Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtotal_loss_train\u001b[38;5;250m \u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mlen\u001b[39m(train_data)\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m .3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;130;01m\\\u001b[39;00m\n\u001b[0;32m    107\u001b[0m \u001b[38;5;124m                | Train Accuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtotal_acc_train\u001b[38;5;250m \u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mlen\u001b[39m(train_data)\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m .3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;130;01m\\\u001b[39;00m\n\u001b[0;32m    108\u001b[0m \u001b[38;5;124m                | Val Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtotal_loss_val\u001b[38;5;250m \u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mlen\u001b[39m(val_data)\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m .3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;130;01m\\\u001b[39;00m\n\u001b[0;32m    109\u001b[0m \u001b[38;5;124m                | Val Accuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtotal_acc_val\u001b[38;5;250m \u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mlen\u001b[39m(val_data)\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m .3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m--> 113\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdf_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdf_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mLR\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mEPOCHS\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[24], line 26\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(model, train_data, val_data, learning_rate, epochs)\u001b[0m\n\u001b[0;32m     24\u001b[0m n\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m train_input, train_label \u001b[38;5;129;01min\u001b[39;00m tqdm(train_dataloader):\n\u001b[1;32m---> 26\u001b[0m     train_label \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_label\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# to cuda GPU\u001b[39;00m\n\u001b[0;32m     27\u001b[0m     mask \u001b[38;5;241m=\u001b[39m train_input[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device) \u001b[38;5;66;03m# attention mask\u001b[39;00m\n\u001b[0;32m     28\u001b[0m     input_id \u001b[38;5;241m=\u001b[39m train_input[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def train(model, train_data, val_data, learning_rate, epochs):\n",
    "\n",
    "    train, val = Dataset(train_data), Dataset(val_data)\n",
    "    \n",
    "    # mini batching\n",
    "    train_dataloader = torch.utils.data.DataLoader(train, batch_size=30)\n",
    "    val_dataloader = torch.utils.data.DataLoader(val, batch_size=30)\n",
    "\n",
    "    use_cuda = torch.cuda.is_available()\n",
    "    device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = Adam(model.parameters(), lr= learning_rate)\n",
    "    \n",
    "    if use_cuda:\n",
    "\n",
    "            model = model.cuda()\n",
    "            criterion = criterion.cuda()\n",
    "\n",
    "    for epoch_num in range(epochs):\n",
    "\n",
    "            total_acc_train = 0\n",
    "            total_loss_train = 0\n",
    "            n=0\n",
    "            for train_input, train_label in tqdm(train_dataloader):\n",
    "                train_label = train_label.to(device) # to cuda GPU\n",
    "                mask = train_input['attention_mask'].to(device) # attention mask\n",
    "                input_id = train_input['input_ids'].squeeze(1).to(device)\n",
    "                \n",
    "                l1_loss=0\n",
    "                \n",
    "                # for L1 regularization\n",
    "                a=0\n",
    "                reg_loss = 0\n",
    "                for param in model.parameters():\n",
    "                    a=a+1\n",
    "                    if a >=201-68:\n",
    "                        reg_loss += torch.norm(param, 1) \n",
    "                '''\n",
    "                factor = 0.00001 #lambda for L1 regularization\n",
    "                l1_loss=factor * reg_loss # L1 loss\n",
    "                '''\n",
    "                # model output\n",
    "                output = model(input_id, mask)\n",
    "                \n",
    "                \n",
    "                # loss value\n",
    "                batch_loss = criterion(output, train_label) + l1_loss\n",
    "                total_loss_train += batch_loss.item() \n",
    "                \n",
    "                # train accuracy \n",
    "                output=output.squeeze(0)\n",
    "                train_label=train_label.squeeze(0)\n",
    "                # print(output)\n",
    "                # print(train_label)\n",
    "                # print(torch.gather(train_label, 1, torch.argmax(output,dim=1).view(-1, 1)))\n",
    "                acc = torch.gather(train_label, 1, torch.argmax(output,dim=1).view(-1, 1)).sum().item()\n",
    "                total_acc_train += acc\n",
    "                \n",
    "                # backpropogation\n",
    "                model.zero_grad()\n",
    "                batch_loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "            total_acc_val = 0\n",
    "            total_loss_val = 0\n",
    "            \n",
    "            # for validation accuracy\n",
    "            with torch.no_grad():\n",
    "\n",
    "                for val_input, val_label in val_dataloader:\n",
    "\n",
    "                    val_label = val_label.to(device)\n",
    "                    mask = val_input['attention_mask'].to(device)\n",
    "                    input_id = val_input['input_ids'].squeeze(1).to(device)\n",
    "                    \n",
    "                    # validation output\n",
    "                    output = model(input_id, mask)\n",
    "                    \n",
    "                    # validation loss value\n",
    "                    batch_loss = criterion(output, val_label)\n",
    "                    total_loss_val += batch_loss.item()\n",
    "                    \n",
    "                    \n",
    "                    # validation accuracy\n",
    "                    output=output.squeeze(0)\n",
    "                    val_label=val_label.squeeze(0)\n",
    "                    '''\n",
    "                    acc = torch.gather(val_label, 1, torch.argmax(output,dim=1).view(-1, 1)).sum().item()\n",
    "                    \n",
    "                    '''\n",
    "                    acc_bool=(torch.tensor(accuracy_calc(output)).to(device)==val_label).int()\n",
    "                    acc_tensor = torch.all(acc_bool == 1, dim=1)\n",
    "                    acc=sum(acc_tensor)\n",
    "                    total_acc_val += acc\n",
    "                '''    \n",
    "                mean = torch.mean(output, dim=1)\n",
    "                std = torch.std(output, dim=1)\n",
    "                output = (output - torch.unsqueeze(mean,0).t()) / torch.unsqueeze(std,0).t()\n",
    "                '''\n",
    "                \n",
    "                print(output)\n",
    "                print(val_label)\n",
    "\n",
    "            print(\n",
    "                f'Epochs: {epoch_num + 1} | Train Loss: {total_loss_train / len(train_data): .3f} \\\n",
    "                | Train Accuracy: {total_acc_train / len(train_data): .3f} \\\n",
    "                | Val Loss: {total_loss_val / len(val_data): .3f} \\\n",
    "                | Val Accuracy: {total_acc_val / len(val_data): .3f}')\n",
    "\n",
    "\n",
    "\n",
    "train(model, df_train, df_val, LR, EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6abdc2f8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "59b754c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "def accuracy_calc(l):\n",
    "    p=l.tolist()\n",
    "    out=[]\n",
    "    for a in p:\n",
    "        \n",
    "        b={}\n",
    "        for i,x in enumerate(a):\n",
    "            b[i]=x\n",
    "        sorted_dict = dict(sorted(b.items(), key=lambda item: item[1], reverse=True))\n",
    "        copy_dic=sorted_dict\n",
    "        copy_dic = copy.deepcopy(sorted_dict)\n",
    "        su=0\n",
    "        for p,q in sorted_dict.items():\n",
    "            su=su+q\n",
    "            copy_dic[p]=1\n",
    "            if su>sum(list(sorted_dict.values()))*.9:\n",
    "                break\n",
    "        l=list(dict(sorted(copy_dic.items(), key=lambda item: item[0])).values())\n",
    "        out.append([0 if x != 1 else x for x in l])    \n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "bb018c36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0858, 0.0000, 0.0000, 0.0000],\n",
       "        [0.0000, 0.1444, 2.4341, 0.0000],\n",
       "        [0.0000, 1.8987, 0.0488, 0.0000],\n",
       "        [0.0000, 2.2600, 1.4062, 0.0000],\n",
       "        [0.0000, 2.0379, 0.8080, 0.0000],\n",
       "        [0.0000, 0.0000, 1.0410, 0.0000],\n",
       "        [0.0000, 1.8934, 0.1403, 0.0000],\n",
       "        [0.0000, 0.0000, 1.9777, 0.0000],\n",
       "        [0.0000, 3.9491, 1.2324, 0.0000],\n",
       "        [0.0000, 0.9085, 1.7385, 0.0000],\n",
       "        [0.0000, 0.1752, 0.0000, 0.0000],\n",
       "        [0.0000, 0.0223, 2.1809, 0.0000],\n",
       "        [0.0000, 2.0517, 0.1653, 0.0000],\n",
       "        [0.0000, 3.8410, 1.6498, 0.0000],\n",
       "        [0.0000, 0.5239, 3.3225, 0.0000],\n",
       "        [0.0000, 0.6871, 0.4947, 0.0000],\n",
       "        [0.0000, 2.7512, 0.4298, 0.0000],\n",
       "        [0.0000, 0.3287, 0.7976, 0.2881],\n",
       "        [0.0710, 0.7761, 1.7600, 0.0000]])"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "z=torch.tensor([[1.0858, 0.0000, 0.0000, 0.0000],\n",
    "        [0.0000, 0.1444, 2.4341, 0.0000],\n",
    "        [0.0000, 1.8987, 0.0488, 0.0000],\n",
    "        [0.0000, 2.2600, 1.4062, 0.0000],\n",
    "        [0.0000, 2.0379, 0.8080, 0.0000],\n",
    "        [0.0000, 0.0000, 1.0410, 0.0000],\n",
    "        [0.0000, 1.8934, 0.1403, 0.0000],\n",
    "        [0.0000, 0.0000, 1.9777, 0.0000],\n",
    "        [0.0000, 3.9491, 1.2324, 0.0000],\n",
    "        [0.0000, 0.9085, 1.7385, 0.0000],\n",
    "        [0.0000, 0.1752, 0.0000, 0.0000],\n",
    "        [0.0000, 0.0223, 2.1809, 0.0000],\n",
    "        [0.0000, 2.0517, 0.1653, 0.0000],\n",
    "        [0.0000, 3.8410, 1.6498, 0.0000],\n",
    "        [0.0000, 0.5239, 3.3225, 0.0000],\n",
    "        [0.0000, 0.6871, 0.4947, 0.0000],\n",
    "        [0.0000, 2.7512, 0.4298, 0.0000],\n",
    "        [0.0000, 0.3287, 0.7976, 0.2881],\n",
    "        [0.0710, 0.7761, 1.7600, 0.0000]])\n",
    "z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "0ced1a99",
   "metadata": {},
   "outputs": [],
   "source": [
    "res=(torch.tensor(accuracy_calc(z)).to(device)==torch.tensor([[1., 0., 0., 0.],\n",
    "        [0., 0., 1., 1.],\n",
    "        [0., 1., 1., 0.],\n",
    "        [0., 1., 0., 0.],\n",
    "        [0., 0., 1., 0.],\n",
    "        [1., 0., 0., 1.],\n",
    "        [0., 0., 1., 0.],\n",
    "        [0., 0., 1., 0.],\n",
    "        [0., 1., 0., 0.],\n",
    "        [0., 1., 1., 0.],\n",
    "        [1., 1., 1., 0.],\n",
    "        [0., 0., 1., 1.],\n",
    "        [1., 1., 0., 0.],\n",
    "        [0., 1., 0., 0.],\n",
    "        [0., 0., 1., 0.],\n",
    "        [0., 0., 0., 1.],\n",
    "        [1., 1., 1., 0.],\n",
    "        [0., 0., 1., 0.],\n",
    "        [0., 1., 1., 0.]]).to(device)).int()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "eff28649",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n",
      "       device='cuda:0', dtype=torch.int32)\n"
     ]
    }
   ],
   "source": [
    "result = torch.all(res == 1, dim=1)\n",
    "\n",
    "# Convert the boolean tensor to integers (1 for True, 0 for False)\n",
    "result = result.int()\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "e17bcc72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.2105, device='cuda:0')"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(result)/len(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc7b91dd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "gpu1",
   "language": "python",
   "name": "gpu1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
