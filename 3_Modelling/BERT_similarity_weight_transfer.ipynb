{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1a664283",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "154ac5ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "model=torch.load('BERT_mask.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "29db6f67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert.embeddings.word_embeddings.weight\n",
      "bert.embeddings.position_embeddings.weight\n",
      "bert.embeddings.token_type_embeddings.weight\n",
      "bert.embeddings.LayerNorm.weight\n",
      "bert.embeddings.LayerNorm.bias\n",
      "bert.encoder.layer.0.attention.self.query.weight\n",
      "bert.encoder.layer.0.attention.self.query.bias\n",
      "bert.encoder.layer.0.attention.self.key.weight\n",
      "bert.encoder.layer.0.attention.self.key.bias\n",
      "bert.encoder.layer.0.attention.self.value.weight\n",
      "bert.encoder.layer.0.attention.self.value.bias\n",
      "bert.encoder.layer.0.attention.output.dense.weight\n",
      "bert.encoder.layer.0.attention.output.dense.bias\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.weight\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.bias\n",
      "bert.encoder.layer.0.intermediate.dense.weight\n",
      "bert.encoder.layer.0.intermediate.dense.bias\n",
      "bert.encoder.layer.0.output.dense.weight\n",
      "bert.encoder.layer.0.output.dense.bias\n",
      "bert.encoder.layer.0.output.LayerNorm.weight\n",
      "bert.encoder.layer.0.output.LayerNorm.bias\n",
      "bert.encoder.layer.1.attention.self.query.weight\n",
      "bert.encoder.layer.1.attention.self.query.bias\n",
      "bert.encoder.layer.1.attention.self.key.weight\n",
      "bert.encoder.layer.1.attention.self.key.bias\n",
      "bert.encoder.layer.1.attention.self.value.weight\n",
      "bert.encoder.layer.1.attention.self.value.bias\n",
      "bert.encoder.layer.1.attention.output.dense.weight\n",
      "bert.encoder.layer.1.attention.output.dense.bias\n",
      "bert.encoder.layer.1.attention.output.LayerNorm.weight\n",
      "bert.encoder.layer.1.attention.output.LayerNorm.bias\n",
      "bert.encoder.layer.1.intermediate.dense.weight\n",
      "bert.encoder.layer.1.intermediate.dense.bias\n",
      "bert.encoder.layer.1.output.dense.weight\n",
      "bert.encoder.layer.1.output.dense.bias\n",
      "bert.encoder.layer.1.output.LayerNorm.weight\n",
      "bert.encoder.layer.1.output.LayerNorm.bias\n",
      "bert.encoder.layer.2.attention.self.query.weight\n",
      "bert.encoder.layer.2.attention.self.query.bias\n",
      "bert.encoder.layer.2.attention.self.key.weight\n",
      "bert.encoder.layer.2.attention.self.key.bias\n",
      "bert.encoder.layer.2.attention.self.value.weight\n",
      "bert.encoder.layer.2.attention.self.value.bias\n",
      "bert.encoder.layer.2.attention.output.dense.weight\n",
      "bert.encoder.layer.2.attention.output.dense.bias\n",
      "bert.encoder.layer.2.attention.output.LayerNorm.weight\n",
      "bert.encoder.layer.2.attention.output.LayerNorm.bias\n",
      "bert.encoder.layer.2.intermediate.dense.weight\n",
      "bert.encoder.layer.2.intermediate.dense.bias\n",
      "bert.encoder.layer.2.output.dense.weight\n",
      "bert.encoder.layer.2.output.dense.bias\n",
      "bert.encoder.layer.2.output.LayerNorm.weight\n",
      "bert.encoder.layer.2.output.LayerNorm.bias\n",
      "bert.encoder.layer.3.attention.self.query.weight\n",
      "bert.encoder.layer.3.attention.self.query.bias\n",
      "bert.encoder.layer.3.attention.self.key.weight\n",
      "bert.encoder.layer.3.attention.self.key.bias\n",
      "bert.encoder.layer.3.attention.self.value.weight\n",
      "bert.encoder.layer.3.attention.self.value.bias\n",
      "bert.encoder.layer.3.attention.output.dense.weight\n",
      "bert.encoder.layer.3.attention.output.dense.bias\n",
      "bert.encoder.layer.3.attention.output.LayerNorm.weight\n",
      "bert.encoder.layer.3.attention.output.LayerNorm.bias\n",
      "bert.encoder.layer.3.intermediate.dense.weight\n",
      "bert.encoder.layer.3.intermediate.dense.bias\n",
      "bert.encoder.layer.3.output.dense.weight\n",
      "bert.encoder.layer.3.output.dense.bias\n",
      "bert.encoder.layer.3.output.LayerNorm.weight\n",
      "bert.encoder.layer.3.output.LayerNorm.bias\n",
      "bert.encoder.layer.4.attention.self.query.weight\n",
      "bert.encoder.layer.4.attention.self.query.bias\n",
      "bert.encoder.layer.4.attention.self.key.weight\n",
      "bert.encoder.layer.4.attention.self.key.bias\n",
      "bert.encoder.layer.4.attention.self.value.weight\n",
      "bert.encoder.layer.4.attention.self.value.bias\n",
      "bert.encoder.layer.4.attention.output.dense.weight\n",
      "bert.encoder.layer.4.attention.output.dense.bias\n",
      "bert.encoder.layer.4.attention.output.LayerNorm.weight\n",
      "bert.encoder.layer.4.attention.output.LayerNorm.bias\n",
      "bert.encoder.layer.4.intermediate.dense.weight\n",
      "bert.encoder.layer.4.intermediate.dense.bias\n",
      "bert.encoder.layer.4.output.dense.weight\n",
      "bert.encoder.layer.4.output.dense.bias\n",
      "bert.encoder.layer.4.output.LayerNorm.weight\n",
      "bert.encoder.layer.4.output.LayerNorm.bias\n",
      "bert.encoder.layer.5.attention.self.query.weight\n",
      "bert.encoder.layer.5.attention.self.query.bias\n",
      "bert.encoder.layer.5.attention.self.key.weight\n",
      "bert.encoder.layer.5.attention.self.key.bias\n",
      "bert.encoder.layer.5.attention.self.value.weight\n",
      "bert.encoder.layer.5.attention.self.value.bias\n",
      "bert.encoder.layer.5.attention.output.dense.weight\n",
      "bert.encoder.layer.5.attention.output.dense.bias\n",
      "bert.encoder.layer.5.attention.output.LayerNorm.weight\n",
      "bert.encoder.layer.5.attention.output.LayerNorm.bias\n",
      "bert.encoder.layer.5.intermediate.dense.weight\n",
      "bert.encoder.layer.5.intermediate.dense.bias\n",
      "bert.encoder.layer.5.output.dense.weight\n",
      "bert.encoder.layer.5.output.dense.bias\n",
      "bert.encoder.layer.5.output.LayerNorm.weight\n",
      "bert.encoder.layer.5.output.LayerNorm.bias\n",
      "bert.encoder.layer.6.attention.self.query.weight\n",
      "bert.encoder.layer.6.attention.self.query.bias\n",
      "bert.encoder.layer.6.attention.self.key.weight\n",
      "bert.encoder.layer.6.attention.self.key.bias\n",
      "bert.encoder.layer.6.attention.self.value.weight\n",
      "bert.encoder.layer.6.attention.self.value.bias\n",
      "bert.encoder.layer.6.attention.output.dense.weight\n",
      "bert.encoder.layer.6.attention.output.dense.bias\n",
      "bert.encoder.layer.6.attention.output.LayerNorm.weight\n",
      "bert.encoder.layer.6.attention.output.LayerNorm.bias\n",
      "bert.encoder.layer.6.intermediate.dense.weight\n",
      "bert.encoder.layer.6.intermediate.dense.bias\n",
      "bert.encoder.layer.6.output.dense.weight\n",
      "bert.encoder.layer.6.output.dense.bias\n",
      "bert.encoder.layer.6.output.LayerNorm.weight\n",
      "bert.encoder.layer.6.output.LayerNorm.bias\n",
      "bert.encoder.layer.7.attention.self.query.weight\n",
      "bert.encoder.layer.7.attention.self.query.bias\n",
      "bert.encoder.layer.7.attention.self.key.weight\n",
      "bert.encoder.layer.7.attention.self.key.bias\n",
      "bert.encoder.layer.7.attention.self.value.weight\n",
      "bert.encoder.layer.7.attention.self.value.bias\n",
      "bert.encoder.layer.7.attention.output.dense.weight\n",
      "bert.encoder.layer.7.attention.output.dense.bias\n",
      "bert.encoder.layer.7.attention.output.LayerNorm.weight\n",
      "bert.encoder.layer.7.attention.output.LayerNorm.bias\n",
      "bert.encoder.layer.7.intermediate.dense.weight\n",
      "bert.encoder.layer.7.intermediate.dense.bias\n",
      "bert.encoder.layer.7.output.dense.weight\n",
      "bert.encoder.layer.7.output.dense.bias\n",
      "bert.encoder.layer.7.output.LayerNorm.weight\n",
      "bert.encoder.layer.7.output.LayerNorm.bias\n",
      "bert.encoder.layer.8.attention.self.query.weight\n",
      "bert.encoder.layer.8.attention.self.query.bias\n",
      "bert.encoder.layer.8.attention.self.key.weight\n",
      "bert.encoder.layer.8.attention.self.key.bias\n",
      "bert.encoder.layer.8.attention.self.value.weight\n",
      "bert.encoder.layer.8.attention.self.value.bias\n",
      "bert.encoder.layer.8.attention.output.dense.weight\n",
      "bert.encoder.layer.8.attention.output.dense.bias\n",
      "bert.encoder.layer.8.attention.output.LayerNorm.weight\n",
      "bert.encoder.layer.8.attention.output.LayerNorm.bias\n",
      "bert.encoder.layer.8.intermediate.dense.weight\n",
      "bert.encoder.layer.8.intermediate.dense.bias\n",
      "bert.encoder.layer.8.output.dense.weight\n",
      "bert.encoder.layer.8.output.dense.bias\n",
      "bert.encoder.layer.8.output.LayerNorm.weight\n",
      "bert.encoder.layer.8.output.LayerNorm.bias\n",
      "bert.encoder.layer.9.attention.self.query.weight\n",
      "bert.encoder.layer.9.attention.self.query.bias\n",
      "bert.encoder.layer.9.attention.self.key.weight\n",
      "bert.encoder.layer.9.attention.self.key.bias\n",
      "bert.encoder.layer.9.attention.self.value.weight\n",
      "bert.encoder.layer.9.attention.self.value.bias\n",
      "bert.encoder.layer.9.attention.output.dense.weight\n",
      "bert.encoder.layer.9.attention.output.dense.bias\n",
      "bert.encoder.layer.9.attention.output.LayerNorm.weight\n",
      "bert.encoder.layer.9.attention.output.LayerNorm.bias\n",
      "bert.encoder.layer.9.intermediate.dense.weight\n",
      "bert.encoder.layer.9.intermediate.dense.bias\n",
      "bert.encoder.layer.9.output.dense.weight\n",
      "bert.encoder.layer.9.output.dense.bias\n",
      "bert.encoder.layer.9.output.LayerNorm.weight\n",
      "bert.encoder.layer.9.output.LayerNorm.bias\n",
      "bert.encoder.layer.10.attention.self.query.weight\n",
      "bert.encoder.layer.10.attention.self.query.bias\n",
      "bert.encoder.layer.10.attention.self.key.weight\n",
      "bert.encoder.layer.10.attention.self.key.bias\n",
      "bert.encoder.layer.10.attention.self.value.weight\n",
      "bert.encoder.layer.10.attention.self.value.bias\n",
      "bert.encoder.layer.10.attention.output.dense.weight\n",
      "bert.encoder.layer.10.attention.output.dense.bias\n",
      "bert.encoder.layer.10.attention.output.LayerNorm.weight\n",
      "bert.encoder.layer.10.attention.output.LayerNorm.bias\n",
      "bert.encoder.layer.10.intermediate.dense.weight\n",
      "bert.encoder.layer.10.intermediate.dense.bias\n",
      "bert.encoder.layer.10.output.dense.weight\n",
      "bert.encoder.layer.10.output.dense.bias\n",
      "bert.encoder.layer.10.output.LayerNorm.weight\n",
      "bert.encoder.layer.10.output.LayerNorm.bias\n",
      "bert.encoder.layer.11.attention.self.query.weight\n",
      "bert.encoder.layer.11.attention.self.query.bias\n",
      "bert.encoder.layer.11.attention.self.key.weight\n",
      "bert.encoder.layer.11.attention.self.key.bias\n",
      "bert.encoder.layer.11.attention.self.value.weight\n",
      "bert.encoder.layer.11.attention.self.value.bias\n",
      "bert.encoder.layer.11.attention.output.dense.weight\n",
      "bert.encoder.layer.11.attention.output.dense.bias\n",
      "bert.encoder.layer.11.attention.output.LayerNorm.weight\n",
      "bert.encoder.layer.11.attention.output.LayerNorm.bias\n",
      "bert.encoder.layer.11.intermediate.dense.weight\n",
      "bert.encoder.layer.11.intermediate.dense.bias\n",
      "bert.encoder.layer.11.output.dense.weight\n",
      "bert.encoder.layer.11.output.dense.bias\n",
      "bert.encoder.layer.11.output.LayerNorm.weight\n",
      "bert.encoder.layer.11.output.LayerNorm.bias\n",
      "cls.predictions.bias\n",
      "cls.predictions.transform.dense.weight\n",
      "cls.predictions.transform.dense.bias\n",
      "cls.predictions.transform.LayerNorm.weight\n",
      "cls.predictions.transform.LayerNorm.bias\n",
      "cls.predictions.decoder.weight\n",
      "cls.predictions.decoder.bias\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "204"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n=0\n",
    "for x in model.state_dict():\n",
    "    n=n+1\n",
    "    print(x)\n",
    "n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "711881f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-6.4523e-03, -2.0317e-02,  4.6948e-02,  1.5610e-02, -4.1688e-02,\n",
       "        -1.3823e-02, -9.9459e-03, -3.1925e-02,  2.5848e-02, -3.5719e-02,\n",
       "        -6.0546e-02,  3.0363e-02,  1.6680e-02,  1.1848e-02, -3.5969e-03,\n",
       "        -4.8558e-02,  4.2298e-04, -9.1702e-02,  1.0390e-02,  1.1714e-02,\n",
       "        -5.8461e-02, -4.7710e-03,  5.0649e-03, -4.4028e-02,  5.0694e-03,\n",
       "        -3.9832e-02, -4.7910e-02, -5.1699e-02, -9.1433e-02, -4.0506e-02,\n",
       "        -2.7266e-02, -4.6149e-02, -1.2191e-02, -7.5270e-02, -4.1507e-02,\n",
       "        -5.7161e-02, -2.7618e-02, -7.8072e-02,  6.9756e-03, -5.9606e-02,\n",
       "         5.3213e-02, -2.1257e-02, -4.5698e-02, -6.8589e-02, -1.0861e-01,\n",
       "        -1.8931e-02, -1.3497e-01, -2.3966e-03, -1.0932e-01, -1.1749e-01,\n",
       "        -2.1131e-02,  4.4923e-02,  3.4135e-02, -2.5268e-02, -8.8217e-02,\n",
       "        -9.0805e-03, -2.2671e-03, -6.2807e-02, -3.3862e-04, -6.9942e-02,\n",
       "         4.4014e-02, -8.4269e-02,  2.0889e-02,  5.0521e-02, -4.3462e-02,\n",
       "        -4.7929e-02, -6.4631e-04, -5.8909e-02,  3.2735e-02,  3.4517e-04,\n",
       "         2.1681e-02, -2.2895e-02, -6.6123e-02, -5.4571e-02, -1.2539e-02,\n",
       "        -8.4490e-02,  4.7765e-02, -4.9521e-02,  4.0564e-02,  3.1120e-02,\n",
       "         3.3802e-02, -3.4906e-02, -5.0345e-02,  2.0176e-02, -5.7624e-02,\n",
       "        -3.7193e-02,  2.8059e-02, -1.3513e-02, -4.7285e-02,  3.0422e-02,\n",
       "         1.4913e-02, -7.7604e-02,  5.2582e-02, -1.5523e-02, -4.0374e-02,\n",
       "        -1.1517e-01, -3.9766e-02, -1.3144e-02,  2.9801e-02, -1.0077e-02,\n",
       "        -8.0950e-02, -7.5172e-02, -9.0253e-02, -2.8527e-02, -1.0854e-01,\n",
       "        -1.8852e-02, -2.7838e-02,  4.8010e-02, -5.0576e-02, -2.2118e-02,\n",
       "        -3.4760e-02, -3.8120e-02,  1.1955e-02,  2.0582e-02, -2.0803e-02,\n",
       "         3.5551e-02, -3.0075e-03,  3.5483e-02, -1.7932e-03, -7.1549e-03,\n",
       "         3.4860e-02,  1.1605e-01, -4.4658e-03, -3.3021e-02,  1.1173e-02,\n",
       "         4.9342e-02,  9.4757e-03, -3.2597e-03, -3.3375e-02, -1.2737e-01,\n",
       "        -6.8389e-03, -1.8048e-03, -5.9507e-03, -3.4198e-02,  6.4502e-02,\n",
       "        -1.1180e-01, -4.6321e-02, -2.9782e-03,  3.8461e-02, -3.4754e-02,\n",
       "        -1.1473e-02, -4.3851e-03,  4.0280e-02, -2.8297e-02, -1.4682e-02,\n",
       "        -8.7001e-02, -3.7303e-02,  2.9345e-02, -6.6815e-02,  3.2023e-02,\n",
       "         6.1036e-03,  5.7273e-02,  7.3194e-02, -7.0628e-02,  1.7109e-02,\n",
       "         8.3086e-02, -1.3743e-02, -6.9930e-02, -1.1724e-01, -1.0780e-02,\n",
       "         2.3437e-02,  5.2473e-02, -1.8402e-02,  5.1213e-02, -6.5672e-02,\n",
       "        -7.0694e-02,  3.3233e-03, -9.4403e-02, -5.0532e-02, -5.2560e-02,\n",
       "        -2.3850e-02, -1.7085e-02,  1.9018e-02, -4.9709e-03, -7.9306e-02,\n",
       "         5.7373e-02,  9.3325e-02, -3.4698e-02, -5.0066e-02, -7.8543e-02,\n",
       "         1.3122e-01,  1.4673e-02, -1.6938e-02, -3.8435e-02, -1.4733e-02,\n",
       "        -6.0461e-02, -6.1729e-02,  6.4496e-03,  3.3840e-02,  9.0751e-03,\n",
       "        -3.2665e-02, -1.1076e-01, -1.0684e-02, -1.2094e-03,  4.3203e-02,\n",
       "        -5.1379e-02,  1.0308e-02,  1.2183e-01, -6.8764e-03,  2.7942e-02,\n",
       "        -4.4556e-02, -2.6252e-02,  3.8895e-02,  5.1236e-02, -8.5446e-02,\n",
       "         4.3974e-02,  4.2842e-02, -6.3490e-02,  4.4482e-02, -6.1192e-02,\n",
       "        -1.3520e-02, -6.8760e-02,  4.3346e-02, -8.8517e-02, -5.1777e-03,\n",
       "        -2.9706e-02,  5.5594e-02, -6.7947e-02, -4.8597e-02,  4.4588e-03,\n",
       "         3.2622e-02, -6.2133e-02, -1.9328e-02,  5.9748e-03, -7.4520e-02,\n",
       "        -1.1827e-01,  3.6686e-02, -1.0178e-01, -3.3162e-02,  1.8303e-03,\n",
       "        -5.3591e-02, -4.5645e-02, -1.0221e-01,  1.5094e-03,  6.1728e-02,\n",
       "        -2.1137e-02, -8.4871e-03, -3.7216e-02,  4.2175e-02,  3.1417e-02,\n",
       "        -4.3526e-02, -1.8746e-03, -4.8861e-02,  1.9841e-02, -2.8993e-03,\n",
       "        -3.3155e-02,  4.4352e-02, -7.5824e-02, -7.5703e-02, -4.3591e-02,\n",
       "        -5.8319e-02, -9.6392e-02, -2.2687e-02, -2.8491e-02,  7.8004e-02,\n",
       "        -7.9945e-02, -4.8014e-02,  6.8501e-03,  2.6866e-02, -5.7744e-02,\n",
       "        -6.0478e-02, -3.2894e-02, -3.1588e-02, -1.2696e-01, -5.1323e-02,\n",
       "        -6.7020e-02, -1.9172e-01, -5.1721e-02,  2.2162e-03,  3.0670e-02,\n",
       "         8.0560e-02, -3.0530e-02,  1.5078e-02, -7.1606e-02, -3.3116e-02,\n",
       "        -5.3048e-02,  1.1599e-01, -5.4143e-04,  1.9723e-03, -6.6084e-02,\n",
       "        -4.3510e-02, -1.0656e-02,  4.7826e-02, -8.9496e-02,  6.5902e-02,\n",
       "        -2.5231e-02,  1.3512e-02, -9.6697e-02, -1.1846e-01,  1.1103e-02,\n",
       "         1.6439e-02,  4.3584e-02, -2.9340e-02, -5.1699e-02, -1.0889e-02,\n",
       "        -3.2376e-02, -2.3116e-02,  8.9311e-02, -6.5233e-02,  1.3737e-02,\n",
       "         3.7024e-02,  3.2160e-02, -9.5401e-02, -4.3724e-02, -1.5979e-02,\n",
       "        -3.1506e-02, -1.2327e-02,  2.6167e-03,  2.2537e-01, -5.3696e-02,\n",
       "        -2.0477e-02,  4.3348e-03, -2.4766e-02, -2.4036e-02,  6.6253e-02,\n",
       "        -1.5251e-02, -3.8840e-03,  3.4556e-02, -2.6489e-02,  4.3656e-02,\n",
       "        -3.0925e-02, -3.2693e-02, -9.1047e-03,  6.6226e-02, -7.7536e-02,\n",
       "        -3.1232e-02, -4.4809e-02,  1.6949e-02, -8.0668e-02, -3.1215e-02,\n",
       "        -2.6046e-02, -4.2385e-02, -9.0958e-02,  6.7670e-02, -3.9040e-02,\n",
       "        -4.5446e-04, -4.8644e-02, -1.5628e-02, -5.0144e-02,  7.3783e-02,\n",
       "        -2.5105e-02, -1.5402e-02, -1.3958e-02, -5.2957e-02,  3.0382e-02,\n",
       "        -4.4465e-02, -3.0508e-02,  1.3959e-02, -1.5415e-02, -4.4661e-02,\n",
       "        -8.1628e-02,  3.7103e-02, -1.0718e-01, -6.8334e-02, -2.4383e-02,\n",
       "        -1.9261e-02,  3.5302e-02, -6.9521e-02, -5.6849e-02, -9.8419e-03,\n",
       "        -5.2158e-02,  6.1590e-02, -9.6127e-02, -3.5450e-02, -9.7808e-02,\n",
       "        -7.0746e-03,  4.3017e-03, -9.1642e-02, -3.5484e-02, -7.4831e-03,\n",
       "        -3.6376e-02, -3.2286e-02, -9.4001e-02,  1.2243e-02,  3.6356e-02,\n",
       "        -3.7538e-02,  1.2484e-02,  6.3775e-02, -2.8093e-02, -6.2089e-02,\n",
       "        -5.1415e-02, -2.0653e-02, -2.2206e-02,  1.7944e-02, -5.7541e-02,\n",
       "        -1.0300e-01, -8.6599e-03, -5.6977e-03, -5.4511e-02,  1.6783e-02,\n",
       "         1.2794e-01,  5.5507e-02,  5.4583e-03, -5.0038e-02,  5.3570e-02,\n",
       "        -1.9344e-03, -2.8522e-02,  5.8118e-02,  2.4355e-02, -7.4415e-02,\n",
       "        -5.9233e-02, -1.1520e-02,  2.4468e-02,  1.0812e-01, -1.6713e-02,\n",
       "        -6.4587e-02,  2.6876e-02, -6.5102e-02,  8.3963e-02, -4.5102e-02,\n",
       "        -1.4063e-03, -4.8374e-02, -2.6398e-02, -2.3276e-02, -5.2105e-02,\n",
       "        -5.2864e-02,  6.0122e-03, -3.1543e-02,  1.6095e-02, -6.7345e-03,\n",
       "        -9.3208e-03, -1.7123e-01, -1.5844e-02, -3.4706e-02, -6.6143e-02,\n",
       "        -2.9764e-02,  7.1169e-03, -5.1042e-03, -7.4857e-02, -1.4608e-02,\n",
       "        -7.8908e-02, -2.6602e-02, -1.5003e-01,  2.2984e-02,  1.0018e-02,\n",
       "        -2.5711e-02,  1.8026e-05, -2.0075e-02, -3.9420e-02, -1.5980e-02,\n",
       "        -2.5962e-02, -6.8212e-02,  3.3517e-02, -1.6346e-03, -4.9878e-02,\n",
       "         5.9397e-02,  4.6518e-02, -1.0219e-01, -2.6494e-02, -4.4878e-02,\n",
       "        -2.1767e-02, -4.0936e-02, -1.3987e-02, -8.7613e-04, -6.7404e-02,\n",
       "        -6.2006e-02, -4.5987e-02, -6.4913e-03,  7.2685e-02, -4.7416e-02,\n",
       "        -1.5137e-02, -3.7320e-02, -1.4006e-01, -9.2600e-02, -1.2160e-02,\n",
       "        -5.2396e-02, -5.8222e-02,  5.3884e-02, -3.8508e-02, -8.9214e-02,\n",
       "        -5.5617e-02, -1.9866e-02, -1.3809e-02,  4.2267e-02, -2.6668e-02,\n",
       "        -1.5454e-02, -5.2973e-02, -7.4215e-02,  5.6920e-02, -6.4374e-02,\n",
       "        -5.5840e-02,  5.6786e-03,  1.2876e-02,  3.2491e-02,  8.7462e-03,\n",
       "         7.9143e-04,  9.4453e-02,  3.8197e-02, -2.4066e-03, -2.4766e-02,\n",
       "        -6.8934e-03, -8.9169e-03, -4.5014e-02, -5.8317e-02, -2.8860e-02,\n",
       "         1.0017e-02, -4.2581e-02,  6.4984e-02,  4.7738e-02, -4.0530e-02,\n",
       "        -4.6422e-02, -1.6326e-02, -4.6603e-02, -1.5878e-02,  5.9152e-05,\n",
       "         9.7149e-02,  1.2494e-02, -1.8111e-02, -2.3818e-02,  5.2363e-03,\n",
       "        -6.2190e-02, -6.4469e-02,  3.2823e-02, -5.4023e-02,  9.1251e-02,\n",
       "        -1.0383e-01, -9.0666e-02,  5.9814e-03, -3.8401e-02, -7.3937e-02,\n",
       "        -4.6832e-02, -1.0060e-02,  2.4417e-03, -9.7398e-02, -5.7268e-02,\n",
       "         8.8186e-03, -4.7020e-02,  1.2186e-04,  3.5820e-02, -6.0436e-03,\n",
       "        -1.2656e-01, -2.7936e-02,  2.9002e-02,  7.4156e-02, -1.2334e-02,\n",
       "        -2.7782e-02,  1.0055e-01,  1.0424e-02,  3.4508e-02,  3.9747e-02,\n",
       "        -3.4627e-02,  6.7815e-02, -8.3191e-02,  9.6089e-04, -5.1771e-02,\n",
       "        -7.2300e-03, -6.4482e-02,  1.2494e-02, -3.5725e-02,  2.3913e-03,\n",
       "        -3.4318e-02, -6.2860e-02, -5.0645e-02,  2.8461e-02, -8.2031e-03,\n",
       "        -6.5231e-04,  1.6712e-02, -6.5494e-02, -1.7305e-02,  2.2664e-02,\n",
       "         2.2108e-02, -4.0129e-02,  1.2364e-02, -8.1461e-02, -2.3197e-02,\n",
       "        -4.8053e-02, -4.9838e-02, -4.7121e-02, -3.6437e-02,  7.6190e-03,\n",
       "        -4.7713e-02,  1.8774e-02, -1.0783e-01,  7.3068e-02, -3.5933e-02,\n",
       "        -1.3488e-01, -6.6914e-02,  3.7804e-02,  1.4475e-02,  7.2211e-03,\n",
       "         5.0949e-02,  4.0962e-02,  5.2091e-02, -5.7858e-02, -2.4915e-02,\n",
       "         1.8958e-02, -6.2235e-03, -4.0995e-03,  3.7999e-02,  1.0362e-02,\n",
       "        -7.2903e-02, -4.5367e-02, -4.7792e-02, -1.0388e-02,  1.5706e-02,\n",
       "        -9.3824e-02,  1.1272e-02,  4.4682e-02, -2.1576e-02,  2.2220e-02,\n",
       "        -2.4623e-02, -2.0144e-02, -9.2318e-02, -5.1539e-02,  3.7208e-02,\n",
       "        -2.6436e-02, -5.4705e-03, -1.9018e-02, -6.9319e-03,  5.4510e-02,\n",
       "        -5.3501e-02, -9.2597e-02, -8.9880e-02, -5.1506e-02,  4.7265e-03,\n",
       "        -1.1477e-01, -2.4999e-02, -1.2306e-02,  3.7221e-02,  9.6846e-03,\n",
       "        -5.9224e-02, -1.6642e-02, -1.0003e-01, -1.2690e-01,  3.3881e-02,\n",
       "        -6.5757e-02, -1.0703e-01, -2.2263e-02,  1.0568e-02, -8.8899e-02,\n",
       "        -5.8447e-02, -1.9237e-02,  3.0744e-02, -8.3834e-02, -4.7459e-02,\n",
       "         3.4953e-02, -2.4201e-02,  9.0150e-03, -2.2455e-02,  1.3748e-01,\n",
       "        -8.5644e-02,  3.7417e-03,  3.2690e-03,  2.5203e-02, -1.8308e-02,\n",
       "        -3.1578e-02, -7.8905e-02, -2.1841e-02, -9.4537e-02,  8.7071e-03,\n",
       "        -2.8523e-02, -3.9510e-02, -1.2959e-02, -5.3671e-02,  3.6715e-02,\n",
       "        -5.9236e-02, -4.1738e-02,  8.4010e-03, -2.3616e-02,  2.7057e-02,\n",
       "        -8.1234e-02, -6.2575e-02,  5.2921e-03, -8.0414e-03, -9.6772e-02,\n",
       "        -2.8053e-02, -6.4268e-02, -4.0277e-02, -1.5943e-02, -3.3610e-02,\n",
       "        -9.3049e-04,  1.0833e-02, -3.1568e-02, -2.7485e-02, -2.2691e-02,\n",
       "        -4.0286e-02,  1.1265e-02,  3.2678e-02,  1.9981e-02, -5.2608e-02,\n",
       "        -6.3314e-02,  2.8367e-02, -4.1600e-02,  3.0145e-02,  7.3242e-02,\n",
       "        -3.5698e-02, -1.3829e-02,  7.0318e-02,  6.3474e-02, -3.6664e-02,\n",
       "         5.1458e-03, -2.2842e-02, -2.2986e-02, -1.0106e-01, -3.1976e-02,\n",
       "         7.4513e-02, -6.8796e-02,  1.6792e-02, -4.1934e-02, -9.0059e-02,\n",
       "        -1.1396e-01, -5.5255e-02, -6.4622e-02, -1.0330e-01, -1.0593e-01,\n",
       "        -2.6068e-03, -2.7830e-02, -8.1022e-03,  1.5933e-02, -3.1834e-02,\n",
       "         2.2023e-02, -3.7949e-02,  1.1529e-02,  1.9512e-02, -2.0324e-02,\n",
       "        -1.7840e-02, -5.6408e-02,  6.8627e-02,  3.6931e-02, -5.3792e-03,\n",
       "        -5.5165e-02, -9.9139e-05, -2.1724e-02, -1.1537e-01, -1.2771e-02,\n",
       "         1.6417e-02,  2.6337e-02, -1.1230e-01, -9.1366e-02, -4.1066e-02,\n",
       "        -7.3055e-03,  6.3705e-02, -1.0127e-02,  6.3471e-02, -7.6329e-02,\n",
       "        -2.1962e-02,  2.5482e-04, -1.7658e-02,  1.3164e-02, -6.1970e-02,\n",
       "        -3.0410e-02, -7.4303e-03,  7.8402e-02, -1.1136e-01, -9.4044e-02,\n",
       "        -5.6273e-02,  1.9627e-02, -4.3710e-02, -5.5866e-02, -7.6812e-02,\n",
       "         5.8543e-02,  2.0116e-02, -6.8352e-02, -7.8896e-02, -1.6737e-03,\n",
       "        -2.2561e-02, -1.0294e-01,  2.1452e-02,  7.4320e-02, -3.9743e-02,\n",
       "        -7.1434e-02, -1.7409e-02,  8.0384e-03, -4.5732e-02, -9.7798e-03,\n",
       "         1.6966e-02,  3.6260e-02, -1.6842e-02], device='cuda:0')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.state_dict()['bert.encoder.layer.11.output.LayerNorm.bias']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "917690aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertModel\n",
    "model1 = BertModel.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4ec93270",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-6.3461e-03, -1.9894e-02,  4.6285e-02,  1.5851e-02, -4.2569e-02,\n",
       "        -1.3283e-02, -9.1369e-03, -3.3534e-02,  2.5846e-02, -3.6451e-02,\n",
       "        -6.1155e-02,  3.0418e-02,  1.7200e-02,  1.2180e-02, -3.3043e-03,\n",
       "        -4.8502e-02,  5.4115e-04, -9.2394e-02,  1.0380e-02,  1.1926e-02,\n",
       "        -5.8754e-02, -6.8212e-03,  4.3755e-03, -4.5268e-02,  5.5810e-03,\n",
       "        -4.1087e-02, -4.8185e-02, -5.1641e-02, -9.1535e-02, -4.0457e-02,\n",
       "        -2.7150e-02, -4.6665e-02, -1.1590e-02, -7.6277e-02, -4.2056e-02,\n",
       "        -5.7515e-02, -2.7733e-02, -7.7364e-02,  6.9931e-03, -5.9466e-02,\n",
       "         5.3589e-02, -2.0961e-02, -4.5835e-02, -6.9220e-02, -1.0842e-01,\n",
       "        -1.9135e-02, -1.3539e-01, -1.7333e-03, -1.0963e-01, -1.1798e-01,\n",
       "        -2.1245e-02,  4.4968e-02,  3.4347e-02, -2.5082e-02, -8.8453e-02,\n",
       "        -8.9132e-03, -2.2133e-03, -6.2073e-02, -5.2548e-04, -6.9640e-02,\n",
       "         4.3890e-02, -8.3953e-02,  2.0464e-02,  5.0537e-02, -4.3895e-02,\n",
       "        -4.7470e-02, -9.3629e-04, -5.9144e-02,  3.3087e-02,  9.0344e-04,\n",
       "         2.1959e-02, -2.3260e-02, -6.6600e-02, -5.4183e-02, -1.2760e-02,\n",
       "        -8.5299e-02,  4.8837e-02, -4.9116e-02,  4.0889e-02,  3.1605e-02,\n",
       "         3.4653e-02, -3.4898e-02, -5.1033e-02,  1.9560e-02, -5.7742e-02,\n",
       "        -3.7249e-02,  2.8692e-02, -1.3316e-02, -4.7234e-02,  2.9975e-02,\n",
       "         1.4817e-02, -7.7454e-02,  5.3532e-02, -1.5101e-02, -4.1975e-02,\n",
       "        -1.1653e-01, -4.0040e-02, -1.4511e-02,  2.9632e-02, -9.7741e-03,\n",
       "        -8.1774e-02, -7.5635e-02, -9.0471e-02, -2.8863e-02, -1.0929e-01,\n",
       "        -1.9059e-02, -2.8311e-02,  4.8349e-02, -5.1201e-02, -2.3034e-02,\n",
       "        -3.5799e-02, -3.8083e-02,  1.2375e-02,  2.0018e-02, -2.0495e-02,\n",
       "         3.5751e-02, -2.9506e-03,  3.5649e-02, -2.2652e-03, -7.3649e-03,\n",
       "         3.4992e-02,  1.1655e-01, -3.2204e-03, -3.2992e-02,  1.1133e-02,\n",
       "         5.0059e-02,  1.0109e-02, -3.6496e-03, -3.4484e-02, -1.2807e-01,\n",
       "        -6.3418e-03, -1.3855e-03, -6.3614e-03, -3.4011e-02,  6.4414e-02,\n",
       "        -1.1152e-01, -4.5309e-02, -2.4458e-03,  3.8958e-02, -3.5377e-02,\n",
       "        -1.1892e-02, -4.7690e-03,  3.9924e-02, -2.8776e-02, -1.5479e-02,\n",
       "        -8.6420e-02, -3.7144e-02,  2.9788e-02, -6.6552e-02,  3.1951e-02,\n",
       "         5.6070e-03,  5.7415e-02,  7.2993e-02, -7.0931e-02,  1.5957e-02,\n",
       "         8.3283e-02, -1.4200e-02, -7.0544e-02, -1.1789e-01, -1.1684e-02,\n",
       "         2.3768e-02,  5.1856e-02, -1.8541e-02,  5.1044e-02, -6.6014e-02,\n",
       "        -7.0924e-02,  3.6743e-03, -9.4640e-02, -5.0436e-02, -5.2546e-02,\n",
       "        -2.4143e-02, -1.7134e-02,  1.8747e-02, -5.1428e-03, -7.9001e-02,\n",
       "         5.7671e-02,  9.3698e-02, -3.3379e-02, -5.0376e-02, -7.9511e-02,\n",
       "         1.3052e-01,  1.4976e-02, -1.7266e-02, -3.8186e-02, -1.4686e-02,\n",
       "        -6.0080e-02, -6.1968e-02,  6.4466e-03,  3.3904e-02,  9.1657e-03,\n",
       "        -3.3732e-02, -1.1071e-01, -1.0519e-02, -6.0141e-04,  4.4024e-02,\n",
       "        -5.1106e-02,  1.0253e-02,  1.2218e-01, -7.0053e-03,  2.8158e-02,\n",
       "        -4.4821e-02, -2.7442e-02,  3.9209e-02,  5.1164e-02, -8.5783e-02,\n",
       "         4.3804e-02,  4.2881e-02, -6.2721e-02,  4.4046e-02, -6.1240e-02,\n",
       "        -1.3413e-02, -6.9396e-02,  4.2974e-02, -8.8764e-02, -4.5121e-03,\n",
       "        -2.9662e-02,  5.5994e-02, -6.8597e-02, -4.9121e-02,  4.4877e-03,\n",
       "         3.2838e-02, -6.2602e-02, -1.9148e-02,  5.5441e-03, -7.5632e-02,\n",
       "        -1.1891e-01,  3.6709e-02, -1.0153e-01, -3.3007e-02,  1.2996e-03,\n",
       "        -5.4093e-02, -4.5864e-02, -1.0291e-01,  1.7535e-03,  6.1727e-02,\n",
       "        -2.1266e-02, -8.1250e-03, -3.6417e-02,  4.2391e-02,  3.1332e-02,\n",
       "        -4.2671e-02, -1.9608e-03, -4.9377e-02,  1.9157e-02, -3.1282e-03,\n",
       "        -3.2789e-02,  4.4702e-02, -7.5857e-02, -7.6272e-02, -4.3770e-02,\n",
       "        -5.8095e-02, -9.6655e-02, -2.2400e-02, -2.8436e-02,  7.7737e-02,\n",
       "        -8.0331e-02, -4.7472e-02,  7.3887e-03,  2.6763e-02, -5.7786e-02,\n",
       "        -6.0987e-02, -3.2469e-02, -3.1629e-02, -1.2727e-01, -5.1392e-02,\n",
       "        -6.6688e-02, -1.9219e-01, -5.1794e-02,  2.4553e-03,  2.9904e-02,\n",
       "         8.0934e-02, -3.0436e-02,  1.5577e-02, -7.2660e-02, -3.3336e-02,\n",
       "        -5.3065e-02,  1.1671e-01,  1.0026e-04,  1.9790e-03, -6.5624e-02,\n",
       "        -4.3014e-02, -1.0482e-02,  4.8351e-02, -8.9676e-02,  6.5467e-02,\n",
       "        -2.4850e-02,  1.4410e-02, -9.7899e-02, -1.1841e-01,  1.1648e-02,\n",
       "         1.5642e-02,  4.3354e-02, -2.9421e-02, -5.1757e-02, -1.1092e-02,\n",
       "        -3.3445e-02, -2.2702e-02,  8.9328e-02, -6.5749e-02,  1.3442e-02,\n",
       "         3.7518e-02,  3.1988e-02, -9.5302e-02, -4.4028e-02, -1.5638e-02,\n",
       "        -3.1500e-02, -1.3504e-02,  2.7924e-03,  2.2506e-01, -5.3851e-02,\n",
       "        -2.0857e-02,  4.1186e-03, -2.5374e-02, -2.3853e-02,  6.7087e-02,\n",
       "        -1.5815e-02, -3.7528e-03,  3.4893e-02, -2.6880e-02,  4.4248e-02,\n",
       "        -3.1470e-02, -3.4574e-02, -9.8893e-03,  6.6473e-02, -7.7952e-02,\n",
       "        -3.0651e-02, -4.4723e-02,  1.6922e-02, -8.1135e-02, -3.1212e-02,\n",
       "        -2.7336e-02, -4.2488e-02, -9.1422e-02,  6.7794e-02, -3.8465e-02,\n",
       "        -8.5077e-04, -4.9080e-02, -1.6648e-02, -5.0307e-02,  7.3965e-02,\n",
       "        -2.5469e-02, -1.5340e-02, -1.3841e-02, -5.3395e-02,  3.1235e-02,\n",
       "        -4.4601e-02, -3.0874e-02,  1.3887e-02, -1.5264e-02, -4.4242e-02,\n",
       "        -8.1788e-02,  3.6415e-02, -1.0788e-01, -6.9351e-02, -2.4133e-02,\n",
       "        -1.9062e-02,  3.4992e-02, -6.9762e-02, -5.6817e-02, -1.0014e-02,\n",
       "        -5.2109e-02,  6.1807e-02, -9.6213e-02, -3.5930e-02, -9.8491e-02,\n",
       "        -8.2680e-03,  4.1227e-03, -9.1548e-02, -3.5262e-02, -7.2005e-03,\n",
       "        -3.6008e-02, -3.1894e-02, -9.3911e-02,  1.2452e-02,  3.7334e-02,\n",
       "        -3.6833e-02,  1.2549e-02,  6.3336e-02, -2.8293e-02, -6.2196e-02,\n",
       "        -5.1179e-02, -2.1169e-02, -2.2330e-02,  1.7721e-02, -5.7899e-02,\n",
       "        -1.0315e-01, -8.9405e-03, -5.2078e-03, -5.5555e-02,  1.6670e-02,\n",
       "         1.2780e-01,  5.6130e-02,  6.1353e-03, -5.0181e-02,  5.3761e-02,\n",
       "        -1.5701e-03, -2.8827e-02,  5.8172e-02,  2.3896e-02, -7.4967e-02,\n",
       "        -5.9917e-02, -1.2450e-02,  2.4290e-02,  1.0785e-01, -1.6169e-02,\n",
       "        -6.5232e-02,  2.7714e-02, -6.4520e-02,  8.3731e-02, -4.5130e-02,\n",
       "        -1.9344e-03, -4.9180e-02, -2.6595e-02, -2.3764e-02, -5.1821e-02,\n",
       "        -5.2828e-02,  5.9051e-03, -3.1387e-02,  1.5917e-02, -6.3710e-03,\n",
       "        -9.6739e-03, -1.7171e-01, -1.5133e-02, -3.4919e-02, -6.6834e-02,\n",
       "        -2.9504e-02,  7.5407e-03, -5.1922e-03, -7.4846e-02, -1.5151e-02,\n",
       "        -7.9631e-02, -2.6341e-02, -1.5012e-01,  2.2460e-02,  1.0107e-02,\n",
       "        -2.5778e-02, -2.6850e-05, -2.0127e-02, -3.9769e-02, -1.5528e-02,\n",
       "        -2.5348e-02, -6.8829e-02,  3.4717e-02, -2.0053e-03, -5.0096e-02,\n",
       "         5.8816e-02,  4.7402e-02, -1.0247e-01, -2.6462e-02, -4.4910e-02,\n",
       "        -2.1627e-02, -4.0818e-02, -1.4128e-02, -9.3792e-04, -6.7596e-02,\n",
       "        -6.2011e-02, -4.6467e-02, -6.8064e-03,  7.3054e-02, -4.7341e-02,\n",
       "        -1.4687e-02, -3.7106e-02, -1.4129e-01, -9.2913e-02, -1.1632e-02,\n",
       "        -5.4049e-02, -5.8600e-02,  5.4550e-02, -3.9708e-02, -9.0867e-02,\n",
       "        -5.5624e-02, -2.0226e-02, -1.3310e-02,  4.2342e-02, -2.7498e-02,\n",
       "        -1.5261e-02, -5.2250e-02, -7.4362e-02,  5.7417e-02, -6.4251e-02,\n",
       "        -5.5464e-02,  5.9414e-03,  1.3054e-02,  3.2055e-02,  9.2786e-03,\n",
       "         9.6044e-04,  9.5211e-02,  3.8422e-02, -1.6226e-03, -2.5338e-02,\n",
       "        -6.2656e-03, -8.6946e-03, -4.5840e-02, -5.9405e-02, -2.9124e-02,\n",
       "         1.0008e-02, -4.2456e-02,  6.5832e-02,  4.8613e-02, -4.0501e-02,\n",
       "        -4.6531e-02, -1.5976e-02, -4.6823e-02, -1.7073e-02, -1.7184e-04,\n",
       "         9.7579e-02,  1.2385e-02, -1.8580e-02, -2.3771e-02,  5.3708e-03,\n",
       "        -6.2866e-02, -6.6282e-02,  3.2546e-02, -5.4184e-02,  9.1790e-02,\n",
       "        -1.0412e-01, -9.0681e-02,  5.6900e-03, -3.8840e-02, -7.4245e-02,\n",
       "        -4.6258e-02, -1.0141e-02,  2.7948e-03, -9.8402e-02, -5.7454e-02,\n",
       "         8.9371e-03, -4.7653e-02, -3.5983e-05,  3.6370e-02, -6.8060e-03,\n",
       "        -1.2731e-01, -2.8675e-02,  2.9252e-02,  7.4892e-02, -1.2447e-02,\n",
       "        -2.8557e-02,  1.0087e-01,  1.0119e-02,  3.5378e-02,  4.1354e-02,\n",
       "        -3.4999e-02,  6.8073e-02, -8.3200e-02,  1.2770e-03, -5.2315e-02,\n",
       "        -7.4785e-03, -6.4946e-02,  1.3443e-02, -3.5037e-02,  2.9521e-03,\n",
       "        -3.4377e-02, -6.3584e-02, -5.1472e-02,  2.8725e-02, -7.8538e-03,\n",
       "        -1.0991e-03,  1.7426e-02, -6.5711e-02, -1.6871e-02,  2.3383e-02,\n",
       "         2.2133e-02, -4.0960e-02,  1.2284e-02, -8.1919e-02, -2.2731e-02,\n",
       "        -4.7884e-02, -4.9796e-02, -4.8432e-02, -3.7191e-02,  8.0479e-03,\n",
       "        -4.7701e-02,  1.8650e-02, -1.0814e-01,  7.4144e-02, -3.5186e-02,\n",
       "        -1.3576e-01, -6.6604e-02,  3.8105e-02,  1.3714e-02,  7.7252e-03,\n",
       "         5.0489e-02,  4.1594e-02,  5.2363e-02, -5.8934e-02, -2.5082e-02,\n",
       "         1.9394e-02, -6.3943e-03, -3.7295e-03,  3.8858e-02,  1.0877e-02,\n",
       "        -7.3624e-02, -4.5368e-02, -4.7722e-02, -1.0537e-02,  1.5779e-02,\n",
       "        -9.4885e-02,  1.1391e-02,  4.5534e-02, -2.0946e-02,  2.2194e-02,\n",
       "        -2.3659e-02, -2.0593e-02, -9.2396e-02, -5.1322e-02,  3.7030e-02,\n",
       "        -2.6163e-02, -5.7035e-03, -1.9519e-02, -7.4590e-03,  5.5139e-02,\n",
       "        -5.2928e-02, -9.3105e-02, -9.0368e-02, -5.1391e-02,  4.1980e-03,\n",
       "        -1.1540e-01, -2.4661e-02, -1.2784e-02,  3.7718e-02,  9.8530e-03,\n",
       "        -5.9503e-02, -1.6615e-02, -1.0033e-01, -1.2805e-01,  3.3951e-02,\n",
       "        -6.6903e-02, -1.0696e-01, -2.2511e-02,  1.0136e-02, -8.9295e-02,\n",
       "        -5.8106e-02, -1.8917e-02,  3.0799e-02, -8.4209e-02, -4.7490e-02,\n",
       "         3.4659e-02, -2.3828e-02,  1.0109e-02, -2.2014e-02,  1.3713e-01,\n",
       "        -8.5138e-02,  4.2718e-03,  3.1069e-03,  2.6038e-02, -1.7890e-02,\n",
       "        -3.1987e-02, -7.9456e-02, -2.1475e-02, -9.4370e-02,  8.7895e-03,\n",
       "        -2.8219e-02, -4.0465e-02, -1.3446e-02, -5.4235e-02,  3.6914e-02,\n",
       "        -5.9405e-02, -4.1999e-02,  7.6936e-03, -2.3912e-02,  2.7201e-02,\n",
       "        -8.1045e-02, -6.2674e-02,  6.0316e-03, -8.3956e-03, -9.7366e-02,\n",
       "        -2.8905e-02, -6.4466e-02, -4.1084e-02, -1.5809e-02, -3.3628e-02,\n",
       "        -7.5521e-04,  1.1060e-02, -3.0861e-02, -2.7453e-02, -2.2560e-02,\n",
       "        -4.0385e-02,  1.1128e-02,  3.2616e-02,  1.9923e-02, -5.2315e-02,\n",
       "        -6.3481e-02,  2.8915e-02, -4.1004e-02,  2.9585e-02,  7.3443e-02,\n",
       "        -3.7008e-02, -1.3618e-02,  7.0967e-02,  6.3854e-02, -3.6539e-02,\n",
       "         5.6841e-03, -2.3552e-02, -2.3075e-02, -1.0181e-01, -3.1658e-02,\n",
       "         7.5154e-02, -6.8950e-02,  1.7453e-02, -4.2651e-02, -8.9301e-02,\n",
       "        -1.1395e-01, -5.5166e-02, -6.5374e-02, -1.0342e-01, -1.0614e-01,\n",
       "        -2.7443e-03, -2.7233e-02, -7.5034e-03,  1.6079e-02, -3.1732e-02,\n",
       "         2.1911e-02, -3.8058e-02,  1.2087e-02,  1.9566e-02, -2.1590e-02,\n",
       "        -1.8401e-02, -5.6781e-02,  6.9947e-02,  3.7185e-02, -6.0096e-03,\n",
       "        -5.5889e-02, -7.1291e-04, -2.1571e-02, -1.1587e-01, -1.2091e-02,\n",
       "         1.6871e-02,  2.5919e-02, -1.1262e-01, -9.1336e-02, -4.0920e-02,\n",
       "        -8.1354e-03,  6.3578e-02, -1.0269e-02,  6.2887e-02, -7.6906e-02,\n",
       "        -2.2190e-02, -3.0619e-04, -1.7264e-02,  1.4001e-02, -6.2175e-02,\n",
       "        -3.0614e-02, -7.7598e-03,  7.8681e-02, -1.1139e-01, -9.4248e-02,\n",
       "        -5.5925e-02,  1.9979e-02, -4.3891e-02, -5.6174e-02, -7.6827e-02,\n",
       "         5.8949e-02,  2.0870e-02, -6.9111e-02, -7.9080e-02, -2.0177e-03,\n",
       "        -2.2291e-02, -1.0346e-01,  2.1730e-02,  7.4742e-02, -3.9678e-02,\n",
       "        -7.2509e-02, -1.7938e-02,  8.0589e-03, -4.5611e-02, -8.8949e-03,\n",
       "         1.6488e-02,  3.7573e-02, -1.7079e-02])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1.state_dict()['encoder.layer.11.output.LayerNorm.bias']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "10e8e073",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-0.0063)\n",
      "tensor(-0.0199)\n",
      "tensor(0.0463)\n",
      "tensor(0.0159)\n",
      "tensor(-0.0426)\n",
      "tensor(-0.0133)\n",
      "tensor(-0.0091)\n",
      "tensor(-0.0335)\n",
      "tensor(0.0258)\n",
      "tensor(-0.0365)\n",
      "tensor(-0.0612)\n",
      "tensor(0.0304)\n",
      "tensor(0.0172)\n",
      "tensor(0.0122)\n",
      "tensor(-0.0033)\n",
      "tensor(-0.0485)\n",
      "tensor(0.0005)\n",
      "tensor(-0.0924)\n",
      "tensor(0.0104)\n",
      "tensor(0.0119)\n",
      "tensor(-0.0588)\n",
      "tensor(-0.0068)\n",
      "tensor(0.0044)\n",
      "tensor(-0.0453)\n",
      "tensor(0.0056)\n",
      "tensor(-0.0411)\n",
      "tensor(-0.0482)\n",
      "tensor(-0.0516)\n",
      "tensor(-0.0915)\n",
      "tensor(-0.0405)\n",
      "tensor(-0.0271)\n",
      "tensor(-0.0467)\n",
      "tensor(-0.0116)\n",
      "tensor(-0.0763)\n",
      "tensor(-0.0421)\n",
      "tensor(-0.0575)\n",
      "tensor(-0.0277)\n",
      "tensor(-0.0774)\n",
      "tensor(0.0070)\n",
      "tensor(-0.0595)\n",
      "tensor(0.0536)\n",
      "tensor(-0.0210)\n",
      "tensor(-0.0458)\n",
      "tensor(-0.0692)\n",
      "tensor(-0.1084)\n",
      "tensor(-0.0191)\n",
      "tensor(-0.1354)\n",
      "tensor(-0.0017)\n",
      "tensor(-0.1096)\n",
      "tensor(-0.1180)\n",
      "tensor(-0.0212)\n",
      "tensor(0.0450)\n",
      "tensor(0.0343)\n",
      "tensor(-0.0251)\n",
      "tensor(-0.0885)\n",
      "tensor(-0.0089)\n",
      "tensor(-0.0022)\n",
      "tensor(-0.0621)\n",
      "tensor(-0.0005)\n",
      "tensor(-0.0696)\n",
      "tensor(0.0439)\n",
      "tensor(-0.0840)\n",
      "tensor(0.0205)\n",
      "tensor(0.0505)\n",
      "tensor(-0.0439)\n",
      "tensor(-0.0475)\n",
      "tensor(-0.0009)\n",
      "tensor(-0.0591)\n",
      "tensor(0.0331)\n",
      "tensor(0.0009)\n",
      "tensor(0.0220)\n",
      "tensor(-0.0233)\n",
      "tensor(-0.0666)\n",
      "tensor(-0.0542)\n",
      "tensor(-0.0128)\n",
      "tensor(-0.0853)\n",
      "tensor(0.0488)\n",
      "tensor(-0.0491)\n",
      "tensor(0.0409)\n",
      "tensor(0.0316)\n",
      "tensor(0.0347)\n",
      "tensor(-0.0349)\n",
      "tensor(-0.0510)\n",
      "tensor(0.0196)\n",
      "tensor(-0.0577)\n",
      "tensor(-0.0372)\n",
      "tensor(0.0287)\n",
      "tensor(-0.0133)\n",
      "tensor(-0.0472)\n",
      "tensor(0.0300)\n",
      "tensor(0.0148)\n",
      "tensor(-0.0775)\n",
      "tensor(0.0535)\n",
      "tensor(-0.0151)\n",
      "tensor(-0.0420)\n",
      "tensor(-0.1165)\n",
      "tensor(-0.0400)\n",
      "tensor(-0.0145)\n",
      "tensor(0.0296)\n",
      "tensor(-0.0098)\n",
      "tensor(-0.0818)\n",
      "tensor(-0.0756)\n",
      "tensor(-0.0905)\n",
      "tensor(-0.0289)\n",
      "tensor(-0.1093)\n",
      "tensor(-0.0191)\n",
      "tensor(-0.0283)\n",
      "tensor(0.0483)\n",
      "tensor(-0.0512)\n",
      "tensor(-0.0230)\n",
      "tensor(-0.0358)\n",
      "tensor(-0.0381)\n",
      "tensor(0.0124)\n",
      "tensor(0.0200)\n",
      "tensor(-0.0205)\n",
      "tensor(0.0358)\n",
      "tensor(-0.0030)\n",
      "tensor(0.0356)\n",
      "tensor(-0.0023)\n",
      "tensor(-0.0074)\n",
      "tensor(0.0350)\n",
      "tensor(0.1166)\n",
      "tensor(-0.0032)\n",
      "tensor(-0.0330)\n",
      "tensor(0.0111)\n",
      "tensor(0.0501)\n",
      "tensor(0.0101)\n",
      "tensor(-0.0036)\n",
      "tensor(-0.0345)\n",
      "tensor(-0.1281)\n",
      "tensor(-0.0063)\n",
      "tensor(-0.0014)\n",
      "tensor(-0.0064)\n",
      "tensor(-0.0340)\n",
      "tensor(0.0644)\n",
      "tensor(-0.1115)\n",
      "tensor(-0.0453)\n",
      "tensor(-0.0024)\n",
      "tensor(0.0390)\n",
      "tensor(-0.0354)\n",
      "tensor(-0.0119)\n",
      "tensor(-0.0048)\n",
      "tensor(0.0399)\n",
      "tensor(-0.0288)\n",
      "tensor(-0.0155)\n",
      "tensor(-0.0864)\n",
      "tensor(-0.0371)\n",
      "tensor(0.0298)\n",
      "tensor(-0.0666)\n",
      "tensor(0.0320)\n",
      "tensor(0.0056)\n",
      "tensor(0.0574)\n",
      "tensor(0.0730)\n",
      "tensor(-0.0709)\n",
      "tensor(0.0160)\n",
      "tensor(0.0833)\n",
      "tensor(-0.0142)\n",
      "tensor(-0.0705)\n",
      "tensor(-0.1179)\n",
      "tensor(-0.0117)\n",
      "tensor(0.0238)\n",
      "tensor(0.0519)\n",
      "tensor(-0.0185)\n",
      "tensor(0.0510)\n",
      "tensor(-0.0660)\n",
      "tensor(-0.0709)\n",
      "tensor(0.0037)\n",
      "tensor(-0.0946)\n",
      "tensor(-0.0504)\n",
      "tensor(-0.0525)\n",
      "tensor(-0.0241)\n",
      "tensor(-0.0171)\n",
      "tensor(0.0187)\n",
      "tensor(-0.0051)\n",
      "tensor(-0.0790)\n",
      "tensor(0.0577)\n",
      "tensor(0.0937)\n",
      "tensor(-0.0334)\n",
      "tensor(-0.0504)\n",
      "tensor(-0.0795)\n",
      "tensor(0.1305)\n",
      "tensor(0.0150)\n",
      "tensor(-0.0173)\n",
      "tensor(-0.0382)\n",
      "tensor(-0.0147)\n",
      "tensor(-0.0601)\n",
      "tensor(-0.0620)\n",
      "tensor(0.0064)\n",
      "tensor(0.0339)\n",
      "tensor(0.0092)\n",
      "tensor(-0.0337)\n",
      "tensor(-0.1107)\n",
      "tensor(-0.0105)\n",
      "tensor(-0.0006)\n",
      "tensor(0.0440)\n",
      "tensor(-0.0511)\n",
      "tensor(0.0103)\n",
      "tensor(0.1222)\n",
      "tensor(-0.0070)\n",
      "tensor(0.0282)\n",
      "tensor(-0.0448)\n",
      "tensor(-0.0274)\n",
      "tensor(0.0392)\n",
      "tensor(0.0512)\n",
      "tensor(-0.0858)\n",
      "tensor(0.0438)\n",
      "tensor(0.0429)\n",
      "tensor(-0.0627)\n",
      "tensor(0.0440)\n",
      "tensor(-0.0612)\n",
      "tensor(-0.0134)\n",
      "tensor(-0.0694)\n",
      "tensor(0.0430)\n",
      "tensor(-0.0888)\n",
      "tensor(-0.0045)\n",
      "tensor(-0.0297)\n",
      "tensor(0.0560)\n",
      "tensor(-0.0686)\n",
      "tensor(-0.0491)\n",
      "tensor(0.0045)\n",
      "tensor(0.0328)\n",
      "tensor(-0.0626)\n",
      "tensor(-0.0191)\n",
      "tensor(0.0055)\n",
      "tensor(-0.0756)\n",
      "tensor(-0.1189)\n",
      "tensor(0.0367)\n",
      "tensor(-0.1015)\n",
      "tensor(-0.0330)\n",
      "tensor(0.0013)\n",
      "tensor(-0.0541)\n",
      "tensor(-0.0459)\n",
      "tensor(-0.1029)\n",
      "tensor(0.0018)\n",
      "tensor(0.0617)\n",
      "tensor(-0.0213)\n",
      "tensor(-0.0081)\n",
      "tensor(-0.0364)\n",
      "tensor(0.0424)\n",
      "tensor(0.0313)\n",
      "tensor(-0.0427)\n",
      "tensor(-0.0020)\n",
      "tensor(-0.0494)\n",
      "tensor(0.0192)\n",
      "tensor(-0.0031)\n",
      "tensor(-0.0328)\n",
      "tensor(0.0447)\n",
      "tensor(-0.0759)\n",
      "tensor(-0.0763)\n",
      "tensor(-0.0438)\n",
      "tensor(-0.0581)\n",
      "tensor(-0.0967)\n",
      "tensor(-0.0224)\n",
      "tensor(-0.0284)\n",
      "tensor(0.0777)\n",
      "tensor(-0.0803)\n",
      "tensor(-0.0475)\n",
      "tensor(0.0074)\n",
      "tensor(0.0268)\n",
      "tensor(-0.0578)\n",
      "tensor(-0.0610)\n",
      "tensor(-0.0325)\n",
      "tensor(-0.0316)\n",
      "tensor(-0.1273)\n",
      "tensor(-0.0514)\n",
      "tensor(-0.0667)\n",
      "tensor(-0.1922)\n",
      "tensor(-0.0518)\n",
      "tensor(0.0025)\n",
      "tensor(0.0299)\n",
      "tensor(0.0809)\n",
      "tensor(-0.0304)\n",
      "tensor(0.0156)\n",
      "tensor(-0.0727)\n",
      "tensor(-0.0333)\n",
      "tensor(-0.0531)\n",
      "tensor(0.1167)\n",
      "tensor(0.0001)\n",
      "tensor(0.0020)\n",
      "tensor(-0.0656)\n",
      "tensor(-0.0430)\n",
      "tensor(-0.0105)\n",
      "tensor(0.0484)\n",
      "tensor(-0.0897)\n",
      "tensor(0.0655)\n",
      "tensor(-0.0249)\n",
      "tensor(0.0144)\n",
      "tensor(-0.0979)\n",
      "tensor(-0.1184)\n",
      "tensor(0.0116)\n",
      "tensor(0.0156)\n",
      "tensor(0.0434)\n",
      "tensor(-0.0294)\n",
      "tensor(-0.0518)\n",
      "tensor(-0.0111)\n",
      "tensor(-0.0334)\n",
      "tensor(-0.0227)\n",
      "tensor(0.0893)\n",
      "tensor(-0.0657)\n",
      "tensor(0.0134)\n",
      "tensor(0.0375)\n",
      "tensor(0.0320)\n",
      "tensor(-0.0953)\n",
      "tensor(-0.0440)\n",
      "tensor(-0.0156)\n",
      "tensor(-0.0315)\n",
      "tensor(-0.0135)\n",
      "tensor(0.0028)\n",
      "tensor(0.2251)\n",
      "tensor(-0.0539)\n",
      "tensor(-0.0209)\n",
      "tensor(0.0041)\n",
      "tensor(-0.0254)\n",
      "tensor(-0.0239)\n",
      "tensor(0.0671)\n",
      "tensor(-0.0158)\n",
      "tensor(-0.0038)\n",
      "tensor(0.0349)\n",
      "tensor(-0.0269)\n",
      "tensor(0.0442)\n",
      "tensor(-0.0315)\n",
      "tensor(-0.0346)\n",
      "tensor(-0.0099)\n",
      "tensor(0.0665)\n",
      "tensor(-0.0780)\n",
      "tensor(-0.0307)\n",
      "tensor(-0.0447)\n",
      "tensor(0.0169)\n",
      "tensor(-0.0811)\n",
      "tensor(-0.0312)\n",
      "tensor(-0.0273)\n",
      "tensor(-0.0425)\n",
      "tensor(-0.0914)\n",
      "tensor(0.0678)\n",
      "tensor(-0.0385)\n",
      "tensor(-0.0009)\n",
      "tensor(-0.0491)\n",
      "tensor(-0.0166)\n",
      "tensor(-0.0503)\n",
      "tensor(0.0740)\n",
      "tensor(-0.0255)\n",
      "tensor(-0.0153)\n",
      "tensor(-0.0138)\n",
      "tensor(-0.0534)\n",
      "tensor(0.0312)\n",
      "tensor(-0.0446)\n",
      "tensor(-0.0309)\n",
      "tensor(0.0139)\n",
      "tensor(-0.0153)\n",
      "tensor(-0.0442)\n",
      "tensor(-0.0818)\n",
      "tensor(0.0364)\n",
      "tensor(-0.1079)\n",
      "tensor(-0.0694)\n",
      "tensor(-0.0241)\n",
      "tensor(-0.0191)\n",
      "tensor(0.0350)\n",
      "tensor(-0.0698)\n",
      "tensor(-0.0568)\n",
      "tensor(-0.0100)\n",
      "tensor(-0.0521)\n",
      "tensor(0.0618)\n",
      "tensor(-0.0962)\n",
      "tensor(-0.0359)\n",
      "tensor(-0.0985)\n",
      "tensor(-0.0083)\n",
      "tensor(0.0041)\n",
      "tensor(-0.0915)\n",
      "tensor(-0.0353)\n",
      "tensor(-0.0072)\n",
      "tensor(-0.0360)\n",
      "tensor(-0.0319)\n",
      "tensor(-0.0939)\n",
      "tensor(0.0125)\n",
      "tensor(0.0373)\n",
      "tensor(-0.0368)\n",
      "tensor(0.0125)\n",
      "tensor(0.0633)\n",
      "tensor(-0.0283)\n",
      "tensor(-0.0622)\n",
      "tensor(-0.0512)\n",
      "tensor(-0.0212)\n",
      "tensor(-0.0223)\n",
      "tensor(0.0177)\n",
      "tensor(-0.0579)\n",
      "tensor(-0.1032)\n",
      "tensor(-0.0089)\n",
      "tensor(-0.0052)\n",
      "tensor(-0.0556)\n",
      "tensor(0.0167)\n",
      "tensor(0.1278)\n",
      "tensor(0.0561)\n",
      "tensor(0.0061)\n",
      "tensor(-0.0502)\n",
      "tensor(0.0538)\n",
      "tensor(-0.0016)\n",
      "tensor(-0.0288)\n",
      "tensor(0.0582)\n",
      "tensor(0.0239)\n",
      "tensor(-0.0750)\n",
      "tensor(-0.0599)\n",
      "tensor(-0.0125)\n",
      "tensor(0.0243)\n",
      "tensor(0.1078)\n",
      "tensor(-0.0162)\n",
      "tensor(-0.0652)\n",
      "tensor(0.0277)\n",
      "tensor(-0.0645)\n",
      "tensor(0.0837)\n",
      "tensor(-0.0451)\n",
      "tensor(-0.0019)\n",
      "tensor(-0.0492)\n",
      "tensor(-0.0266)\n",
      "tensor(-0.0238)\n",
      "tensor(-0.0518)\n",
      "tensor(-0.0528)\n",
      "tensor(0.0059)\n",
      "tensor(-0.0314)\n",
      "tensor(0.0159)\n",
      "tensor(-0.0064)\n",
      "tensor(-0.0097)\n",
      "tensor(-0.1717)\n",
      "tensor(-0.0151)\n",
      "tensor(-0.0349)\n",
      "tensor(-0.0668)\n",
      "tensor(-0.0295)\n",
      "tensor(0.0075)\n",
      "tensor(-0.0052)\n",
      "tensor(-0.0748)\n",
      "tensor(-0.0152)\n",
      "tensor(-0.0796)\n",
      "tensor(-0.0263)\n",
      "tensor(-0.1501)\n",
      "tensor(0.0225)\n",
      "tensor(0.0101)\n",
      "tensor(-0.0258)\n",
      "tensor(-2.6850e-05)\n",
      "tensor(-0.0201)\n",
      "tensor(-0.0398)\n",
      "tensor(-0.0155)\n",
      "tensor(-0.0253)\n",
      "tensor(-0.0688)\n",
      "tensor(0.0347)\n",
      "tensor(-0.0020)\n",
      "tensor(-0.0501)\n",
      "tensor(0.0588)\n",
      "tensor(0.0474)\n",
      "tensor(-0.1025)\n",
      "tensor(-0.0265)\n",
      "tensor(-0.0449)\n",
      "tensor(-0.0216)\n",
      "tensor(-0.0408)\n",
      "tensor(-0.0141)\n",
      "tensor(-0.0009)\n",
      "tensor(-0.0676)\n",
      "tensor(-0.0620)\n",
      "tensor(-0.0465)\n",
      "tensor(-0.0068)\n",
      "tensor(0.0731)\n",
      "tensor(-0.0473)\n",
      "tensor(-0.0147)\n",
      "tensor(-0.0371)\n",
      "tensor(-0.1413)\n",
      "tensor(-0.0929)\n",
      "tensor(-0.0116)\n",
      "tensor(-0.0540)\n",
      "tensor(-0.0586)\n",
      "tensor(0.0546)\n",
      "tensor(-0.0397)\n",
      "tensor(-0.0909)\n",
      "tensor(-0.0556)\n",
      "tensor(-0.0202)\n",
      "tensor(-0.0133)\n",
      "tensor(0.0423)\n",
      "tensor(-0.0275)\n",
      "tensor(-0.0153)\n",
      "tensor(-0.0523)\n",
      "tensor(-0.0744)\n",
      "tensor(0.0574)\n",
      "tensor(-0.0643)\n",
      "tensor(-0.0555)\n",
      "tensor(0.0059)\n",
      "tensor(0.0131)\n",
      "tensor(0.0321)\n",
      "tensor(0.0093)\n",
      "tensor(0.0010)\n",
      "tensor(0.0952)\n",
      "tensor(0.0384)\n",
      "tensor(-0.0016)\n",
      "tensor(-0.0253)\n",
      "tensor(-0.0063)\n",
      "tensor(-0.0087)\n",
      "tensor(-0.0458)\n",
      "tensor(-0.0594)\n",
      "tensor(-0.0291)\n",
      "tensor(0.0100)\n",
      "tensor(-0.0425)\n",
      "tensor(0.0658)\n",
      "tensor(0.0486)\n",
      "tensor(-0.0405)\n",
      "tensor(-0.0465)\n",
      "tensor(-0.0160)\n",
      "tensor(-0.0468)\n",
      "tensor(-0.0171)\n",
      "tensor(-0.0002)\n",
      "tensor(0.0976)\n",
      "tensor(0.0124)\n",
      "tensor(-0.0186)\n",
      "tensor(-0.0238)\n",
      "tensor(0.0054)\n",
      "tensor(-0.0629)\n",
      "tensor(-0.0663)\n",
      "tensor(0.0325)\n",
      "tensor(-0.0542)\n",
      "tensor(0.0918)\n",
      "tensor(-0.1041)\n",
      "tensor(-0.0907)\n",
      "tensor(0.0057)\n",
      "tensor(-0.0388)\n",
      "tensor(-0.0742)\n",
      "tensor(-0.0463)\n",
      "tensor(-0.0101)\n",
      "tensor(0.0028)\n",
      "tensor(-0.0984)\n",
      "tensor(-0.0575)\n",
      "tensor(0.0089)\n",
      "tensor(-0.0477)\n",
      "tensor(-3.5983e-05)\n",
      "tensor(0.0364)\n",
      "tensor(-0.0068)\n",
      "tensor(-0.1273)\n",
      "tensor(-0.0287)\n",
      "tensor(0.0293)\n",
      "tensor(0.0749)\n",
      "tensor(-0.0124)\n",
      "tensor(-0.0286)\n",
      "tensor(0.1009)\n",
      "tensor(0.0101)\n",
      "tensor(0.0354)\n",
      "tensor(0.0414)\n",
      "tensor(-0.0350)\n",
      "tensor(0.0681)\n",
      "tensor(-0.0832)\n",
      "tensor(0.0013)\n",
      "tensor(-0.0523)\n",
      "tensor(-0.0075)\n",
      "tensor(-0.0649)\n",
      "tensor(0.0134)\n",
      "tensor(-0.0350)\n",
      "tensor(0.0030)\n",
      "tensor(-0.0344)\n",
      "tensor(-0.0636)\n",
      "tensor(-0.0515)\n",
      "tensor(0.0287)\n",
      "tensor(-0.0079)\n",
      "tensor(-0.0011)\n",
      "tensor(0.0174)\n",
      "tensor(-0.0657)\n",
      "tensor(-0.0169)\n",
      "tensor(0.0234)\n",
      "tensor(0.0221)\n",
      "tensor(-0.0410)\n",
      "tensor(0.0123)\n",
      "tensor(-0.0819)\n",
      "tensor(-0.0227)\n",
      "tensor(-0.0479)\n",
      "tensor(-0.0498)\n",
      "tensor(-0.0484)\n",
      "tensor(-0.0372)\n",
      "tensor(0.0080)\n",
      "tensor(-0.0477)\n",
      "tensor(0.0186)\n",
      "tensor(-0.1081)\n",
      "tensor(0.0741)\n",
      "tensor(-0.0352)\n",
      "tensor(-0.1358)\n",
      "tensor(-0.0666)\n",
      "tensor(0.0381)\n",
      "tensor(0.0137)\n",
      "tensor(0.0077)\n",
      "tensor(0.0505)\n",
      "tensor(0.0416)\n",
      "tensor(0.0524)\n",
      "tensor(-0.0589)\n",
      "tensor(-0.0251)\n",
      "tensor(0.0194)\n",
      "tensor(-0.0064)\n",
      "tensor(-0.0037)\n",
      "tensor(0.0389)\n",
      "tensor(0.0109)\n",
      "tensor(-0.0736)\n",
      "tensor(-0.0454)\n",
      "tensor(-0.0477)\n",
      "tensor(-0.0105)\n",
      "tensor(0.0158)\n",
      "tensor(-0.0949)\n",
      "tensor(0.0114)\n",
      "tensor(0.0455)\n",
      "tensor(-0.0209)\n",
      "tensor(0.0222)\n",
      "tensor(-0.0237)\n",
      "tensor(-0.0206)\n",
      "tensor(-0.0924)\n",
      "tensor(-0.0513)\n",
      "tensor(0.0370)\n",
      "tensor(-0.0262)\n",
      "tensor(-0.0057)\n",
      "tensor(-0.0195)\n",
      "tensor(-0.0075)\n",
      "tensor(0.0551)\n",
      "tensor(-0.0529)\n",
      "tensor(-0.0931)\n",
      "tensor(-0.0904)\n",
      "tensor(-0.0514)\n",
      "tensor(0.0042)\n",
      "tensor(-0.1154)\n",
      "tensor(-0.0247)\n",
      "tensor(-0.0128)\n",
      "tensor(0.0377)\n",
      "tensor(0.0099)\n",
      "tensor(-0.0595)\n",
      "tensor(-0.0166)\n",
      "tensor(-0.1003)\n",
      "tensor(-0.1280)\n",
      "tensor(0.0340)\n",
      "tensor(-0.0669)\n",
      "tensor(-0.1070)\n",
      "tensor(-0.0225)\n",
      "tensor(0.0101)\n",
      "tensor(-0.0893)\n",
      "tensor(-0.0581)\n",
      "tensor(-0.0189)\n",
      "tensor(0.0308)\n",
      "tensor(-0.0842)\n",
      "tensor(-0.0475)\n",
      "tensor(0.0347)\n",
      "tensor(-0.0238)\n",
      "tensor(0.0101)\n",
      "tensor(-0.0220)\n",
      "tensor(0.1371)\n",
      "tensor(-0.0851)\n",
      "tensor(0.0043)\n",
      "tensor(0.0031)\n",
      "tensor(0.0260)\n",
      "tensor(-0.0179)\n",
      "tensor(-0.0320)\n",
      "tensor(-0.0795)\n",
      "tensor(-0.0215)\n",
      "tensor(-0.0944)\n",
      "tensor(0.0088)\n",
      "tensor(-0.0282)\n",
      "tensor(-0.0405)\n",
      "tensor(-0.0134)\n",
      "tensor(-0.0542)\n",
      "tensor(0.0369)\n",
      "tensor(-0.0594)\n",
      "tensor(-0.0420)\n",
      "tensor(0.0077)\n",
      "tensor(-0.0239)\n",
      "tensor(0.0272)\n",
      "tensor(-0.0810)\n",
      "tensor(-0.0627)\n",
      "tensor(0.0060)\n",
      "tensor(-0.0084)\n",
      "tensor(-0.0974)\n",
      "tensor(-0.0289)\n",
      "tensor(-0.0645)\n",
      "tensor(-0.0411)\n",
      "tensor(-0.0158)\n",
      "tensor(-0.0336)\n",
      "tensor(-0.0008)\n",
      "tensor(0.0111)\n",
      "tensor(-0.0309)\n",
      "tensor(-0.0275)\n",
      "tensor(-0.0226)\n",
      "tensor(-0.0404)\n",
      "tensor(0.0111)\n",
      "tensor(0.0326)\n",
      "tensor(0.0199)\n",
      "tensor(-0.0523)\n",
      "tensor(-0.0635)\n",
      "tensor(0.0289)\n",
      "tensor(-0.0410)\n",
      "tensor(0.0296)\n",
      "tensor(0.0734)\n",
      "tensor(-0.0370)\n",
      "tensor(-0.0136)\n",
      "tensor(0.0710)\n",
      "tensor(0.0639)\n",
      "tensor(-0.0365)\n",
      "tensor(0.0057)\n",
      "tensor(-0.0236)\n",
      "tensor(-0.0231)\n",
      "tensor(-0.1018)\n",
      "tensor(-0.0317)\n",
      "tensor(0.0752)\n",
      "tensor(-0.0689)\n",
      "tensor(0.0175)\n",
      "tensor(-0.0427)\n",
      "tensor(-0.0893)\n",
      "tensor(-0.1139)\n",
      "tensor(-0.0552)\n",
      "tensor(-0.0654)\n",
      "tensor(-0.1034)\n",
      "tensor(-0.1061)\n",
      "tensor(-0.0027)\n",
      "tensor(-0.0272)\n",
      "tensor(-0.0075)\n",
      "tensor(0.0161)\n",
      "tensor(-0.0317)\n",
      "tensor(0.0219)\n",
      "tensor(-0.0381)\n",
      "tensor(0.0121)\n",
      "tensor(0.0196)\n",
      "tensor(-0.0216)\n",
      "tensor(-0.0184)\n",
      "tensor(-0.0568)\n",
      "tensor(0.0699)\n",
      "tensor(0.0372)\n",
      "tensor(-0.0060)\n",
      "tensor(-0.0559)\n",
      "tensor(-0.0007)\n",
      "tensor(-0.0216)\n",
      "tensor(-0.1159)\n",
      "tensor(-0.0121)\n",
      "tensor(0.0169)\n",
      "tensor(0.0259)\n",
      "tensor(-0.1126)\n",
      "tensor(-0.0913)\n",
      "tensor(-0.0409)\n",
      "tensor(-0.0081)\n",
      "tensor(0.0636)\n",
      "tensor(-0.0103)\n",
      "tensor(0.0629)\n",
      "tensor(-0.0769)\n",
      "tensor(-0.0222)\n",
      "tensor(-0.0003)\n",
      "tensor(-0.0173)\n",
      "tensor(0.0140)\n",
      "tensor(-0.0622)\n",
      "tensor(-0.0306)\n",
      "tensor(-0.0078)\n",
      "tensor(0.0787)\n",
      "tensor(-0.1114)\n",
      "tensor(-0.0942)\n",
      "tensor(-0.0559)\n",
      "tensor(0.0200)\n",
      "tensor(-0.0439)\n",
      "tensor(-0.0562)\n",
      "tensor(-0.0768)\n",
      "tensor(0.0589)\n",
      "tensor(0.0209)\n",
      "tensor(-0.0691)\n",
      "tensor(-0.0791)\n",
      "tensor(-0.0020)\n",
      "tensor(-0.0223)\n",
      "tensor(-0.1035)\n",
      "tensor(0.0217)\n",
      "tensor(0.0747)\n",
      "tensor(-0.0397)\n",
      "tensor(-0.0725)\n",
      "tensor(-0.0179)\n",
      "tensor(0.0081)\n",
      "tensor(-0.0456)\n",
      "tensor(-0.0089)\n",
      "tensor(0.0165)\n",
      "tensor(0.0376)\n",
      "tensor(-0.0171)\n"
     ]
    }
   ],
   "source": [
    "for x in model1.state_dict()['encoder.layer.11.output.LayerNorm.bias']:\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6a71865f",
   "metadata": {},
   "outputs": [],
   "source": [
    "state_dict = model.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f2265c0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "keys_to_delete = ['cls.predictions.decoder.bias']  # Example keys\n",
    "for key in keys_to_delete:\n",
    "    if key in state_dict:\n",
    "        del state_dict[key]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8d2b7ee8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert.embeddings.word_embeddings.weight\n",
      "bert.embeddings.position_embeddings.weight\n",
      "bert.embeddings.token_type_embeddings.weight\n",
      "bert.embeddings.LayerNorm.weight\n",
      "bert.embeddings.LayerNorm.bias\n",
      "bert.encoder.layer.0.attention.self.query.weight\n",
      "bert.encoder.layer.0.attention.self.query.bias\n",
      "bert.encoder.layer.0.attention.self.key.weight\n",
      "bert.encoder.layer.0.attention.self.key.bias\n",
      "bert.encoder.layer.0.attention.self.value.weight\n",
      "bert.encoder.layer.0.attention.self.value.bias\n",
      "bert.encoder.layer.0.attention.output.dense.weight\n",
      "bert.encoder.layer.0.attention.output.dense.bias\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.weight\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.bias\n",
      "bert.encoder.layer.0.intermediate.dense.weight\n",
      "bert.encoder.layer.0.intermediate.dense.bias\n",
      "bert.encoder.layer.0.output.dense.weight\n",
      "bert.encoder.layer.0.output.dense.bias\n",
      "bert.encoder.layer.0.output.LayerNorm.weight\n",
      "bert.encoder.layer.0.output.LayerNorm.bias\n",
      "bert.encoder.layer.1.attention.self.query.weight\n",
      "bert.encoder.layer.1.attention.self.query.bias\n",
      "bert.encoder.layer.1.attention.self.key.weight\n",
      "bert.encoder.layer.1.attention.self.key.bias\n",
      "bert.encoder.layer.1.attention.self.value.weight\n",
      "bert.encoder.layer.1.attention.self.value.bias\n",
      "bert.encoder.layer.1.attention.output.dense.weight\n",
      "bert.encoder.layer.1.attention.output.dense.bias\n",
      "bert.encoder.layer.1.attention.output.LayerNorm.weight\n",
      "bert.encoder.layer.1.attention.output.LayerNorm.bias\n",
      "bert.encoder.layer.1.intermediate.dense.weight\n",
      "bert.encoder.layer.1.intermediate.dense.bias\n",
      "bert.encoder.layer.1.output.dense.weight\n",
      "bert.encoder.layer.1.output.dense.bias\n",
      "bert.encoder.layer.1.output.LayerNorm.weight\n",
      "bert.encoder.layer.1.output.LayerNorm.bias\n",
      "bert.encoder.layer.2.attention.self.query.weight\n",
      "bert.encoder.layer.2.attention.self.query.bias\n",
      "bert.encoder.layer.2.attention.self.key.weight\n",
      "bert.encoder.layer.2.attention.self.key.bias\n",
      "bert.encoder.layer.2.attention.self.value.weight\n",
      "bert.encoder.layer.2.attention.self.value.bias\n",
      "bert.encoder.layer.2.attention.output.dense.weight\n",
      "bert.encoder.layer.2.attention.output.dense.bias\n",
      "bert.encoder.layer.2.attention.output.LayerNorm.weight\n",
      "bert.encoder.layer.2.attention.output.LayerNorm.bias\n",
      "bert.encoder.layer.2.intermediate.dense.weight\n",
      "bert.encoder.layer.2.intermediate.dense.bias\n",
      "bert.encoder.layer.2.output.dense.weight\n",
      "bert.encoder.layer.2.output.dense.bias\n",
      "bert.encoder.layer.2.output.LayerNorm.weight\n",
      "bert.encoder.layer.2.output.LayerNorm.bias\n",
      "bert.encoder.layer.3.attention.self.query.weight\n",
      "bert.encoder.layer.3.attention.self.query.bias\n",
      "bert.encoder.layer.3.attention.self.key.weight\n",
      "bert.encoder.layer.3.attention.self.key.bias\n",
      "bert.encoder.layer.3.attention.self.value.weight\n",
      "bert.encoder.layer.3.attention.self.value.bias\n",
      "bert.encoder.layer.3.attention.output.dense.weight\n",
      "bert.encoder.layer.3.attention.output.dense.bias\n",
      "bert.encoder.layer.3.attention.output.LayerNorm.weight\n",
      "bert.encoder.layer.3.attention.output.LayerNorm.bias\n",
      "bert.encoder.layer.3.intermediate.dense.weight\n",
      "bert.encoder.layer.3.intermediate.dense.bias\n",
      "bert.encoder.layer.3.output.dense.weight\n",
      "bert.encoder.layer.3.output.dense.bias\n",
      "bert.encoder.layer.3.output.LayerNorm.weight\n",
      "bert.encoder.layer.3.output.LayerNorm.bias\n",
      "bert.encoder.layer.4.attention.self.query.weight\n",
      "bert.encoder.layer.4.attention.self.query.bias\n",
      "bert.encoder.layer.4.attention.self.key.weight\n",
      "bert.encoder.layer.4.attention.self.key.bias\n",
      "bert.encoder.layer.4.attention.self.value.weight\n",
      "bert.encoder.layer.4.attention.self.value.bias\n",
      "bert.encoder.layer.4.attention.output.dense.weight\n",
      "bert.encoder.layer.4.attention.output.dense.bias\n",
      "bert.encoder.layer.4.attention.output.LayerNorm.weight\n",
      "bert.encoder.layer.4.attention.output.LayerNorm.bias\n",
      "bert.encoder.layer.4.intermediate.dense.weight\n",
      "bert.encoder.layer.4.intermediate.dense.bias\n",
      "bert.encoder.layer.4.output.dense.weight\n",
      "bert.encoder.layer.4.output.dense.bias\n",
      "bert.encoder.layer.4.output.LayerNorm.weight\n",
      "bert.encoder.layer.4.output.LayerNorm.bias\n",
      "bert.encoder.layer.5.attention.self.query.weight\n",
      "bert.encoder.layer.5.attention.self.query.bias\n",
      "bert.encoder.layer.5.attention.self.key.weight\n",
      "bert.encoder.layer.5.attention.self.key.bias\n",
      "bert.encoder.layer.5.attention.self.value.weight\n",
      "bert.encoder.layer.5.attention.self.value.bias\n",
      "bert.encoder.layer.5.attention.output.dense.weight\n",
      "bert.encoder.layer.5.attention.output.dense.bias\n",
      "bert.encoder.layer.5.attention.output.LayerNorm.weight\n",
      "bert.encoder.layer.5.attention.output.LayerNorm.bias\n",
      "bert.encoder.layer.5.intermediate.dense.weight\n",
      "bert.encoder.layer.5.intermediate.dense.bias\n",
      "bert.encoder.layer.5.output.dense.weight\n",
      "bert.encoder.layer.5.output.dense.bias\n",
      "bert.encoder.layer.5.output.LayerNorm.weight\n",
      "bert.encoder.layer.5.output.LayerNorm.bias\n",
      "bert.encoder.layer.6.attention.self.query.weight\n",
      "bert.encoder.layer.6.attention.self.query.bias\n",
      "bert.encoder.layer.6.attention.self.key.weight\n",
      "bert.encoder.layer.6.attention.self.key.bias\n",
      "bert.encoder.layer.6.attention.self.value.weight\n",
      "bert.encoder.layer.6.attention.self.value.bias\n",
      "bert.encoder.layer.6.attention.output.dense.weight\n",
      "bert.encoder.layer.6.attention.output.dense.bias\n",
      "bert.encoder.layer.6.attention.output.LayerNorm.weight\n",
      "bert.encoder.layer.6.attention.output.LayerNorm.bias\n",
      "bert.encoder.layer.6.intermediate.dense.weight\n",
      "bert.encoder.layer.6.intermediate.dense.bias\n",
      "bert.encoder.layer.6.output.dense.weight\n",
      "bert.encoder.layer.6.output.dense.bias\n",
      "bert.encoder.layer.6.output.LayerNorm.weight\n",
      "bert.encoder.layer.6.output.LayerNorm.bias\n",
      "bert.encoder.layer.7.attention.self.query.weight\n",
      "bert.encoder.layer.7.attention.self.query.bias\n",
      "bert.encoder.layer.7.attention.self.key.weight\n",
      "bert.encoder.layer.7.attention.self.key.bias\n",
      "bert.encoder.layer.7.attention.self.value.weight\n",
      "bert.encoder.layer.7.attention.self.value.bias\n",
      "bert.encoder.layer.7.attention.output.dense.weight\n",
      "bert.encoder.layer.7.attention.output.dense.bias\n",
      "bert.encoder.layer.7.attention.output.LayerNorm.weight\n",
      "bert.encoder.layer.7.attention.output.LayerNorm.bias\n",
      "bert.encoder.layer.7.intermediate.dense.weight\n",
      "bert.encoder.layer.7.intermediate.dense.bias\n",
      "bert.encoder.layer.7.output.dense.weight\n",
      "bert.encoder.layer.7.output.dense.bias\n",
      "bert.encoder.layer.7.output.LayerNorm.weight\n",
      "bert.encoder.layer.7.output.LayerNorm.bias\n",
      "bert.encoder.layer.8.attention.self.query.weight\n",
      "bert.encoder.layer.8.attention.self.query.bias\n",
      "bert.encoder.layer.8.attention.self.key.weight\n",
      "bert.encoder.layer.8.attention.self.key.bias\n",
      "bert.encoder.layer.8.attention.self.value.weight\n",
      "bert.encoder.layer.8.attention.self.value.bias\n",
      "bert.encoder.layer.8.attention.output.dense.weight\n",
      "bert.encoder.layer.8.attention.output.dense.bias\n",
      "bert.encoder.layer.8.attention.output.LayerNorm.weight\n",
      "bert.encoder.layer.8.attention.output.LayerNorm.bias\n",
      "bert.encoder.layer.8.intermediate.dense.weight\n",
      "bert.encoder.layer.8.intermediate.dense.bias\n",
      "bert.encoder.layer.8.output.dense.weight\n",
      "bert.encoder.layer.8.output.dense.bias\n",
      "bert.encoder.layer.8.output.LayerNorm.weight\n",
      "bert.encoder.layer.8.output.LayerNorm.bias\n",
      "bert.encoder.layer.9.attention.self.query.weight\n",
      "bert.encoder.layer.9.attention.self.query.bias\n",
      "bert.encoder.layer.9.attention.self.key.weight\n",
      "bert.encoder.layer.9.attention.self.key.bias\n",
      "bert.encoder.layer.9.attention.self.value.weight\n",
      "bert.encoder.layer.9.attention.self.value.bias\n",
      "bert.encoder.layer.9.attention.output.dense.weight\n",
      "bert.encoder.layer.9.attention.output.dense.bias\n",
      "bert.encoder.layer.9.attention.output.LayerNorm.weight\n",
      "bert.encoder.layer.9.attention.output.LayerNorm.bias\n",
      "bert.encoder.layer.9.intermediate.dense.weight\n",
      "bert.encoder.layer.9.intermediate.dense.bias\n",
      "bert.encoder.layer.9.output.dense.weight\n",
      "bert.encoder.layer.9.output.dense.bias\n",
      "bert.encoder.layer.9.output.LayerNorm.weight\n",
      "bert.encoder.layer.9.output.LayerNorm.bias\n",
      "bert.encoder.layer.10.attention.self.query.weight\n",
      "bert.encoder.layer.10.attention.self.query.bias\n",
      "bert.encoder.layer.10.attention.self.key.weight\n",
      "bert.encoder.layer.10.attention.self.key.bias\n",
      "bert.encoder.layer.10.attention.self.value.weight\n",
      "bert.encoder.layer.10.attention.self.value.bias\n",
      "bert.encoder.layer.10.attention.output.dense.weight\n",
      "bert.encoder.layer.10.attention.output.dense.bias\n",
      "bert.encoder.layer.10.attention.output.LayerNorm.weight\n",
      "bert.encoder.layer.10.attention.output.LayerNorm.bias\n",
      "bert.encoder.layer.10.intermediate.dense.weight\n",
      "bert.encoder.layer.10.intermediate.dense.bias\n",
      "bert.encoder.layer.10.output.dense.weight\n",
      "bert.encoder.layer.10.output.dense.bias\n",
      "bert.encoder.layer.10.output.LayerNorm.weight\n",
      "bert.encoder.layer.10.output.LayerNorm.bias\n",
      "bert.encoder.layer.11.attention.self.query.weight\n",
      "bert.encoder.layer.11.attention.self.query.bias\n",
      "bert.encoder.layer.11.attention.self.key.weight\n",
      "bert.encoder.layer.11.attention.self.key.bias\n",
      "bert.encoder.layer.11.attention.self.value.weight\n",
      "bert.encoder.layer.11.attention.self.value.bias\n",
      "bert.encoder.layer.11.attention.output.dense.weight\n",
      "bert.encoder.layer.11.attention.output.dense.bias\n",
      "bert.encoder.layer.11.attention.output.LayerNorm.weight\n",
      "bert.encoder.layer.11.attention.output.LayerNorm.bias\n",
      "bert.encoder.layer.11.intermediate.dense.weight\n",
      "bert.encoder.layer.11.intermediate.dense.bias\n",
      "bert.encoder.layer.11.output.dense.weight\n",
      "bert.encoder.layer.11.output.dense.bias\n",
      "bert.encoder.layer.11.output.LayerNorm.weight\n",
      "bert.encoder.layer.11.output.LayerNorm.bias\n",
      "cls.predictions.bias\n",
      "cls.predictions.transform.dense.weight\n",
      "cls.predictions.transform.dense.bias\n",
      "cls.predictions.transform.LayerNorm.weight\n",
      "cls.predictions.transform.LayerNorm.bias\n",
      "cls.predictions.decoder.weight\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['bert.embeddings.word_embeddings.weight',\n",
       " 'bert.embeddings.position_embeddings.weight',\n",
       " 'bert.embeddings.token_type_embeddings.weight',\n",
       " 'bert.embeddings.LayerNorm.weight',\n",
       " 'bert.embeddings.LayerNorm.bias',\n",
       " 'bert.encoder.layer.0.attention.self.query.weight',\n",
       " 'bert.encoder.layer.0.attention.self.query.bias',\n",
       " 'bert.encoder.layer.0.attention.self.key.weight',\n",
       " 'bert.encoder.layer.0.attention.self.key.bias',\n",
       " 'bert.encoder.layer.0.attention.self.value.weight',\n",
       " 'bert.encoder.layer.0.attention.self.value.bias',\n",
       " 'bert.encoder.layer.0.attention.output.dense.weight',\n",
       " 'bert.encoder.layer.0.attention.output.dense.bias',\n",
       " 'bert.encoder.layer.0.attention.output.LayerNorm.weight',\n",
       " 'bert.encoder.layer.0.attention.output.LayerNorm.bias',\n",
       " 'bert.encoder.layer.0.intermediate.dense.weight',\n",
       " 'bert.encoder.layer.0.intermediate.dense.bias',\n",
       " 'bert.encoder.layer.0.output.dense.weight',\n",
       " 'bert.encoder.layer.0.output.dense.bias',\n",
       " 'bert.encoder.layer.0.output.LayerNorm.weight',\n",
       " 'bert.encoder.layer.0.output.LayerNorm.bias',\n",
       " 'bert.encoder.layer.1.attention.self.query.weight',\n",
       " 'bert.encoder.layer.1.attention.self.query.bias',\n",
       " 'bert.encoder.layer.1.attention.self.key.weight',\n",
       " 'bert.encoder.layer.1.attention.self.key.bias',\n",
       " 'bert.encoder.layer.1.attention.self.value.weight',\n",
       " 'bert.encoder.layer.1.attention.self.value.bias',\n",
       " 'bert.encoder.layer.1.attention.output.dense.weight',\n",
       " 'bert.encoder.layer.1.attention.output.dense.bias',\n",
       " 'bert.encoder.layer.1.attention.output.LayerNorm.weight',\n",
       " 'bert.encoder.layer.1.attention.output.LayerNorm.bias',\n",
       " 'bert.encoder.layer.1.intermediate.dense.weight',\n",
       " 'bert.encoder.layer.1.intermediate.dense.bias',\n",
       " 'bert.encoder.layer.1.output.dense.weight',\n",
       " 'bert.encoder.layer.1.output.dense.bias',\n",
       " 'bert.encoder.layer.1.output.LayerNorm.weight',\n",
       " 'bert.encoder.layer.1.output.LayerNorm.bias',\n",
       " 'bert.encoder.layer.2.attention.self.query.weight',\n",
       " 'bert.encoder.layer.2.attention.self.query.bias',\n",
       " 'bert.encoder.layer.2.attention.self.key.weight',\n",
       " 'bert.encoder.layer.2.attention.self.key.bias',\n",
       " 'bert.encoder.layer.2.attention.self.value.weight',\n",
       " 'bert.encoder.layer.2.attention.self.value.bias',\n",
       " 'bert.encoder.layer.2.attention.output.dense.weight',\n",
       " 'bert.encoder.layer.2.attention.output.dense.bias',\n",
       " 'bert.encoder.layer.2.attention.output.LayerNorm.weight',\n",
       " 'bert.encoder.layer.2.attention.output.LayerNorm.bias',\n",
       " 'bert.encoder.layer.2.intermediate.dense.weight',\n",
       " 'bert.encoder.layer.2.intermediate.dense.bias',\n",
       " 'bert.encoder.layer.2.output.dense.weight',\n",
       " 'bert.encoder.layer.2.output.dense.bias',\n",
       " 'bert.encoder.layer.2.output.LayerNorm.weight',\n",
       " 'bert.encoder.layer.2.output.LayerNorm.bias',\n",
       " 'bert.encoder.layer.3.attention.self.query.weight',\n",
       " 'bert.encoder.layer.3.attention.self.query.bias',\n",
       " 'bert.encoder.layer.3.attention.self.key.weight',\n",
       " 'bert.encoder.layer.3.attention.self.key.bias',\n",
       " 'bert.encoder.layer.3.attention.self.value.weight',\n",
       " 'bert.encoder.layer.3.attention.self.value.bias',\n",
       " 'bert.encoder.layer.3.attention.output.dense.weight',\n",
       " 'bert.encoder.layer.3.attention.output.dense.bias',\n",
       " 'bert.encoder.layer.3.attention.output.LayerNorm.weight',\n",
       " 'bert.encoder.layer.3.attention.output.LayerNorm.bias',\n",
       " 'bert.encoder.layer.3.intermediate.dense.weight',\n",
       " 'bert.encoder.layer.3.intermediate.dense.bias',\n",
       " 'bert.encoder.layer.3.output.dense.weight',\n",
       " 'bert.encoder.layer.3.output.dense.bias',\n",
       " 'bert.encoder.layer.3.output.LayerNorm.weight',\n",
       " 'bert.encoder.layer.3.output.LayerNorm.bias',\n",
       " 'bert.encoder.layer.4.attention.self.query.weight',\n",
       " 'bert.encoder.layer.4.attention.self.query.bias',\n",
       " 'bert.encoder.layer.4.attention.self.key.weight',\n",
       " 'bert.encoder.layer.4.attention.self.key.bias',\n",
       " 'bert.encoder.layer.4.attention.self.value.weight',\n",
       " 'bert.encoder.layer.4.attention.self.value.bias',\n",
       " 'bert.encoder.layer.4.attention.output.dense.weight',\n",
       " 'bert.encoder.layer.4.attention.output.dense.bias',\n",
       " 'bert.encoder.layer.4.attention.output.LayerNorm.weight',\n",
       " 'bert.encoder.layer.4.attention.output.LayerNorm.bias',\n",
       " 'bert.encoder.layer.4.intermediate.dense.weight',\n",
       " 'bert.encoder.layer.4.intermediate.dense.bias',\n",
       " 'bert.encoder.layer.4.output.dense.weight',\n",
       " 'bert.encoder.layer.4.output.dense.bias',\n",
       " 'bert.encoder.layer.4.output.LayerNorm.weight',\n",
       " 'bert.encoder.layer.4.output.LayerNorm.bias',\n",
       " 'bert.encoder.layer.5.attention.self.query.weight',\n",
       " 'bert.encoder.layer.5.attention.self.query.bias',\n",
       " 'bert.encoder.layer.5.attention.self.key.weight',\n",
       " 'bert.encoder.layer.5.attention.self.key.bias',\n",
       " 'bert.encoder.layer.5.attention.self.value.weight',\n",
       " 'bert.encoder.layer.5.attention.self.value.bias',\n",
       " 'bert.encoder.layer.5.attention.output.dense.weight',\n",
       " 'bert.encoder.layer.5.attention.output.dense.bias',\n",
       " 'bert.encoder.layer.5.attention.output.LayerNorm.weight',\n",
       " 'bert.encoder.layer.5.attention.output.LayerNorm.bias',\n",
       " 'bert.encoder.layer.5.intermediate.dense.weight',\n",
       " 'bert.encoder.layer.5.intermediate.dense.bias',\n",
       " 'bert.encoder.layer.5.output.dense.weight',\n",
       " 'bert.encoder.layer.5.output.dense.bias',\n",
       " 'bert.encoder.layer.5.output.LayerNorm.weight',\n",
       " 'bert.encoder.layer.5.output.LayerNorm.bias',\n",
       " 'bert.encoder.layer.6.attention.self.query.weight',\n",
       " 'bert.encoder.layer.6.attention.self.query.bias',\n",
       " 'bert.encoder.layer.6.attention.self.key.weight',\n",
       " 'bert.encoder.layer.6.attention.self.key.bias',\n",
       " 'bert.encoder.layer.6.attention.self.value.weight',\n",
       " 'bert.encoder.layer.6.attention.self.value.bias',\n",
       " 'bert.encoder.layer.6.attention.output.dense.weight',\n",
       " 'bert.encoder.layer.6.attention.output.dense.bias',\n",
       " 'bert.encoder.layer.6.attention.output.LayerNorm.weight',\n",
       " 'bert.encoder.layer.6.attention.output.LayerNorm.bias',\n",
       " 'bert.encoder.layer.6.intermediate.dense.weight',\n",
       " 'bert.encoder.layer.6.intermediate.dense.bias',\n",
       " 'bert.encoder.layer.6.output.dense.weight',\n",
       " 'bert.encoder.layer.6.output.dense.bias',\n",
       " 'bert.encoder.layer.6.output.LayerNorm.weight',\n",
       " 'bert.encoder.layer.6.output.LayerNorm.bias',\n",
       " 'bert.encoder.layer.7.attention.self.query.weight',\n",
       " 'bert.encoder.layer.7.attention.self.query.bias',\n",
       " 'bert.encoder.layer.7.attention.self.key.weight',\n",
       " 'bert.encoder.layer.7.attention.self.key.bias',\n",
       " 'bert.encoder.layer.7.attention.self.value.weight',\n",
       " 'bert.encoder.layer.7.attention.self.value.bias',\n",
       " 'bert.encoder.layer.7.attention.output.dense.weight',\n",
       " 'bert.encoder.layer.7.attention.output.dense.bias',\n",
       " 'bert.encoder.layer.7.attention.output.LayerNorm.weight',\n",
       " 'bert.encoder.layer.7.attention.output.LayerNorm.bias',\n",
       " 'bert.encoder.layer.7.intermediate.dense.weight',\n",
       " 'bert.encoder.layer.7.intermediate.dense.bias',\n",
       " 'bert.encoder.layer.7.output.dense.weight',\n",
       " 'bert.encoder.layer.7.output.dense.bias',\n",
       " 'bert.encoder.layer.7.output.LayerNorm.weight',\n",
       " 'bert.encoder.layer.7.output.LayerNorm.bias',\n",
       " 'bert.encoder.layer.8.attention.self.query.weight',\n",
       " 'bert.encoder.layer.8.attention.self.query.bias',\n",
       " 'bert.encoder.layer.8.attention.self.key.weight',\n",
       " 'bert.encoder.layer.8.attention.self.key.bias',\n",
       " 'bert.encoder.layer.8.attention.self.value.weight',\n",
       " 'bert.encoder.layer.8.attention.self.value.bias',\n",
       " 'bert.encoder.layer.8.attention.output.dense.weight',\n",
       " 'bert.encoder.layer.8.attention.output.dense.bias',\n",
       " 'bert.encoder.layer.8.attention.output.LayerNorm.weight',\n",
       " 'bert.encoder.layer.8.attention.output.LayerNorm.bias',\n",
       " 'bert.encoder.layer.8.intermediate.dense.weight',\n",
       " 'bert.encoder.layer.8.intermediate.dense.bias',\n",
       " 'bert.encoder.layer.8.output.dense.weight',\n",
       " 'bert.encoder.layer.8.output.dense.bias',\n",
       " 'bert.encoder.layer.8.output.LayerNorm.weight',\n",
       " 'bert.encoder.layer.8.output.LayerNorm.bias',\n",
       " 'bert.encoder.layer.9.attention.self.query.weight',\n",
       " 'bert.encoder.layer.9.attention.self.query.bias',\n",
       " 'bert.encoder.layer.9.attention.self.key.weight',\n",
       " 'bert.encoder.layer.9.attention.self.key.bias',\n",
       " 'bert.encoder.layer.9.attention.self.value.weight',\n",
       " 'bert.encoder.layer.9.attention.self.value.bias',\n",
       " 'bert.encoder.layer.9.attention.output.dense.weight',\n",
       " 'bert.encoder.layer.9.attention.output.dense.bias',\n",
       " 'bert.encoder.layer.9.attention.output.LayerNorm.weight',\n",
       " 'bert.encoder.layer.9.attention.output.LayerNorm.bias',\n",
       " 'bert.encoder.layer.9.intermediate.dense.weight',\n",
       " 'bert.encoder.layer.9.intermediate.dense.bias',\n",
       " 'bert.encoder.layer.9.output.dense.weight',\n",
       " 'bert.encoder.layer.9.output.dense.bias',\n",
       " 'bert.encoder.layer.9.output.LayerNorm.weight',\n",
       " 'bert.encoder.layer.9.output.LayerNorm.bias',\n",
       " 'bert.encoder.layer.10.attention.self.query.weight',\n",
       " 'bert.encoder.layer.10.attention.self.query.bias',\n",
       " 'bert.encoder.layer.10.attention.self.key.weight',\n",
       " 'bert.encoder.layer.10.attention.self.key.bias',\n",
       " 'bert.encoder.layer.10.attention.self.value.weight',\n",
       " 'bert.encoder.layer.10.attention.self.value.bias',\n",
       " 'bert.encoder.layer.10.attention.output.dense.weight',\n",
       " 'bert.encoder.layer.10.attention.output.dense.bias',\n",
       " 'bert.encoder.layer.10.attention.output.LayerNorm.weight',\n",
       " 'bert.encoder.layer.10.attention.output.LayerNorm.bias',\n",
       " 'bert.encoder.layer.10.intermediate.dense.weight',\n",
       " 'bert.encoder.layer.10.intermediate.dense.bias',\n",
       " 'bert.encoder.layer.10.output.dense.weight',\n",
       " 'bert.encoder.layer.10.output.dense.bias',\n",
       " 'bert.encoder.layer.10.output.LayerNorm.weight',\n",
       " 'bert.encoder.layer.10.output.LayerNorm.bias',\n",
       " 'bert.encoder.layer.11.attention.self.query.weight',\n",
       " 'bert.encoder.layer.11.attention.self.query.bias',\n",
       " 'bert.encoder.layer.11.attention.self.key.weight',\n",
       " 'bert.encoder.layer.11.attention.self.key.bias',\n",
       " 'bert.encoder.layer.11.attention.self.value.weight',\n",
       " 'bert.encoder.layer.11.attention.self.value.bias',\n",
       " 'bert.encoder.layer.11.attention.output.dense.weight',\n",
       " 'bert.encoder.layer.11.attention.output.dense.bias',\n",
       " 'bert.encoder.layer.11.attention.output.LayerNorm.weight',\n",
       " 'bert.encoder.layer.11.attention.output.LayerNorm.bias',\n",
       " 'bert.encoder.layer.11.intermediate.dense.weight',\n",
       " 'bert.encoder.layer.11.intermediate.dense.bias',\n",
       " 'bert.encoder.layer.11.output.dense.weight',\n",
       " 'bert.encoder.layer.11.output.dense.bias',\n",
       " 'bert.encoder.layer.11.output.LayerNorm.weight',\n",
       " 'bert.encoder.layer.11.output.LayerNorm.bias',\n",
       " 'cls.predictions.bias',\n",
       " 'cls.predictions.transform.dense.weight',\n",
       " 'cls.predictions.transform.dense.bias',\n",
       " 'cls.predictions.transform.LayerNorm.weight',\n",
       " 'cls.predictions.transform.LayerNorm.bias',\n",
       " 'cls.predictions.decoder.weight']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n=0\n",
    "a=[]\n",
    "for x in state_dict:\n",
    "    n=n+1\n",
    "    print(x)\n",
    "    a.append(str(x))\n",
    "    \n",
    "n\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7c01e442",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertModel(\n",
       "  (embeddings): BertEmbeddings(\n",
       "    (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "    (position_embeddings): Embedding(512, 768)\n",
       "    (token_type_embeddings): Embedding(2, 768)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): BertEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0-11): 12 x BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): BertPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def transfer_weights(model1,model2):\n",
    "    \n",
    "    #retrieve the layers\n",
    "    n1=0\n",
    "    del_layers=[]\n",
    "    for x in model1.state_dict():\n",
    "        n1=n1+1\n",
    "        if n1>197:\n",
    "            del_layers.append(str(x))\n",
    "           \n",
    "    # delete the lyers\n",
    "    state_dict = model1.state_dict()\n",
    "    for key in del_layers:\n",
    "        if key in state_dict:\n",
    "            del state_dict[key]\n",
    "    \n",
    "    # transfer weights\n",
    "    for x, y in zip(state_dict,model2.state_dict()):\n",
    "        for n in range(len(model2.state_dict()[str(y)])):\n",
    "            model2.state_dict()[str(y)][n] = state_dict[str(x)][n]\n",
    "            \n",
    "    return model2\n",
    "\n",
    "transfer_weights(model,model1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "bf62e1e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model3 = transfer_weights(model,model1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4f1ef9bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-6.4523e-03, -2.0317e-02,  4.6948e-02,  1.5610e-02, -4.1688e-02,\n",
       "        -1.3823e-02, -9.9459e-03, -3.1925e-02,  2.5848e-02, -3.5719e-02,\n",
       "        -6.0546e-02,  3.0363e-02,  1.6680e-02,  1.1848e-02, -3.5969e-03,\n",
       "        -4.8558e-02,  4.2298e-04, -9.1702e-02,  1.0390e-02,  1.1714e-02,\n",
       "        -5.8461e-02, -4.7710e-03,  5.0649e-03, -4.4028e-02,  5.0694e-03,\n",
       "        -3.9832e-02, -4.7910e-02, -5.1699e-02, -9.1433e-02, -4.0506e-02,\n",
       "        -2.7266e-02, -4.6149e-02, -1.2191e-02, -7.5270e-02, -4.1507e-02,\n",
       "        -5.7161e-02, -2.7618e-02, -7.8072e-02,  6.9756e-03, -5.9606e-02,\n",
       "         5.3213e-02, -2.1257e-02, -4.5698e-02, -6.8589e-02, -1.0861e-01,\n",
       "        -1.8931e-02, -1.3497e-01, -2.3966e-03, -1.0932e-01, -1.1749e-01,\n",
       "        -2.1131e-02,  4.4923e-02,  3.4135e-02, -2.5268e-02, -8.8217e-02,\n",
       "        -9.0805e-03, -2.2671e-03, -6.2807e-02, -3.3862e-04, -6.9942e-02,\n",
       "         4.4014e-02, -8.4269e-02,  2.0889e-02,  5.0521e-02, -4.3462e-02,\n",
       "        -4.7929e-02, -6.4631e-04, -5.8909e-02,  3.2735e-02,  3.4517e-04,\n",
       "         2.1681e-02, -2.2895e-02, -6.6123e-02, -5.4571e-02, -1.2539e-02,\n",
       "        -8.4490e-02,  4.7765e-02, -4.9521e-02,  4.0564e-02,  3.1120e-02,\n",
       "         3.3802e-02, -3.4906e-02, -5.0345e-02,  2.0176e-02, -5.7624e-02,\n",
       "        -3.7193e-02,  2.8059e-02, -1.3513e-02, -4.7285e-02,  3.0422e-02,\n",
       "         1.4913e-02, -7.7604e-02,  5.2582e-02, -1.5523e-02, -4.0374e-02,\n",
       "        -1.1517e-01, -3.9766e-02, -1.3144e-02,  2.9801e-02, -1.0077e-02,\n",
       "        -8.0950e-02, -7.5172e-02, -9.0253e-02, -2.8527e-02, -1.0854e-01,\n",
       "        -1.8852e-02, -2.7838e-02,  4.8010e-02, -5.0576e-02, -2.2118e-02,\n",
       "        -3.4760e-02, -3.8120e-02,  1.1955e-02,  2.0582e-02, -2.0803e-02,\n",
       "         3.5551e-02, -3.0075e-03,  3.5483e-02, -1.7932e-03, -7.1549e-03,\n",
       "         3.4860e-02,  1.1605e-01, -4.4658e-03, -3.3021e-02,  1.1173e-02,\n",
       "         4.9342e-02,  9.4757e-03, -3.2597e-03, -3.3375e-02, -1.2737e-01,\n",
       "        -6.8389e-03, -1.8048e-03, -5.9507e-03, -3.4198e-02,  6.4502e-02,\n",
       "        -1.1180e-01, -4.6321e-02, -2.9782e-03,  3.8461e-02, -3.4754e-02,\n",
       "        -1.1473e-02, -4.3851e-03,  4.0280e-02, -2.8297e-02, -1.4682e-02,\n",
       "        -8.7001e-02, -3.7303e-02,  2.9345e-02, -6.6815e-02,  3.2023e-02,\n",
       "         6.1036e-03,  5.7273e-02,  7.3194e-02, -7.0628e-02,  1.7109e-02,\n",
       "         8.3086e-02, -1.3743e-02, -6.9930e-02, -1.1724e-01, -1.0780e-02,\n",
       "         2.3437e-02,  5.2473e-02, -1.8402e-02,  5.1213e-02, -6.5672e-02,\n",
       "        -7.0694e-02,  3.3233e-03, -9.4403e-02, -5.0532e-02, -5.2560e-02,\n",
       "        -2.3850e-02, -1.7085e-02,  1.9018e-02, -4.9709e-03, -7.9306e-02,\n",
       "         5.7373e-02,  9.3325e-02, -3.4698e-02, -5.0066e-02, -7.8543e-02,\n",
       "         1.3122e-01,  1.4673e-02, -1.6938e-02, -3.8435e-02, -1.4733e-02,\n",
       "        -6.0461e-02, -6.1729e-02,  6.4496e-03,  3.3840e-02,  9.0751e-03,\n",
       "        -3.2665e-02, -1.1076e-01, -1.0684e-02, -1.2094e-03,  4.3203e-02,\n",
       "        -5.1379e-02,  1.0308e-02,  1.2183e-01, -6.8764e-03,  2.7942e-02,\n",
       "        -4.4556e-02, -2.6252e-02,  3.8895e-02,  5.1236e-02, -8.5446e-02,\n",
       "         4.3974e-02,  4.2842e-02, -6.3490e-02,  4.4482e-02, -6.1192e-02,\n",
       "        -1.3520e-02, -6.8760e-02,  4.3346e-02, -8.8517e-02, -5.1777e-03,\n",
       "        -2.9706e-02,  5.5594e-02, -6.7947e-02, -4.8597e-02,  4.4588e-03,\n",
       "         3.2622e-02, -6.2133e-02, -1.9328e-02,  5.9748e-03, -7.4520e-02,\n",
       "        -1.1827e-01,  3.6686e-02, -1.0178e-01, -3.3162e-02,  1.8303e-03,\n",
       "        -5.3591e-02, -4.5645e-02, -1.0221e-01,  1.5094e-03,  6.1728e-02,\n",
       "        -2.1137e-02, -8.4871e-03, -3.7216e-02,  4.2175e-02,  3.1417e-02,\n",
       "        -4.3526e-02, -1.8746e-03, -4.8861e-02,  1.9841e-02, -2.8993e-03,\n",
       "        -3.3155e-02,  4.4352e-02, -7.5824e-02, -7.5703e-02, -4.3591e-02,\n",
       "        -5.8319e-02, -9.6392e-02, -2.2687e-02, -2.8491e-02,  7.8004e-02,\n",
       "        -7.9945e-02, -4.8014e-02,  6.8501e-03,  2.6866e-02, -5.7744e-02,\n",
       "        -6.0478e-02, -3.2894e-02, -3.1588e-02, -1.2696e-01, -5.1323e-02,\n",
       "        -6.7020e-02, -1.9172e-01, -5.1721e-02,  2.2162e-03,  3.0670e-02,\n",
       "         8.0560e-02, -3.0530e-02,  1.5078e-02, -7.1606e-02, -3.3116e-02,\n",
       "        -5.3048e-02,  1.1599e-01, -5.4143e-04,  1.9723e-03, -6.6084e-02,\n",
       "        -4.3510e-02, -1.0656e-02,  4.7826e-02, -8.9496e-02,  6.5902e-02,\n",
       "        -2.5231e-02,  1.3512e-02, -9.6697e-02, -1.1846e-01,  1.1103e-02,\n",
       "         1.6439e-02,  4.3584e-02, -2.9340e-02, -5.1699e-02, -1.0889e-02,\n",
       "        -3.2376e-02, -2.3116e-02,  8.9311e-02, -6.5233e-02,  1.3737e-02,\n",
       "         3.7024e-02,  3.2160e-02, -9.5401e-02, -4.3724e-02, -1.5979e-02,\n",
       "        -3.1506e-02, -1.2327e-02,  2.6167e-03,  2.2537e-01, -5.3696e-02,\n",
       "        -2.0477e-02,  4.3348e-03, -2.4766e-02, -2.4036e-02,  6.6253e-02,\n",
       "        -1.5251e-02, -3.8840e-03,  3.4556e-02, -2.6489e-02,  4.3656e-02,\n",
       "        -3.0925e-02, -3.2693e-02, -9.1047e-03,  6.6226e-02, -7.7536e-02,\n",
       "        -3.1232e-02, -4.4809e-02,  1.6949e-02, -8.0668e-02, -3.1215e-02,\n",
       "        -2.6046e-02, -4.2385e-02, -9.0958e-02,  6.7670e-02, -3.9040e-02,\n",
       "        -4.5446e-04, -4.8644e-02, -1.5628e-02, -5.0144e-02,  7.3783e-02,\n",
       "        -2.5105e-02, -1.5402e-02, -1.3958e-02, -5.2957e-02,  3.0382e-02,\n",
       "        -4.4465e-02, -3.0508e-02,  1.3959e-02, -1.5415e-02, -4.4661e-02,\n",
       "        -8.1628e-02,  3.7103e-02, -1.0718e-01, -6.8334e-02, -2.4383e-02,\n",
       "        -1.9261e-02,  3.5302e-02, -6.9521e-02, -5.6849e-02, -9.8419e-03,\n",
       "        -5.2158e-02,  6.1590e-02, -9.6127e-02, -3.5450e-02, -9.7808e-02,\n",
       "        -7.0746e-03,  4.3017e-03, -9.1642e-02, -3.5484e-02, -7.4831e-03,\n",
       "        -3.6376e-02, -3.2286e-02, -9.4001e-02,  1.2243e-02,  3.6356e-02,\n",
       "        -3.7538e-02,  1.2484e-02,  6.3775e-02, -2.8093e-02, -6.2089e-02,\n",
       "        -5.1415e-02, -2.0653e-02, -2.2206e-02,  1.7944e-02, -5.7541e-02,\n",
       "        -1.0300e-01, -8.6599e-03, -5.6977e-03, -5.4511e-02,  1.6783e-02,\n",
       "         1.2794e-01,  5.5507e-02,  5.4583e-03, -5.0038e-02,  5.3570e-02,\n",
       "        -1.9344e-03, -2.8522e-02,  5.8118e-02,  2.4355e-02, -7.4415e-02,\n",
       "        -5.9233e-02, -1.1520e-02,  2.4468e-02,  1.0812e-01, -1.6713e-02,\n",
       "        -6.4587e-02,  2.6876e-02, -6.5102e-02,  8.3963e-02, -4.5102e-02,\n",
       "        -1.4063e-03, -4.8374e-02, -2.6398e-02, -2.3276e-02, -5.2105e-02,\n",
       "        -5.2864e-02,  6.0122e-03, -3.1543e-02,  1.6095e-02, -6.7345e-03,\n",
       "        -9.3208e-03, -1.7123e-01, -1.5844e-02, -3.4706e-02, -6.6143e-02,\n",
       "        -2.9764e-02,  7.1169e-03, -5.1042e-03, -7.4857e-02, -1.4608e-02,\n",
       "        -7.8908e-02, -2.6602e-02, -1.5003e-01,  2.2984e-02,  1.0018e-02,\n",
       "        -2.5711e-02,  1.8026e-05, -2.0075e-02, -3.9420e-02, -1.5980e-02,\n",
       "        -2.5962e-02, -6.8212e-02,  3.3517e-02, -1.6346e-03, -4.9878e-02,\n",
       "         5.9397e-02,  4.6518e-02, -1.0219e-01, -2.6494e-02, -4.4878e-02,\n",
       "        -2.1767e-02, -4.0936e-02, -1.3987e-02, -8.7613e-04, -6.7404e-02,\n",
       "        -6.2006e-02, -4.5987e-02, -6.4913e-03,  7.2685e-02, -4.7416e-02,\n",
       "        -1.5137e-02, -3.7320e-02, -1.4006e-01, -9.2600e-02, -1.2160e-02,\n",
       "        -5.2396e-02, -5.8222e-02,  5.3884e-02, -3.8508e-02, -8.9214e-02,\n",
       "        -5.5617e-02, -1.9866e-02, -1.3809e-02,  4.2267e-02, -2.6668e-02,\n",
       "        -1.5454e-02, -5.2973e-02, -7.4215e-02,  5.6920e-02, -6.4374e-02,\n",
       "        -5.5840e-02,  5.6786e-03,  1.2876e-02,  3.2491e-02,  8.7462e-03,\n",
       "         7.9143e-04,  9.4453e-02,  3.8197e-02, -2.4066e-03, -2.4766e-02,\n",
       "        -6.8934e-03, -8.9169e-03, -4.5014e-02, -5.8317e-02, -2.8860e-02,\n",
       "         1.0017e-02, -4.2581e-02,  6.4984e-02,  4.7738e-02, -4.0530e-02,\n",
       "        -4.6422e-02, -1.6326e-02, -4.6603e-02, -1.5878e-02,  5.9152e-05,\n",
       "         9.7149e-02,  1.2494e-02, -1.8111e-02, -2.3818e-02,  5.2363e-03,\n",
       "        -6.2190e-02, -6.4469e-02,  3.2823e-02, -5.4023e-02,  9.1251e-02,\n",
       "        -1.0383e-01, -9.0666e-02,  5.9814e-03, -3.8401e-02, -7.3937e-02,\n",
       "        -4.6832e-02, -1.0060e-02,  2.4417e-03, -9.7398e-02, -5.7268e-02,\n",
       "         8.8186e-03, -4.7020e-02,  1.2186e-04,  3.5820e-02, -6.0436e-03,\n",
       "        -1.2656e-01, -2.7936e-02,  2.9002e-02,  7.4156e-02, -1.2334e-02,\n",
       "        -2.7782e-02,  1.0055e-01,  1.0424e-02,  3.4508e-02,  3.9747e-02,\n",
       "        -3.4627e-02,  6.7815e-02, -8.3191e-02,  9.6089e-04, -5.1771e-02,\n",
       "        -7.2300e-03, -6.4482e-02,  1.2494e-02, -3.5725e-02,  2.3913e-03,\n",
       "        -3.4318e-02, -6.2860e-02, -5.0645e-02,  2.8461e-02, -8.2031e-03,\n",
       "        -6.5231e-04,  1.6712e-02, -6.5494e-02, -1.7305e-02,  2.2664e-02,\n",
       "         2.2108e-02, -4.0129e-02,  1.2364e-02, -8.1461e-02, -2.3197e-02,\n",
       "        -4.8053e-02, -4.9838e-02, -4.7121e-02, -3.6437e-02,  7.6190e-03,\n",
       "        -4.7713e-02,  1.8774e-02, -1.0783e-01,  7.3068e-02, -3.5933e-02,\n",
       "        -1.3488e-01, -6.6914e-02,  3.7804e-02,  1.4475e-02,  7.2211e-03,\n",
       "         5.0949e-02,  4.0962e-02,  5.2091e-02, -5.7858e-02, -2.4915e-02,\n",
       "         1.8958e-02, -6.2235e-03, -4.0995e-03,  3.7999e-02,  1.0362e-02,\n",
       "        -7.2903e-02, -4.5367e-02, -4.7792e-02, -1.0388e-02,  1.5706e-02,\n",
       "        -9.3824e-02,  1.1272e-02,  4.4682e-02, -2.1576e-02,  2.2220e-02,\n",
       "        -2.4623e-02, -2.0144e-02, -9.2318e-02, -5.1539e-02,  3.7208e-02,\n",
       "        -2.6436e-02, -5.4705e-03, -1.9018e-02, -6.9319e-03,  5.4510e-02,\n",
       "        -5.3501e-02, -9.2597e-02, -8.9880e-02, -5.1506e-02,  4.7265e-03,\n",
       "        -1.1477e-01, -2.4999e-02, -1.2306e-02,  3.7221e-02,  9.6846e-03,\n",
       "        -5.9224e-02, -1.6642e-02, -1.0003e-01, -1.2690e-01,  3.3881e-02,\n",
       "        -6.5757e-02, -1.0703e-01, -2.2263e-02,  1.0568e-02, -8.8899e-02,\n",
       "        -5.8447e-02, -1.9237e-02,  3.0744e-02, -8.3834e-02, -4.7459e-02,\n",
       "         3.4953e-02, -2.4201e-02,  9.0150e-03, -2.2455e-02,  1.3748e-01,\n",
       "        -8.5644e-02,  3.7417e-03,  3.2690e-03,  2.5203e-02, -1.8308e-02,\n",
       "        -3.1578e-02, -7.8905e-02, -2.1841e-02, -9.4537e-02,  8.7071e-03,\n",
       "        -2.8523e-02, -3.9510e-02, -1.2959e-02, -5.3671e-02,  3.6715e-02,\n",
       "        -5.9236e-02, -4.1738e-02,  8.4010e-03, -2.3616e-02,  2.7057e-02,\n",
       "        -8.1234e-02, -6.2575e-02,  5.2921e-03, -8.0414e-03, -9.6772e-02,\n",
       "        -2.8053e-02, -6.4268e-02, -4.0277e-02, -1.5943e-02, -3.3610e-02,\n",
       "        -9.3049e-04,  1.0833e-02, -3.1568e-02, -2.7485e-02, -2.2691e-02,\n",
       "        -4.0286e-02,  1.1265e-02,  3.2678e-02,  1.9981e-02, -5.2608e-02,\n",
       "        -6.3314e-02,  2.8367e-02, -4.1600e-02,  3.0145e-02,  7.3242e-02,\n",
       "        -3.5698e-02, -1.3829e-02,  7.0318e-02,  6.3474e-02, -3.6664e-02,\n",
       "         5.1458e-03, -2.2842e-02, -2.2986e-02, -1.0106e-01, -3.1976e-02,\n",
       "         7.4513e-02, -6.8796e-02,  1.6792e-02, -4.1934e-02, -9.0059e-02,\n",
       "        -1.1396e-01, -5.5255e-02, -6.4622e-02, -1.0330e-01, -1.0593e-01,\n",
       "        -2.6068e-03, -2.7830e-02, -8.1022e-03,  1.5933e-02, -3.1834e-02,\n",
       "         2.2023e-02, -3.7949e-02,  1.1529e-02,  1.9512e-02, -2.0324e-02,\n",
       "        -1.7840e-02, -5.6408e-02,  6.8627e-02,  3.6931e-02, -5.3792e-03,\n",
       "        -5.5165e-02, -9.9139e-05, -2.1724e-02, -1.1537e-01, -1.2771e-02,\n",
       "         1.6417e-02,  2.6337e-02, -1.1230e-01, -9.1366e-02, -4.1066e-02,\n",
       "        -7.3055e-03,  6.3705e-02, -1.0127e-02,  6.3471e-02, -7.6329e-02,\n",
       "        -2.1962e-02,  2.5482e-04, -1.7658e-02,  1.3164e-02, -6.1970e-02,\n",
       "        -3.0410e-02, -7.4303e-03,  7.8402e-02, -1.1136e-01, -9.4044e-02,\n",
       "        -5.6273e-02,  1.9627e-02, -4.3710e-02, -5.5866e-02, -7.6812e-02,\n",
       "         5.8543e-02,  2.0116e-02, -6.8352e-02, -7.8896e-02, -1.6737e-03,\n",
       "        -2.2561e-02, -1.0294e-01,  2.1452e-02,  7.4320e-02, -3.9743e-02,\n",
       "        -7.1434e-02, -1.7409e-02,  8.0384e-03, -4.5732e-02, -9.7798e-03,\n",
       "         1.6966e-02,  3.6260e-02, -1.6842e-02])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model3.state_dict()['encoder.layer.11.output.LayerNorm.bias']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d0f50c2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model3, 'BERT_comparison.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5060412",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpu1",
   "language": "python",
   "name": "gpu1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
