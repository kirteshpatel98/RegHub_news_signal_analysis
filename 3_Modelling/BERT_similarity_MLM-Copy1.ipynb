{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aa596283",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===================================BUG REPORT===================================\n",
      "Welcome to bitsandbytes. For bug reports, please submit your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
      "================================================================================\n",
      "binary_path: C:\\Users\\hp\\miniconda3\\envs\\tf_gpu\\lib\\site-packages\\bitsandbytes\\cuda_setup\\libbitsandbytes_cuda116.dll\n",
      "CUDA SETUP: Loading binary C:\\Users\\hp\\miniconda3\\envs\\tf_gpu\\lib\\site-packages\\bitsandbytes\\cuda_setup\\libbitsandbytes_cuda116.dll...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hp\\miniconda3\\envs\\tf_gpu\\lib\\site-packages\\scipy\\__init__.py:132: UserWarning: A NumPy version >=1.21.6 and <1.28.0 is required for this version of SciPy (detected version 1.20.0)\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "# ! pip install transformers\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "import torchsummary as summary\n",
    "from tqdm import tqdm\n",
    "\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from transformers import BertModel\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import base64\n",
    "import gc\n",
    "import glob, os\n",
    "import random\n",
    "\n",
    "from torch.optim import Adam\n",
    "from tqdm import tqdm\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import json\n",
    "from general_functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5cbb8ae0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>news_content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Berenberg Bank analysts have provided a buy ra...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The article states that Berenberg, a German in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>In their analysis on October 30, 2023, experts...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Berenberg Bank has issued a \\\"buy\\\" recommenda...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The private bank Berenberg has upgraded its ra...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14496</th>\n",
       "      <td>INTERVIEW Interview with Les Échos Interview w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14497</th>\n",
       "      <td>UBS's latest Investor Watch report reveals tha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14498</th>\n",
       "      <td>SNB erwartet für 2021 Jahresgewinn von rund 26...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14499</th>\n",
       "      <td>0:00 News A cryptocurrency exchange in Hong Ko...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14500</th>\n",
       "      <td>Bloomberg has reported that a number of indivi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>14501 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            news_content\n",
       "0      Berenberg Bank analysts have provided a buy ra...\n",
       "1      The article states that Berenberg, a German in...\n",
       "2      In their analysis on October 30, 2023, experts...\n",
       "3      Berenberg Bank has issued a \\\"buy\\\" recommenda...\n",
       "4      The private bank Berenberg has upgraded its ra...\n",
       "...                                                  ...\n",
       "14496  INTERVIEW Interview with Les Échos Interview w...\n",
       "14497  UBS's latest Investor Watch report reveals tha...\n",
       "14498  SNB erwartet für 2021 Jahresgewinn von rund 26...\n",
       "14499  0:00 News A cryptocurrency exchange in Hong Ko...\n",
       "14500  Bloomberg has reported that a number of indivi...\n",
       "\n",
       "[14501 rows x 1 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import data\n",
    "# Access aws credentials from json file\n",
    "with open(\"../aws_credentials.json\", 'r') as file:\n",
    "    aws_creds_json = json.load(file)\n",
    "# Specify s3 bucket\n",
    "bucket = \"fs-reghub-news-analysis\"\n",
    "\n",
    "# Connect to aws and dowload the files\n",
    "aws = awsOps(aws_creds_json)\n",
    "df = aws.get_df(bucket=bucket, file=\"data_rule_labels_v1.csv\")\n",
    "df.drop('Unnamed: 0',axis=1,inplace=True)\n",
    "df=df[['news_content']]\n",
    "df = df.reset_index()\n",
    "del df['index']\n",
    "# df=df[:65*2]\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "954e5ab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dynamic_whole_word_masking(df=df, column_name='news_content', mask_probability=0.15):\n",
    "    # Initialize BERT tokenizer\n",
    "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "    # Function to mask tokens of a sentence\n",
    "    def mask_sentence(sentence):\n",
    "        tokens = tokenizer.tokenize(sentence)\n",
    "        new_tokens = []\n",
    "        mask_indices = random.sample(range(len(tokens)), int(len(tokens) * mask_probability))\n",
    "\n",
    "        for i, token in enumerate(tokens):\n",
    "            # Check if the token is part of a word to be masked\n",
    "            if i in mask_indices or (i > 0 and tokens[i-1] in mask_indices and token.startswith('##')):\n",
    "                new_tokens.append('[MASK]')\n",
    "            else:\n",
    "                new_tokens.append(token)\n",
    "\n",
    "        return tokenizer.convert_tokens_to_string(new_tokens)\n",
    "\n",
    "    # Apply masking to each row in the DataFrame\n",
    "    df[column_name] = df[column_name].apply(mask_sentence)\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "874b743f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import BERT tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "class Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self,df,transform=dynamic_whole_word_masking):\n",
    "        self.labels = [tokenizer(text,padding='max_length',truncation=True,return_tensors=\"pt\") for text in df['news_content']]\n",
    "        self.transform=transform\n",
    "        if self.transform:\n",
    "            df1=self.transform(df=df)\n",
    "        self.text=[tokenizer(text,padding='max_length',truncation=True,return_tensors=\"pt\") for text in df1['news_content']]\n",
    "\n",
    "    def classes(self):\n",
    "        return self.labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def get_batch_labels(self, idx):\n",
    "        # Fetch a batch of labels\n",
    "        return self.labels[idx]\n",
    "\n",
    "    def get_batch_texts(self, idx):\n",
    "        # Fetch a batch of inputs\n",
    "        return self.text[idx]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        batch_texts = self.get_batch_texts(idx)\n",
    "        batch_labels = self.get_batch_labels(idx)\n",
    "\n",
    "        return batch_texts, batch_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cba81ca8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# change runtype to GPU\n",
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "91c3dd75",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train, df_val = train_test_split(df[['news_content']], test_size=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "03f35c6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "58e371b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'bert.pooler.dense.bias', 'cls.seq_relationship.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, BertForMaskedLM\n",
    "model = BertForMaskedLM.from_pretrained(\"bert-base-uncased\")\n",
    "model=model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "aca577d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>news_content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The article provides information for students ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The article discusses a recent email sent by J...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>UBS has sent about 100 people to assess client...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>UBS has announced the launch of an exchange-tr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>In der wichtigsten Geschäftsstelle der Zuger K...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3621</th>\n",
       "      <td>... EURO STOXX 50 DVP EURO STOXX Banks sind ei...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3622</th>\n",
       "      <td>Die spektakuläre Insolvenz der Silicon Valley ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3623</th>\n",
       "      <td>And the UK must make it easier for banking sta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3624</th>\n",
       "      <td>Für die Aktie Commerzbank aus dem Segment Dive...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3625</th>\n",
       "      <td>The volatility we are seeing in markets highli...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3626 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           news_content\n",
       "0     The article provides information for students ...\n",
       "1     The article discusses a recent email sent by J...\n",
       "2     UBS has sent about 100 people to assess client...\n",
       "3     UBS has announced the launch of an exchange-tr...\n",
       "4     In der wichtigsten Geschäftsstelle der Zuger K...\n",
       "...                                                 ...\n",
       "3621  ... EURO STOXX 50 DVP EURO STOXX Banks sind ei...\n",
       "3622  Die spektakuläre Insolvenz der Silicon Valley ...\n",
       "3623  And the UK must make it easier for banking sta...\n",
       "3624  Für die Aktie Commerzbank aus dem Segment Dive...\n",
       "3625  The volatility we are seeing in markets highli...\n",
       "\n",
       "[3626 rows x 1 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train=df_train.reset_index()\n",
    "df_train.drop('index',axis=1,inplace=True)\n",
    "\n",
    "df_val=df_val.reset_index()\n",
    "df_val.drop('index',axis=1,inplace=True)\n",
    "df_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f66054f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# display BERT layers\\nn=0\\nfor x in model.state_dict():\\n    n=n+1\\n    print(x)\\nn\\n'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "# display BERT layers\n",
    "n=0\n",
    "for x in model.state_dict():\n",
    "    n=n+1\n",
    "    print(x)\n",
    "n\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "060a1c60",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f4031443",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>news_content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Berenberg Bank analysts have provided a buy ra...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The article states that Berenberg, a German in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>In their analysis on October 30, 2023, experts...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Berenberg Bank has issued a \\\"buy\\\" recommenda...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The private bank Berenberg has upgraded its ra...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14496</th>\n",
       "      <td>INTERVIEW Interview with Les Échos Interview w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14497</th>\n",
       "      <td>UBS's latest Investor Watch report reveals tha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14498</th>\n",
       "      <td>SNB erwartet für 2021 Jahresgewinn von rund 26...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14499</th>\n",
       "      <td>0:00 News A cryptocurrency exchange in Hong Ko...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14500</th>\n",
       "      <td>Bloomberg has reported that a number of indivi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>14501 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            news_content\n",
       "0      Berenberg Bank analysts have provided a buy ra...\n",
       "1      The article states that Berenberg, a German in...\n",
       "2      In their analysis on October 30, 2023, experts...\n",
       "3      Berenberg Bank has issued a \\\"buy\\\" recommenda...\n",
       "4      The private bank Berenberg has upgraded its ra...\n",
       "...                                                  ...\n",
       "14496  INTERVIEW Interview with Les Échos Interview w...\n",
       "14497  UBS's latest Investor Watch report reveals tha...\n",
       "14498  SNB erwartet für 2021 Jahresgewinn von rund 26...\n",
       "14499  0:00 News A cryptocurrency exchange in Hong Ko...\n",
       "14500  Bloomberg has reported that a number of indivi...\n",
       "\n",
       "[14501 rows x 1 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f5c31206",
   "metadata": {},
   "outputs": [],
   "source": [
    "# change datatypes of input data\n",
    "df_train['news_content']=df_train['news_content'].astype(str)\n",
    "\n",
    "df_val['news_content']=df_val['news_content'].astype(str)\n",
    "df_train['news_content'].iloc[0]\n",
    "\n",
    "train_data=df_train\n",
    "val_data=df_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7f4519c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AdamW\n",
    "# initialize optimizer\n",
    "optim = AdamW(model.parameters(), lr=5e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "888747bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "27b231fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "LR = 0.00001\n",
    "EPOCHS = 250\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5e7a1e28",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using pad_token, but it is not set yet.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "51bfc04117474badb4781f4ffa153558",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Some parameters are on the meta device device because they were offloaded to the cpu.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "# Add the parent directory to the Python path\n",
    "from reghub_pack.models import BERT_RegHub_Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fbdd53b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'bert.pooler.dense.bias', 'cls.seq_relationship.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'bert.pooler.dense.bias', 'cls.seq_relationship.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model = BERT_RegHub_Similarity()\n",
    "model.load_mlm()\n",
    "model.model_training(train_data=train_data, val_data=val_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84eee080",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.bert.train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a3fc369",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c15b920",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_data, val_data, learning_rate, epochs):\n",
    "    \n",
    "    stop_criteria=0\n",
    "    \n",
    "    train, val = Dataset(train_data), Dataset(val_data)\n",
    "    \n",
    "    # mini batching\n",
    "    train_dataloader = torch.utils.data.DataLoader(train, batch_size=20) # max 24\n",
    "    val_dataloader = torch.utils.data.DataLoader(val, batch_size=20)\n",
    "\n",
    "    use_cuda = torch.cuda.is_available()\n",
    "    device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = Adam(model.parameters(), lr= learning_rate)\n",
    "    \n",
    "    if use_cuda:\n",
    "\n",
    "            model = model.cuda()\n",
    "            criterion = criterion.cuda()\n",
    "            \n",
    "    plot_val_acc=[]\n",
    "    plot_train_acc=[]\n",
    "    plot_epoch=[]\n",
    "    plot_train_loss=[]\n",
    "    plot_val_loss=[]\n",
    "\n",
    "    for epoch_num in range(epochs):\n",
    "\n",
    "            total_loss_train = 0\n",
    "            total_loss_val = 0\n",
    "\n",
    "            true_acc=0\n",
    "            n=0\n",
    "            with tqdm(total=len(train_dataloader), desc=f'Epoch {epoch_num + 1}/{epochs}', unit='item',position=0,leave=True) as p_bar:\n",
    "                for train_input, train_label in train_dataloader:\n",
    "                    \n",
    "                    optimizer.zero_grad()\n",
    "\n",
    "    \n",
    "                    train_label = train_label['input_ids'].to(device) # to cuda GPU\n",
    "                    mask = train_input['attention_mask'].to(device) # attention mask\n",
    "                    input_id = train_input['input_ids'].squeeze(1).to(device)\n",
    "\n",
    "                    # model output\n",
    "                    outputs = model(input_ids = input_id, attention_mask=mask,labels=train_label)\n",
    "                    \n",
    "                    batch_loss = outputs.loss\n",
    "                    batch_loss.backward()\n",
    "                    optimizer.step()\n",
    "\n",
    "\n",
    "                    # loss value\n",
    "                    total_loss_train += batch_loss.item()  \n",
    "\n",
    "                    p_bar.set_postfix(loss=batch_loss.item() / len(train_input['input_ids']))\n",
    "                    p_bar.update()\n",
    "                p_bar.set_postfix({'Epoch': epoch_num + 1})\n",
    "            # print(output)\n",
    "            # print(train_label)\n",
    "\n",
    "            # for validation accuracy\n",
    "            with torch.no_grad():\n",
    "\n",
    "                for val_input, val_label in val_dataloader:\n",
    "                    \n",
    "                    val_label = val_label['input_ids'].to(device)\n",
    "                    mask = val_input['attention_mask'].to(device)\n",
    "                    input_id = val_input['input_ids'].squeeze(1).to(device)\n",
    "                    \n",
    "                    outputs = model(input_ids = input_id, attention_mask=mask,labels=val_label)\n",
    "            \n",
    "                    batch_loss = outputs.loss                                        \n",
    "                    # validation loss value\n",
    "                    total_loss_val += batch_loss.item()\n",
    "           \n",
    "     \n",
    "            print(\n",
    "            f'Epochs: {epoch_num + 1} | Train Loss: {total_loss_train / len(train_data): .3f} \\\n",
    "            | Val Loss: {total_loss_val / len(val_data): .3f}',end='\\r')\n",
    "   \n",
    "        \n",
    "train(model, df_train, df_val, LR, EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2407aff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, 'BERT_mask.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11a92d75",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "from transformers import BertModel\n",
    "model1 = BertModel.from_pretrained('bert-base-uncased')\n",
    "model1\n",
    "\n",
    "# display BERT layers\n",
    "n=0\n",
    "for x in model1.state_dict():\n",
    "    n=n+1\n",
    "    print(x)\n",
    "n\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "227517cb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpu1",
   "language": "python",
   "name": "gpu1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
